{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The objectives of this assignment are to:\n",
    "\n",
    "1 - Write a program to recognize named entities in text\n",
    "\n",
    "2 - Learn how to manage a text data set\n",
    "\n",
    "3 - Apply recurrent neural networks to text\n",
    "\n",
    "4 - Know what word embeddings are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming\n",
    "\n",
    "#### 0 - Collecting a Dataset\n",
    "\n",
    "You will use a dataset from the CoNLL conferences that benchmark natural language processing systems and tasks. There were two conferences on named entity recognition: CoNLL 2002 (Spanish and Dutch) and CoNLL 2003 (English and German). In this assignment, you will work on the English dataset. Read the description of the task.\n",
    "\n",
    "1 - The datasets are protected by a license and you need to obtain it to reconstruct the data. Alternatively, you can use a local copy or try to find one on github (type conll2003 in the search box) or use the Google dataset search: https://toolbox.google.com/datasetsearch. You can find a local copy in the /usr/local/cs/EDAN95/datasets/NER-data folder.\n",
    "\n",
    "2 - The dataset comes in the form of three files: a training set, a development set, and a test set. You will use the test set to evaluate your models. For this, you will apply the conlleval script that will compute the harmonic mean of the precision and recall: F1. You have a local copy of this script in /usr/local/cs/EDAN95/datasets/ner/bin. conlleval is written in Perl. Be sure to have it on your machine to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Headers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "from keras import models, layers\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Bidirectional, SimpleRNN, Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'rmsprop'\n",
    "SCALER = True\n",
    "SIMPLE_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "#MINI_CORPUS = True\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "UNKNOWN_TOKEN = '__UNK__'\n",
    "W_SIZE = 2\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "\n",
    "LSTM_UNITS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the GloVe word embeddings\n",
    "Head to https://nlp.stanford.edu/projects/glove/ (where you can learn more about the GloVe algorithm), and download the pre-computed embeddings from 2014 English Wikipedia. It's a 822MB zip file named glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens). Un-zip it.\n",
    "\n",
    "Here we Write a function that reads GloVe embeddings and store them in a dictionary, where the keys will be the words and the values, the embeddings.\n",
    "\n",
    "An embedding is a mapping from discrete objects, such as words, to vectors of real numbers. The individual dimensions in these vectors typically have no inherent meaning. Instead, it's the overall patterns of location and distance between vectors that machine learning takes advantage of. Embeddings are important for input to machine learning. Classifiers, and neural networks more generally, work on vectors of real numbers. They train best on dense vectors, where all values contribute to define an object. However, many important inputs to machine learning, such as words of text, do not have a natural vector representation. Embedding functions are the standard and effective way to transform such discrete input objects into useful continuous vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    \n",
    "    glove = open(file)\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    \n",
    "    embeddings_dict = embeddings\n",
    "    #embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    \n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/lab4/'\n",
    "embedding_file = BASE_DIR + 'glove.6B/glove.6B.100d.txt'\n",
    "embeddings_dict = load(embedding_file)\n",
    "embedded_words = sorted(list(embeddings_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings_dict['table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/lab4/'\n",
    "glove_dir = BASE_DIR + 'glove.6B/'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a cosine similarity, compute the 5 closest words to the words table, france, and sweden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(embeddings_dict.values())[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(embeddings_dict.keys())[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('france', 0.0),\n",
       " ('belgium', 0.19235771894454956),\n",
       " ('french', 0.19956225156784058),\n",
       " ('britain', 0.2049471139907837),\n",
       " ('spain', 0.24425369501113892)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "table = embeddings_dict['france']\n",
    "#table = np.random.rand(1,100)\n",
    "#a = np.random.rand(5,100)\n",
    "#similarities = []\n",
    "sim_dict = {}\n",
    "#simmi = {}\n",
    "for word, vector in embeddings_dict.items():\n",
    "#for word in embeddings_dict.values:\n",
    "        #print(cosine(table, a[i]))\n",
    "        #print(cosine(table, a[i]))\n",
    "        sim = cosine(table, vector)\n",
    "        #sim = cosine_similarity(table,word)\n",
    "        #key = embeddings_dict.get(word)\n",
    "        #print (sim)\n",
    "        #sim_dict.update(word=sim)\n",
    "        sim_dict[word] = sim\n",
    "\n",
    "sorted_by_value = sorted(sim_dict.items(), key = lambda kv: kv[1])        \n",
    "\n",
    "sorted_by_value[0 : 5]\n",
    "#print(table)\n",
    "#print(len(sim_dict))\n",
    "\n",
    "#print(similarities[:3])\n",
    "#print(sim_dict.get(1.438331514596939))\n",
    "#sim_dict.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Preprocessing is more complex through 4 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "Like all other neural networks, deep-learning models don’t take as input raw text: they only work with numeric tensors. Vectorizing text is the process of transforming text into numeric tensors. The same, We can’t feed lists of integers into a neural network. You have to turn your lists into tensors. Vectorizing text is the process of transforming text into numeric tensors. \n",
    "\n",
    "Collectively, the different units into which you can break down text (words, characters, or n-grams) are called tokens, and breaking text into such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme and\n",
    "then associating numeric vectors with the generated tokens. These vectors, packed into sequence tensors, are fed into deep neural networks.\n",
    "\n",
    "There are multiple ways to associate a vector with a token: one-hot encoding of tokens, and token embedding (typically used exclusively for words, and called word embedding).\n",
    "\n",
    "There are two ways to do that:\n",
    "\n",
    "1 - Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the Embedding layer, which we’ll cover in detail later in the book).\n",
    "\n",
    "2 - One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000 dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a Dense layer, capable of handling floating-point vector data.\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Loading the Corpus: function for reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/lab4/'\n",
    "\n",
    "def load_conll2009():\n",
    "    train_file = BASE_DIR + 'NER-data/eng.train'\n",
    "    dev_file = BASE_DIR + 'NER-data/eng.valid'\n",
    "    test_file = BASE_DIR + 'NER-data/eng.test'\n",
    "    # test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    #column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "    column_names = ['form', 'pos', 'chunk', 'ner']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "# Read the corpus\n",
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2009()\n",
    "\n",
    "print(type(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Class for tokenization / Storing the rows in dictionaries\n",
    "Converting the Corpus in a Dictionary, we follow the fit-transform pattern of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store the rows in dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, train: [{'form': '-DOCSTART-', 'pos': '-X-', 'chunk': 'O', 'ner': 'O'}]\n",
      "Second sentence, train: [{'form': 'EU', 'pos': 'NNP', 'chunk': 'I-NP', 'ner': 'I-ORG'}, {'form': 'rejects', 'pos': 'VBZ', 'chunk': 'I-VP', 'ner': 'O'}, {'form': 'German', 'pos': 'JJ', 'chunk': 'I-NP', 'ner': 'I-MISC'}, {'form': 'call', 'pos': 'NN', 'chunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP', 'ner': 'O'}, {'form': 'boycott', 'pos': 'VB', 'chunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'pos': 'JJ', 'chunk': 'I-NP', 'ner': 'I-MISC'}, {'form': 'lamb', 'pos': 'NN', 'chunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'pos': '.', 'chunk': 'O', 'ner': 'O'}]\n",
      "First sentence, test: [{'form': '-DOCSTART-', 'pos': '-X-', 'chunk': '-X-', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "#conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "\n",
    "#if MINI_CORPUS:\n",
    "#   train_dict = train_dict[:len(train_dict) // 5]\n",
    "    \n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "\n",
    "print('First sentence, train:', train_dict[0])\n",
    "print('Second sentence, train:', train_dict[1])\n",
    "print('First sentence, test:', test_dict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3 - Extracting the Context and Dictorizing it\n",
    "Extract the features and store them in dictionaries.\n",
    "\n",
    "We extract windows of five words surrounding the word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Vectorizing the symbols (X and Matrices)\n",
    "Vectorizing X: We transform the x symbols into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the X and Y Sequences \n",
    "Function to build the two-way sequences: Two vectors: x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_sequences(corpus_dict, key_x='form', key_y='upos', tolower=True):\n",
    "def build_sequences(corpus_dict, key_x='form', key_y='ner', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "            \n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "            \n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words \n",
      " ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "First sentence, NER \n",
      " ['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_cat, Y_train_cat = build_sequences(train_dict)\n",
    "\n",
    "print('First sentence, words \\n', X_train_cat[1])\n",
    "print('First sentence, NER \\n', Y_train_cat[1])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing the same for the development set\n",
    "X_dev_cat, Y_dev_cat = build_sequences(dev_dict)\n",
    "\n",
    "print('First sentence in dev_dict, words \\n', X_dev_cat[1])\n",
    "print('First sentence in dev_dict, NER \\n', Y_dev_cat[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the Unique Words and Named Entities Recognition \n",
    "Training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training \n",
      " ['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary_words = sorted(list(\n",
    "    set([word for sentence \n",
    "         in X_train_cat for word in sentence])))\n",
    "\n",
    "ner = sorted(list(set([ner for sentence \n",
    "                       in Y_train_cat for ner in sentence])))\n",
    "print('Unique words in training \\n', ner)\n",
    "NB_CLASSES = len(ner)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting  - Dev set\n",
    "vocabulary_words_dev = sorted(list(\n",
    "    set([word for sentence \n",
    "         in X_dev_cat for word in sentence])))\n",
    "\n",
    "ner = sorted(list(set([ner for sentence \n",
    "                       in Y_dev_cat for ner in sentence])))\n",
    "print('Unique words in Dev set \\n', ner)\n",
    "NB_CLASSES = len(ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create the dictionary\n",
    "We add two words for the padding symbol and unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in GloVe: 400000\n",
      "# unique words in the training vocabulary: embeddings and corpus: 402597\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "\n",
    "vocabulary_words = sorted(list(set(vocabulary_words + \n",
    "                                   list(embeddings_words))))\n",
    "cnt_uniq = len(vocabulary_words) + 2\n",
    "print('# unique words in the training vocabulary: embeddings and corpus:', \n",
    "      cnt_uniq)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We create the dictionary - Dev set\n",
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "\n",
    "vocabulary_words_dev = sorted(list(set(vocabulary_words_dev + \n",
    "                                   list(embeddings_words))))\n",
    "cnt_uniq = len(vocabulary_words_dev) + 2\n",
    "print('# unique words in the dev vocabulary: embeddings and corpus:', \n",
    "      cnt_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert the words or NER to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(X, idx):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or NER lists) to indexes\n",
    "    :param X: List of word (or NER) lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown words to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, 1), x))\n",
    "        X_idx += [x_idx]\n",
    "        \n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index: \n",
      " [('!', 2), ('!!', 3), ('!!!', 4), ('!!!!', 5), ('!!!!!', 6), ('!?', 7), ('!?!', 8), ('\"', 9), ('#', 10), ('##', 11)]\n",
      "NER index: \n",
      " [('B-LOC', 2), ('B-MISC', 3), ('B-ORG', 4), ('I-LOC', 5), ('I-MISC', 6), ('I-ORG', 7), ('I-PER', 8), ('O', 9)]\n",
      "First sentences, word indices \n",
      " [[935], [142143, 307143, 161836, 91321, 363368, 83766, 85852, 218260, 936], [284434, 79019]]\n",
      "First sentences, NER indices \n",
      " [[9], [7, 9, 6, 9, 9, 9, 6, 9, 9], [8, 8]]\n"
     ]
    }
   ],
   "source": [
    "# We start at one to make provision for the padding symbol 0 \n",
    "# in RNN and LSTMs and 1 for the unknown words\n",
    "\n",
    "rev_word_idx = dict(enumerate(vocabulary_words, start=2))\n",
    "#rev_ner_idx = dict(enumerate(ner, start=2))\n",
    "ner_rev_idx = dict(enumerate(ner, start=2))\n",
    "\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "#ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "ner_idx = {v: k for k, v in ner_rev_idx.items()}\n",
    "\n",
    "print('word index: \\n', list(word_idx.items())[:10])\n",
    "print('NER index: \\n', list(ner_idx.items())[:10])\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_idx = to_index(X_train_cat, word_idx)\n",
    "Y_idx = to_index(Y_train_cat, ner_idx)\n",
    "\n",
    "print('First sentences, word indices \\n', X_idx[:3])\n",
    "print('First sentences, NER indices \\n', Y_idx[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create the indexes - Dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We start at one to make provision for the padding symbol 0 \n",
    "# in RNN and LSTMs and 1 for the unknown words\n",
    "\n",
    "rev_word_idx_dev = dict(enumerate(vocabulary_words_dev, start=2))\n",
    "rev_ner_idx_dev = dict(enumerate(ner, start=2))\n",
    "\n",
    "word_idx_dev = {v: k for k, v in rev_word_idx_dev.items()}\n",
    "ner_idx_dev = {v: k for k, v in rev_ner_idx_dev.items()}\n",
    "\n",
    "#print('word index:', list(word_idx.items())[:10])\n",
    "#print('NER index:', list(ner_idx.items())[:10])\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_idx_dev = to_index(X_dev_cat, word_idx_dev)\n",
    "Y_idx_dev = to_index(Y_dev_cat, ner_idx_dev)\n",
    "\n",
    "print('First sentences, word indices \\n', X_idx_dev[:3])\n",
    "print('First sentences, NER indices \\n', Y_idx_dev[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We pad the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0 935]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 9]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X_idx)\n",
    "Y = pad_sequences(Y_idx)\n",
    "\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "# The number of NER classes and 0 (padding symbol)\n",
    "Y_train = to_categorical(Y, num_classes=len(ner) + 2)\n",
    "print(Y_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We pad the sentences - Dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = pad_sequences(X_idx_dev)\n",
    "Y = pad_sequences(Y_idx_dev)\n",
    "\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "# The number of NER classes and 0 (padding symbol)\n",
    "Y_dev = to_categorical(Y, num_classes=len(ner) + 2)\n",
    "print(Y_dev[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### We create an embedding matrix\n",
    "\n",
    "0 is the padding symbol and index one is a unknown word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdstate = np.random.RandomState(1234567)\n",
    "embedding_matrix = rdstate.uniform(-0.05, 0.05, \n",
    "                                   (len(vocabulary_words) + 2, \n",
    "                                    EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402597, 100)\n",
      "Embedding of table \n",
      " [-0.61453998  0.89692998  0.56770998  0.39102    -0.22437     0.49035001\n",
      "  0.10868     0.27410999 -0.23833001 -0.52152997  0.73550999 -0.32653999\n",
      "  0.51304001  0.32415    -0.46709001  0.68050998 -0.25497001 -0.040484\n",
      " -0.54417998 -1.05480003 -0.46691999  0.23557     0.31233999 -0.34536999\n",
      "  0.14793    -0.53745002 -0.43215001 -0.48723999 -0.51019001 -0.90509999\n",
      " -0.17918999 -0.018376    0.09719    -0.31623     0.75120002  0.92236\n",
      " -0.49965     0.14036    -0.28296    -0.97443002 -0.0094408  -0.62944001\n",
      "  0.14711    -0.94375998  0.0075222   0.18565001 -0.99172002  0.072789\n",
      " -0.18474001 -0.52901     0.38995001 -0.45677    -0.21932     1.37230003\n",
      " -0.29635999 -2.2342     -0.36667001  0.04987     0.63420999  0.53275001\n",
      " -0.53955001  0.31398001 -0.44698    -0.38389     0.066668   -0.02168\n",
      "  0.20558     0.59456003 -0.24891999 -0.52794999 -0.3761      0.077104\n",
      "  0.75221997 -0.2647     -0.0587      0.67540997 -0.16559    -0.49278\n",
      " -0.26326999 -0.21214999  0.24316999  0.17005999 -0.29260001 -0.50089997\n",
      " -0.56638002 -0.40377    -0.48451999 -0.32539001  0.75292999  0.0049585\n",
      " -0.32115     0.28898999 -0.042392    0.63862997 -0.20332    -0.46785\n",
      " -0.15661     0.21789999  1.41429996  0.40033999]\n",
      "Embedding of the padding symbol, idx 0, random numbers \n",
      " [-0.02629708 -0.04923516 -0.04801697 -0.01869074 -0.04005453 -0.03048257\n",
      " -0.0292702  -0.03350688  0.0211879  -0.04679333 -0.03026304  0.0464557\n",
      "  0.00738946  0.01992277  0.04746414  0.01543505 -0.02391317 -0.03035904\n",
      "  0.03614633 -0.01292743 -0.01311645  0.00429138  0.01827985  0.03228761\n",
      " -0.03686076 -0.04223968  0.03409078 -0.0278994   0.02529113 -0.0156977\n",
      " -0.04902496 -0.01042922 -0.029072   -0.00319148 -0.01353996  0.00950514\n",
      "  0.0413734  -0.00028032 -0.01519774 -0.01369095  0.03702888 -0.01152137\n",
      "  0.03035301  0.00264644 -0.0463597  -0.02356203 -0.033484   -0.02621933\n",
      " -0.03773337  0.01826283  0.03646911 -0.04109766 -0.03953006  0.04822013\n",
      " -0.02821295 -0.0431476  -0.02476419  0.04927545 -0.02866612 -0.00881531\n",
      " -0.01183301  0.02965345 -0.03483367  0.0109977  -0.04514807 -0.0231921\n",
      "  0.00176915  0.00485835 -0.0040727  -0.01905112 -0.02361332 -0.03425079\n",
      "  0.04564135 -0.02310861  0.04121954 -0.04504611  0.00330172  0.01987282\n",
      " -0.00783856  0.0198199  -0.02306001 -0.0187176   0.03677814 -0.03901311\n",
      " -0.03016801  0.03832556  0.02892775  0.04970791 -0.01038987  0.04192337\n",
      "  0.0403074  -0.01217054  0.04221675 -0.02228818  0.03978376 -0.00845296\n",
      "  0.00691154  0.01943966  0.00093845 -0.03084958]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "print('Embedding of table \\n', embedding_matrix[word_idx['table']])\n",
    "print('Embedding of the padding symbol, idx 0, random numbers \\n', \n",
    "      embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding matrix - Dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rdstate = np.random.RandomState(1234567)\n",
    "embedding_matrix_dev = rdstate.uniform(-0.05, 0.05, \n",
    "                                   (len(vocabulary_words_dev) + 2, \n",
    "                                    EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for word in vocabulary_words_dev:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "print('Embedding of table', embedding_matrix[word_idx_dev['table']])\n",
    "print('Embedding of the padding symbol, idx 0, random numbers', \n",
    "      embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using word embeddings\n",
    "Another popular and powerful way to associate a vector with a word is the use of dense word vectors, also called word embeddings. Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary), word embeddings are low-\n",
    "dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors).\n",
    "\n",
    "Unlike the word vectors obtained via one-hot encoding, word\n",
    "embeddings are learned from data. It’s common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information into far fewer dimensions.\n",
    "\n",
    "There are two ways to obtain word embeddings:\n",
    "\n",
    "1 - Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
    "\n",
    "2 - Load into your model word embeddings that were precomputed using a different machine-learning task than the one you’re trying to solve. These are called pretrained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simple Reccurent Network (Tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e6818e74ae19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.add(layers.Embedding(len(vocabulary_words) + 2,      \n\u001b[1;32m      4\u001b[0m                            \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(len(vocabulary_words) + 2,      \n",
    "                           EMBEDDING_DIM,\n",
    "                           mask_zero=True,\n",
    "                           input_length=None))\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "# The default is True\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# a simple RNN network\n",
    "#model.add(SimpleRNN(100, return_sequences=True))\n",
    "\n",
    "# a simple RNN network with Bidirectional\n",
    "#model.add(Bidirectional(SimpleRNN(100, return_sequences=True)))\n",
    "\n",
    "#a simple LSTM network\n",
    "#model.add(LSTM(100, return_sequences=True))                         # dropout=0.1, recurrent_dropout=0.5,\n",
    "\n",
    "# a stack of several recurrent layers \n",
    "# Using recurrent dropout to fight overfitting\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True)))           # the last layer only returns the last output           \n",
    "\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(Dense(NB_CLASSES + 2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, None, 100)         40069600  \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, None, 100)         20100     \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, None, 200)         160800    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, None, 9)           1809      \n",
      "=================================================================\n",
      "Total params: 40,252,309\n",
      "Trainable params: 40,252,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2997/2997 [==============================] - 30s 10ms/step - loss: 0.6388 - acc: 0.8191\n",
      "Epoch 2/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.3150 - acc: 0.9034\n",
      "Epoch 3/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.2170 - acc: 0.9353\n",
      "Epoch 4/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.1638 - acc: 0.9504\n",
      "Epoch 5/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.1333 - acc: 0.9585\n",
      "Epoch 6/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.1101 - acc: 0.9663\n",
      "Epoch 7/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.0883 - acc: 0.9729\n",
      "Epoch 8/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.0752 - acc: 0.9767\n",
      "Epoch 9/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.0605 - acc: 0.9813\n",
      "Epoch 10/10\n",
      "2997/2997 [==============================] - 21s 7ms/step - loss: 0.0471 - acc: 0.9856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b748dcb38>"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of system: \n",
    "\n",
    "#### Formatting the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0] test idx [891]\n",
      "Y[0] test idx [8]\n",
      "X[0] test idx passed [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 891]\n",
      "Y[0] test idx padded [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 8]\n",
      "Y[0] test idx padded vectorized [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "(3684, 124)\n",
      "(3684, 124, 9)\n"
     ]
    }
   ],
   "source": [
    "# In X_dict, we replace the words with their index\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict)\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_test_idx = to_index(X_test_cat, word_idx)\n",
    "Y_test_idx = to_index(Y_test_cat, ner_idx)\n",
    "\n",
    "print('X[0] test idx', X_test_idx[0])\n",
    "print('Y[0] test idx', Y_test_idx[0])\n",
    "\n",
    "X_test_padded = pad_sequences(X_test_idx)\n",
    "Y_test_padded = pad_sequences(Y_test_idx)\n",
    "\n",
    "print('X[0] test idx passed \\n', X_test_padded[0])\n",
    "print('Y[0] test idx padded \\n', Y_test_padded[0])\n",
    "\n",
    "# One extra symbol for 0 (padding)\n",
    "Y_test_padded_vectorized = to_categorical(Y_test_padded, \n",
    "                                          num_classes=len(ner) + 2)\n",
    "print('Y[0] test idx padded vectorized \\n', Y_test_padded_vectorized[0])\n",
    "print(X_test_padded.shape)\n",
    "print(Y_test_padded_vectorized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 11s 3ms/step\n",
      "Loss: 0.21689066391976208\n",
      "Accuracy: 0.9342477429702668\n"
     ]
    }
   ],
   "source": [
    "# Evaluates with the padding symbol\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, \n",
    "                                     Y_test_padded_vectorized)\n",
    "print('Loss:', test_loss)\n",
    "print('Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We evaluate on all the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test\n",
      " ['-docstart-']\n",
      "X_test_padded\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 891]\n",
      "Y_test\n",
      " ['O']\n",
      "Y_test_padded\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 8]\n",
      "predictions\n",
      " [[3.8932071e-03 2.0994707e-03 6.1671459e-03 ... 3.7002046e-02\n",
      "  2.2152795e-02 8.5350865e-01]\n",
      " [3.8932071e-03 2.0994707e-03 6.1671459e-03 ... 3.7002046e-02\n",
      "  2.2152795e-02 8.5350865e-01]\n",
      " [3.8932071e-03 2.0994707e-03 6.1671459e-03 ... 3.7002046e-02\n",
      "  2.2152795e-02 8.5350865e-01]\n",
      " ...\n",
      " [3.8932071e-03 2.0994707e-03 6.1671459e-03 ... 3.7002046e-02\n",
      "  2.2152795e-02 8.5350865e-01]\n",
      " [3.8932071e-03 2.0994707e-03 6.1671459e-03 ... 3.7002046e-02\n",
      "  2.2152795e-02 8.5350865e-01]\n",
      " [4.7312988e-06 2.9684611e-06 2.4525192e-05 ... 5.2190293e-04\n",
      "  2.3054771e-04 9.9840838e-01]]\n"
     ]
    }
   ],
   "source": [
    "print('X_test' + '\\n', X_test_cat[0])\n",
    "print('X_test_padded' + '\\n', X_test_padded[0])\n",
    "\n",
    "corpus_ner_predictions = model.predict(X_test_padded)\n",
    "\n",
    "print('Y_test' + '\\n', Y_test_cat[0])\n",
    "print('Y_test_padded' + '\\n', Y_test_padded[0])\n",
    "print('predictions' + '\\n', corpus_ner_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[4.7312988e-06, 2.9684611e-06, 2.4525192e-05, 1.1403169e-04,\n",
      "        1.4805868e-04, 5.4486602e-04, 5.2190293e-04, 2.3054771e-04,\n",
      "        9.9840838e-01]], dtype=float32), array([[2.35657950e-07, 4.72813220e-08, 2.13263456e-06, 2.08299498e-05,\n",
      "        1.93560903e-04, 2.43502064e-03, 6.14209042e-04, 1.60869149e-05,\n",
      "        9.96717870e-01],\n",
      "       [3.48824331e-10, 1.01019589e-10, 1.23680586e-08, 1.13703969e-07,\n",
      "        1.92286166e-06, 3.88664557e-05, 1.63839941e-05, 1.45495562e-06,\n",
      "        9.99941230e-01],\n",
      "       [8.62602178e-07, 4.31470397e-07, 4.58842114e-05, 6.77316712e-05,\n",
      "        9.16631699e-01, 3.71768326e-02, 3.94256413e-02, 2.82957801e-04,\n",
      "        6.36795023e-03],\n",
      "       [6.13386320e-09, 1.94492467e-09, 1.21948574e-07, 2.15761929e-06,\n",
      "        5.64679322e-05, 7.13074172e-04, 3.63511936e-04, 4.09523673e-05,\n",
      "        9.98823702e-01],\n",
      "       [4.29836389e-09, 1.28940214e-09, 6.39856381e-08, 7.42536997e-07,\n",
      "        1.09179764e-05, 7.81541690e-04, 5.43775735e-04, 5.81360189e-04,\n",
      "        9.98081565e-01],\n",
      "       [6.43800735e-10, 1.72447071e-10, 1.36088767e-08, 1.96481409e-07,\n",
      "        4.40446456e-06, 4.32842673e-04, 3.31720919e-04, 2.30273217e-04,\n",
      "        9.99000609e-01],\n",
      "       [7.98143496e-12, 2.41304676e-12, 4.05897982e-10, 5.07702369e-09,\n",
      "        2.48313512e-07, 2.50334483e-06, 5.84952068e-06, 7.99434747e-06,\n",
      "        9.99983430e-01],\n",
      "       [4.18449524e-08, 1.73846839e-08, 3.83538918e-06, 3.43985084e-06,\n",
      "        9.29568470e-01, 7.98128254e-04, 1.14234453e-02, 8.21224123e-04,\n",
      "        5.73814362e-02],\n",
      "       [1.67796194e-10, 9.42395675e-11, 2.32396786e-08, 1.05581925e-07,\n",
      "        4.32438625e-04, 2.03513537e-05, 2.22702038e-05, 2.21698133e-06,\n",
      "        9.99522567e-01],\n",
      "       [1.57600344e-09, 5.66777680e-10, 4.79578368e-08, 3.91327347e-07,\n",
      "        1.13353613e-04, 1.25054692e-04, 5.29325553e-05, 1.16547953e-05,\n",
      "        9.99696612e-01],\n",
      "       [2.72708911e-10, 9.35786240e-11, 6.39464881e-09, 1.00485025e-07,\n",
      "        1.32481262e-06, 3.42804597e-05, 1.85174522e-05, 1.83662962e-06,\n",
      "        9.99943972e-01],\n",
      "       [5.80734849e-10, 3.07929848e-10, 1.31543620e-08, 1.45982114e-07,\n",
      "        4.77724370e-07, 1.43797160e-05, 1.28555084e-05, 2.04876642e-06,\n",
      "        9.99970078e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "ner_pred_num = []\n",
    "\n",
    "for sent_nbr, sent_ner_predictions in enumerate(corpus_ner_predictions):\n",
    "    ner_pred_num += [sent_ner_predictions[-len(X_test_cat[sent_nbr]):]]\n",
    "    \n",
    "print(ner_pred_num[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert NER indices to symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O'], ['O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O']]\n",
      "3684\n",
      "[['O'], ['O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "ner_pred = []\n",
    "\n",
    "for sentence in ner_pred_num:\n",
    "    ner_pred_idx = list(map(np.argmax, sentence))\n",
    "    #ner_pred_cat = list(map(rev_ner_idx.get, ner_pred_idx))\n",
    "    ner_pred_cat = list(map(ner_rev_idx.get, ner_pred_idx))\n",
    "    ner_pred += [ner_pred_cat]\n",
    "\n",
    "print(ner_pred[:2])\n",
    "print(len(ner_pred))\n",
    "print(Y_test_cat[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writting the results of our predictions and test set in one file\n",
    "After using the preict() method to predict the tags of the whole test set, we need to write our results in a file, where the two last columns will be the hand-annotated tag and the predicted tag. The fields must be separated by a space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file, test_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in test_dict:\n",
    "            sentence_lst = []\n",
    "            \n",
    "            for row in sentence:\n",
    "                # The lambda technique is useful for example when we want to pass \n",
    "                # a simple function as an argument to another function, like this:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                #sentence_lst += '\\t'.join(items) + '\\n'\n",
    "                sentence_lst += ' '.join(items) + '\\n'\n",
    "                \n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684\n",
      "[{'form': 'Nadim', 'pos': 'NNP', 'chunk': 'I-NP', 'ner': 'I-PER'}, {'form': 'Ladki', 'pos': 'NNP', 'chunk': 'I-NP', 'ner': 'I-PER'}]\n",
      "['I-LOC', 'I-ORG']\n",
      "hello [{'form': 'SOCCER', 'pos': 'NN', 'chunk': 'I-NP', 'ner': 'O'}, {'form': '-', 'pos': ':', 'chunk': 'O', 'ner': 'O'}, {'form': 'JAPAN', 'pos': 'NNP', 'chunk': 'I-NP', 'ner': 'I-LOC'}, {'form': 'GET', 'pos': 'VB', 'chunk': 'I-VP', 'ner': 'O'}, {'form': 'LUCKY', 'pos': 'NNP', 'chunk': 'I-NP', 'ner': 'O'}, {'form': 'WIN', 'pos': 'NNP', 'chunk': 'I-NP', 'ner': 'O'}, {'form': ',', 'pos': ',', 'chunk': 'O', 'ner': 'O'}, {'form': 'CHINA', 'pos': 'NNP', 'chunk': 'I-NP', 'ner': 'I-PER'}, {'form': 'IN', 'pos': 'IN', 'chunk': 'I-PP', 'ner': 'O'}, {'form': 'SURPRISE', 'pos': 'DT', 'chunk': 'I-NP', 'ner': 'O'}, {'form': 'DEFEAT', 'pos': 'NN', 'chunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'pos': '.', 'chunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testfile = 'NER-data/eng.test'\n",
    "\n",
    "#column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "column_names_pred = ['form', 'pos', 'chunk', 'ner', 'predicted-ner']\n",
    "\n",
    "testset = open(testfile).read().strip()\n",
    "\n",
    "#conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')    # XXXXXX\n",
    "conll_dict_pred = CoNLLDictorizer(column_names_pred, col_sep=' +')\n",
    "test_dict_pred = conll_dict_pred.transform(testset)\n",
    "print(len(test_dict_pred))\n",
    "\n",
    "print(test_dict_pred[2])\n",
    "#print(list(test_dict[:2]))\n",
    "# word_idx = {v: k for k, v in rev_word_idx.items()} \n",
    "\n",
    "print((ner_pred[2]))\n",
    "#pred_dict = {}\n",
    "#sent_index = 0\n",
    "\n",
    "print(\"hello\",test_dict_pred[1])\n",
    "\n",
    "for sent_index in range(len(test_dict_pred)):\n",
    "#for sentence in test_dict_pred:\n",
    "    #word_index = 0\n",
    "    \n",
    "    #if len(ner_pred[currentIndex]) != len(sentence):\n",
    "    #    print(\"ERROR!\")\n",
    "    #ext_keys = []\n",
    "    #ext_tags = []\n",
    "    for word_index in range(len(test_dict_pred[sent_index])):\n",
    "    #for word in sentence:   # every word is a row - dictionary\n",
    "        #list(word.keys()).append('predicted_ner')\n",
    "        #list(word.values()).append(str(ner_pred[sent_index][word_index]))\n",
    "        #ext_keys = list(word.keys()).append('predicted_ner')\n",
    "        #ext_tags = list(word.values()).append(str(ner_pred[sent_index][word_index]))\n",
    "        #test_dict_pred = dict(zip(ext_keys, ext_tags))\n",
    "        \n",
    "        #pred_dict['predicted-ner'] = ner_pred[sent_index][word_index]\n",
    "        #zip_dict = dict(zip(word, pred_dict))\n",
    "        #word['predicted-ner'] = ner_pred[sent_index][word_index]\n",
    "        test_dict_pred[sent_index][word_index]['predicted-ner'] = str(ner_pred[sent_index][word_index])\n",
    "        #test_dict_pred.update({'predicted-ner': ner_pred[sent_index][word_index]})\n",
    "        #sim_dict.update(word=sim)\n",
    "        #value = ner_pred[sent_index][word_index]\n",
    "        #word.update('predicted-ner'=value)\n",
    "        #word_index += 1\n",
    "    \n",
    "    \n",
    "    #sent_index += 1\n",
    "    \n",
    "    #if sent_index > 2000:\n",
    "    #    break\n",
    "    # i vårt test_dict, har vi meningar    \n",
    "# para ihop dessa meningar med ner_pred mening\n",
    "# para ihop orden i test-dict-meningen med tag i ner_pred mening   \n",
    "\n",
    "# skriv ut file med: \"ord\", \"GS\", \"pred-tag\"\n",
    "\n",
    "#print(type(zip_dict[:1]))\n",
    "save('out', test_dict_pred, column_names_pred)\n",
    "#save('out', zip_dict, column_names_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the evaluator in conlleval script\n",
    "Compute F1 = precision/recall\n",
    "By applying conlleval to the producted output and report the F1 result.\n",
    "\n",
    "Run the scorer in terminal like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perl conlleval < out\n",
    "#where out is replaced with the name of your output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output results from conlleval\n",
    "The evaluator prints precision and recall measures, and their harmonic mean (the F-measure, FB1). We also see the performance for each of the types of names, LOC, MISC, ORG and PER in this case. Note that this evaluation is quite tough: we get no credit for an almost-correct group.\n",
    "\n",
    "Here is what the evaluator writes when running it on the output file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 46666 tokens with 5648 phrases; found: 5156 phrases; correct: 3100.\n",
    "accuracy:  91.59%; precision:  60.12%; recall:  54.89%; FB1:  57.39\n",
    "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  7\n",
    "              LOC: precision:  63.00%; recall:  68.41%; FB1:  65.59  1811\n",
    "             MISC: precision:  41.90%; recall:  40.88%; FB1:  41.38  685\n",
    "              ORG: precision:  57.35%; recall:  32.63%; FB1:  41.60  945\n",
    "              PER: precision:  66.16%; recall:  69.88%; FB1:  67.97  1708\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 46666, correct 43811, accuracy 0.938821\n",
      "total unknown 1186, correct 944, accuracy 0.795953\n"
     ]
    }
   ],
   "source": [
    "total, correct, total_ukn, correct_ukn = 0, 0, 0, 0\n",
    "\n",
    "for id_s, sentence in enumerate(X_test_cat):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        total += 1\n",
    "        if ner_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "            correct += 1\n",
    "        # The word is not in the dictionary\n",
    "        if word not in word_idx:\n",
    "            total_ukn += 1\n",
    "            if ner_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "                correct_ukn += 1\n",
    "\n",
    "print('total %d, correct %d, accuracy %f' % \n",
    "      (total, correct, correct / total))\n",
    "if total_ukn != 0:\n",
    "    print('total unknown %d, correct %d, accuracy %f' % \n",
    "          (total_ukn, correct_ukn, correct_ukn / total_ukn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Named Entities Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, model, word_idx, \n",
    "                     vocabulary_words, rev_idx_ner, verbose=False):\n",
    "    # Predict one sentence\n",
    "    sentence = sentence.split()\n",
    "    word_idxs = to_index([sentence], word_idx)\n",
    "    word_idx_padded = pad_sequences(word_idxs)\n",
    "\n",
    "    ner_idx_pred = model.predict(word_idx_padded)\n",
    "    \n",
    "    # We remove padding\n",
    "    ner_idx_pred = ner_idx_pred[0][-len(sentence):]\n",
    "    ner_idx = list(map(np.argmax, ner_idx_pred))\n",
    "    ner = list(map(rev_idx_ner.get, ner_idx))\n",
    "    \n",
    "    if verbose:\n",
    "        print('Sentence', sentence)\n",
    "        print('Sentence word indexes', word_idxs)\n",
    "        print('Padded sentence', word_idx_padded)\n",
    "        print('NER predicted', ner_idx_pred[0])\n",
    "        print('NER shape', ner_idx_pred.shape)\n",
    "        \n",
    "    return ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def predict_sentence(sentence, dict_vect, model, ner_rev_idx):\n",
    "\n",
    "    #column_names = ['id', 'form']\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    \n",
    "    sentence = list(enumerate(sentence.lower().split(), start=1))\n",
    "    \n",
    "    conll_cols = ''\n",
    "    for tuple in sentence:\n",
    "        conll_cols += str(tuple[0]) + '\\t' + tuple[1] + '\\n'\n",
    "    #print(conll_cols)\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    sent_dict = conll_dict.transform(conll_cols)\n",
    "    #print('Sentence:', sent_dict[0])\n",
    "\n",
    "    context_dictorizer = ContextDictorizer()\n",
    "    context_dictorizer.fit(sent_dict)\n",
    "    X_dict, y = context_dictorizer.transform(sent_dict, \n",
    "                                             training_step=False)\n",
    "    # print('Sentence, padded:', X_dict)\n",
    "    # print('NER, y:', y)\n",
    "    \n",
    "    X_num = dict_vect.transform(X_dict)\n",
    "    if SCALER:\n",
    "    # We standardize X_num\n",
    "        X = scaler.transform(X_num)\n",
    "    else:\n",
    "        X = X_num\n",
    "\n",
    "    # print(X)\n",
    "    y_prob = model.predict(X)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    y_pred_cat = [ner_rev_idx[i] for i in y_pred]   # XXXXXX\n",
    "    \n",
    "    return y_pred_cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sentence in sentences:\n",
    "    y_test_pred_cat = predict_sentence(sentence.lower(), \n",
    "                                       model, word_idx, \n",
    "                                       vocabulary_words, \n",
    "                                       rev_ner_idx)\n",
    "    print(sentence)\n",
    "    print(y_test_pred_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences = [\"That round table might collapse .\",\n",
    "                 \"The man can learn well .\",\n",
    "                 \"The man can swim .\",\n",
    "                 \"The man can simwo .\"]\n",
    "for sentence in sentences:\n",
    "    y_test_pred_cat = predict_sentence(sentence.lower(), \n",
    "                                       dict_vectorizer,\n",
    "                                       model,  \n",
    "                                       ner_rev_idx)\n",
    "    #print(sentence)\n",
    "    #print(y_test_pred_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That round table might collapse .\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "The man can learn well .\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "The man can swim .\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "The man can simwo .\n",
      "['O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"That round table might collapse .\",\n",
    "                 \"The man can learn well .\",\n",
    "                 \"The man can swim .\",\n",
    "                 \"The man can simwo .\"]\n",
    "for sentence in sentences:\n",
    "    y_test_pred_cat = predict_sentence(sentence.lower(), \n",
    "                                       model, word_idx, \n",
    "                                       vocabulary_words, \n",
    "                                       ner_rev_idx)\n",
    "    print(sentence)\n",
    "    print(y_test_pred_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- -X- O\n",
      "\n",
      "SOCCER NN I-NP O\n",
      "- : O O\n",
      "JAP\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Tokenize the data ---------- Chollet 6.1\n",
    "\n",
    "Let's vectorize the texts we collected, and prepare a training and validation split. We will merely be using the concepts we introduced earlier in this section.\n",
    "\n",
    "Because pre-trained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, task-specific embeddings are likely to outperform them), we will add the following twist: we restrict the training data to its first 200 samples. So we will be learning to classify movie reviews after looking at just 200 examples...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build an embedding matrix that we will be able to load into an Embedding layer. \n",
    "It must be a matrix of shape (max_words, embedding_dim), where each entry i contains the embedding_dim-dimensional vector for the word of index i in our reference word index (built during tokenization). Note that the index 0 is not supposed to stand for any word or token -- it's a placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Download the GloVe embeddings from https://nlp.stanford.edu/projects/glove/ and keep the 100d vectors.\n",
    "\n",
    "2 - Write a function that reads GloVe embeddings and store them in a dictionary, where the keys will be the words and the values, the embeddings.\n",
    "\n",
    "3 - Using a cosine similarity, compute the 5 closest words to the word table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Reading the Corpus and Building Indices\n",
    "\n",
    "You will read the corpus with programs available from https://github.com/pnugues/edan95. These programs will enable you to load the files in the form of a list of dictionaries.\n",
    "\n",
    "1 - Write a function that extracts the words and NER tags and returns X and Y list of symbols.\n",
    "\n",
    "2 - Create indices and inverted indices for the words and the NER: i.e. you will associate each word with a number. The words will be the set of all the words observed in the training set and the words in GloVe. You will use index 0 for the padding symbol and 1 for unknown words. (see Chollet page 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Building the Embedding Matrix\n",
    "\n",
    "4 - Create a matrix whose size will be that of all the words. Initialize it with random values.\n",
    "\n",
    "5 - Fill the matrix with the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Creating the X and Y Sequences\n",
    "\n",
    "You will now create the input and output sequences with numerical indices\n",
    "\n",
    "1 - Convert the X and Y list of symbols in a list of numbers using the indices you created.\n",
    "\n",
    "2 - Pad the sentences using the pad_sequences function.\n",
    "\n",
    "3 - Do the same for the development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building a Simple Recurrent Neural Network\n",
    "\n",
    "1 - Create a simple recurrent network and train a model with the train set. As layers, you will use Embedding, SimpleRNN, and Dense.\n",
    "\n",
    "2 - Compile and fit your network. You will report the training and validation losses and accuracies and comment on the possible overfit.\n",
    "\n",
    "3 - Apply your network to the test set and report the accuracy as well as the confusion matrix you obtained. You will use the evaluate method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Evaluating your System\n",
    "\n",
    "You will use the official script to evaluate the performance of your system\n",
    "\n",
    "1 - Use the predict method to predict the tags of the whole test set\n",
    "\n",
    "2 - Write your results in a file, where the two last columns will be the hand-annotated tag and the predicted tag. The fields must be separated by a space.\n",
    "\n",
    "3 - Apply conlleval to your output. Report the F1 result.\n",
    "\n",
    "4 - Try to improve your model by modifying some parameters, adding layers, adding Bidirectional and Dropout.\n",
    "\n",
    "5 - Evaluate your network again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Building a LSTM Network\n",
    "\n",
    "1 - Create a simple LSTM network and train a model with the train set. As layers, you will use Embedding, LSTM, and Dense.\n",
    "\n",
    "2 - Apply conlleval to your output. Report the F1 result.\n",
    "\n",
    "3 - Try to improve your model by modifying some parameters, adding layers, adding Bidirectional, Dropout, possibly mixing SimpleRNN.\n",
    "\n",
    "4 - Apply your network to the test set and report the accuracy as well as the confusion matrix you obtained. you need to reach a F1 of 84 to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

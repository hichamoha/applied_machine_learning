{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging using Feedforward Networks and Embeddings\n",
    "Author: Pierre Nugues\n",
    "\n",
    "A part-of-speech tagger using feed-forward networks and GloVe embeddings and trained on a corpus following the Universal Dependencies format. Here we use the English Web Treebank:\n",
    "https://github.com/UniversalDependencies/UD_English-EWT/tree/master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "from keras import models, layers\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'rmsprop'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 4\n",
    "MINI_CORPUS = False\n",
    "EMBEDDING_DIM = 100\n",
    "W_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Embeddings\n",
    "We will use GloVe embeddings and load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    glove = open(file)\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    embeddings_dict = embeddings\n",
    "    embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/lab4/'\n",
    "embedding_file = BASE_DIR + 'glove.6B/glove.6B.100d.txt'\n",
    "embeddings_dict = load(embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61454  ,  0.89693  ,  0.56771  ,  0.39102  , -0.22437  ,\n",
       "        0.49035  ,  0.10868  ,  0.27411  , -0.23833  , -0.52153  ,\n",
       "        0.73551  , -0.32654  ,  0.51304  ,  0.32415  , -0.46709  ,\n",
       "        0.68051  , -0.25497  , -0.040484 , -0.54418  , -1.0548   ,\n",
       "       -0.46692  ,  0.23557  ,  0.31234  , -0.34537  ,  0.14793  ,\n",
       "       -0.53745  , -0.43215  , -0.48724  , -0.51019  , -0.9051   ,\n",
       "       -0.17919  , -0.018376 ,  0.09719  , -0.31623  ,  0.7512   ,\n",
       "        0.92236  , -0.49965  ,  0.14036  , -0.28296  , -0.97443  ,\n",
       "       -0.0094408, -0.62944  ,  0.14711  , -0.94376  ,  0.0075222,\n",
       "        0.18565  , -0.99172  ,  0.072789 , -0.18474  , -0.52901  ,\n",
       "        0.38995  , -0.45677  , -0.21932  ,  1.3723   , -0.29636  ,\n",
       "       -2.2342   , -0.36667  ,  0.04987  ,  0.63421  ,  0.53275  ,\n",
       "       -0.53955  ,  0.31398  , -0.44698  , -0.38389  ,  0.066668 ,\n",
       "       -0.02168  ,  0.20558  ,  0.59456  , -0.24892  , -0.52795  ,\n",
       "       -0.3761   ,  0.077104 ,  0.75222  , -0.2647   , -0.0587   ,\n",
       "        0.67541  , -0.16559  , -0.49278  , -0.26327  , -0.21215  ,\n",
       "        0.24317  ,  0.17006  , -0.2926   , -0.5009   , -0.56638  ,\n",
       "       -0.40377  , -0.48452  , -0.32539  ,  0.75293  ,  0.0049585,\n",
       "       -0.32115  ,  0.28899  , -0.042392 ,  0.63863  , -0.20332  ,\n",
       "       -0.46785  , -0.15661  ,  0.2179   ,  1.4143   ,  0.40034  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# newdoc id = weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000\\n# sent_id = weblog-jua'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/lab4/'\n",
    "\n",
    "def load_ud_en_ewt():\n",
    "\n",
    "    train_file = 'ud_en/en_ewt-ud-train.conllu'\n",
    "    dev_file = 'ud_en/en_ewt-ud-dev.conllu'\n",
    "    test_file = 'ud_en/en_ewt-ud-test.conllu'\n",
    "    column_names = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', \n",
    "                    'FEATS', 'HEAD', 'DEPREL', 'HEAD', 'DEPS', 'MISC']\n",
    "    column_names = list(map(str.lower, column_names))\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-train-pos.txt'\n",
    "    dev_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-development-pos.txt'\n",
    "    test_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-test-words-pos.txt'\n",
    "    # test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "# train_sentences, dev_sentences, test_sentences, column_names = \\\n",
    "# load_conll2009_pos()\n",
    "train_sentences, dev_sentences, test_sentences, column_names =\\\n",
    "load_ud_en_ewt()\n",
    "train_sentences[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Corpus in a Dictionary\n",
    "We follow the fit-transform pattern of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        rows = [row for row in rows if row[0] != '#']\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, train: [{'id': '1', 'form': 'Al', 'lemma': 'Al', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '0:root', 'deprel': 'root', 'deps': 'SpaceAfter=No'}, {'id': '2', 'form': '-', 'lemma': '-', 'upos': 'PUNCT', 'xpos': 'HYPH', 'feats': '_', 'head': '1:punct', 'deprel': 'punct', 'deps': 'SpaceAfter=No'}, {'id': '3', 'form': 'Zaman', 'lemma': 'Zaman', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '1:flat', 'deprel': 'flat', 'deps': '_'}, {'id': '4', 'form': ':', 'lemma': ':', 'upos': 'PUNCT', 'xpos': ':', 'feats': '_', 'head': '1:punct', 'deprel': 'punct', 'deps': '_'}, {'id': '5', 'form': 'American', 'lemma': 'american', 'upos': 'ADJ', 'xpos': 'JJ', 'feats': 'Degree=Pos', 'head': '6:amod', 'deprel': 'amod', 'deps': '_'}, {'id': '6', 'form': 'forces', 'lemma': 'force', 'upos': 'NOUN', 'xpos': 'NNS', 'feats': 'Number=Plur', 'head': '7:nsubj', 'deprel': 'nsubj', 'deps': '_'}, {'id': '7', 'form': 'killed', 'lemma': 'kill', 'upos': 'VERB', 'xpos': 'VBD', 'feats': 'Mood=Ind|Tense=Past|VerbForm=Fin', 'head': '1:parataxis', 'deprel': 'parataxis', 'deps': '_'}, {'id': '8', 'form': 'Shaikh', 'lemma': 'Shaikh', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '7:obj', 'deprel': 'obj', 'deps': '_'}, {'id': '9', 'form': 'Abdullah', 'lemma': 'Abdullah', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8:flat', 'deprel': 'flat', 'deps': '_'}, {'id': '10', 'form': 'al', 'lemma': 'al', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8:flat', 'deprel': 'flat', 'deps': 'SpaceAfter=No'}, {'id': '11', 'form': '-', 'lemma': '-', 'upos': 'PUNCT', 'xpos': 'HYPH', 'feats': '_', 'head': '8:punct', 'deprel': 'punct', 'deps': 'SpaceAfter=No'}, {'id': '12', 'form': 'Ani', 'lemma': 'Ani', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8:flat', 'deprel': 'flat', 'deps': 'SpaceAfter=No'}, {'id': '13', 'form': ',', 'lemma': ',', 'upos': 'PUNCT', 'xpos': ',', 'feats': '_', 'head': '8:punct', 'deprel': 'punct', 'deps': '_'}, {'id': '14', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '15:det', 'deprel': 'det', 'deps': '_'}, {'id': '15', 'form': 'preacher', 'lemma': 'preacher', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '8:appos', 'deprel': 'appos', 'deps': '_'}, {'id': '16', 'form': 'at', 'lemma': 'at', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '18:case', 'deprel': 'case', 'deps': '_'}, {'id': '17', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '18:det', 'deprel': 'det', 'deps': '_'}, {'id': '18', 'form': 'mosque', 'lemma': 'mosque', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '7:obl:at', 'deprel': 'obl', 'deps': '_'}, {'id': '19', 'form': 'in', 'lemma': 'in', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '21:case', 'deprel': 'case', 'deps': '_'}, {'id': '20', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '21:det', 'deprel': 'det', 'deps': '_'}, {'id': '21', 'form': 'town', 'lemma': 'town', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '18:nmod:in', 'deprel': 'nmod', 'deps': '_'}, {'id': '22', 'form': 'of', 'lemma': 'of', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '23:case', 'deprel': 'case', 'deps': '_'}, {'id': '23', 'form': 'Qaim', 'lemma': 'Qaim', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '21:nmod:of', 'deprel': 'nmod', 'deps': 'SpaceAfter=No'}, {'id': '24', 'form': ',', 'lemma': ',', 'upos': 'PUNCT', 'xpos': ',', 'feats': '_', 'head': '21:punct', 'deprel': 'punct', 'deps': '_'}, {'id': '25', 'form': 'near', 'lemma': 'near', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '28:case', 'deprel': 'case', 'deps': '_'}, {'id': '26', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '28:det', 'deprel': 'det', 'deps': '_'}, {'id': '27', 'form': 'Syrian', 'lemma': 'syrian', 'upos': 'ADJ', 'xpos': 'JJ', 'feats': 'Degree=Pos', 'head': '28:amod', 'deprel': 'amod', 'deps': '_'}, {'id': '28', 'form': 'border', 'lemma': 'border', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '21:nmod:near', 'deprel': 'nmod', 'deps': 'SpaceAfter=No'}, {'id': '29', 'form': '.', 'lemma': '.', 'upos': 'PUNCT', 'xpos': '.', 'feats': '_', 'head': '1:punct', 'deprel': 'punct', 'deps': '_'}]\n"
     ]
    }
   ],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "if MINI_CORPUS:\n",
    "    train_dict = train_dict[:len(train_dict) // 15]\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "print('First sentence, train:', train_dict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Context and Dictorizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDictorizer():\n",
    "    \"\"\"\n",
    "    Extract contexts of words in a sequence\n",
    "    Contexts are of w_size to the left and to the right\n",
    "    Builds an X matrix in the form of a dictionary\n",
    "    and possibly extracts the output, y, if not in the test step\n",
    "    If the test_step is True, returns y = []\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input='form', output='upos', w_size=2, tolower=True):\n",
    "        self.BOS_symbol = '__BOS__'\n",
    "        self.EOS_symbol = '__EOS__'\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.w_size = w_size\n",
    "        self.tolower = tolower\n",
    "        # This was not correct as the names were not sorted\n",
    "        # self.feature_names = [input + '_' + str(i)\n",
    "        #                     for i in range(-w_size, w_size + 1)]\n",
    "        # To be sure the names are ordered\n",
    "        zeros = math.ceil(math.log10(2 * w_size + 1))\n",
    "        self.feature_names = [input + '_' + str(i).zfill(zeros) for \n",
    "                              i in range(2 * w_size + 1)]\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"\n",
    "        Build the padding rows\n",
    "        :param sentences:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.column_names = sentences[0][0].keys()\n",
    "        start = [self.BOS_symbol] * len(self.column_names)\n",
    "        end = [self.EOS_symbol] * len(self.column_names)\n",
    "        start_token = Token(dict(zip(self.column_names, start)))\n",
    "        end_token = Token(dict(zip(self.column_names, end)))\n",
    "        self.start_rows = [start_token] * self.w_size\n",
    "        self.end_rows = [end_token] * self.w_size\n",
    "\n",
    "    def transform(self, sentences, training_step=True):\n",
    "        X_corpus = []\n",
    "        y_corpus = []\n",
    "        for sentence in sentences:\n",
    "            X, y = self._transform_sentence(sentence, training_step)\n",
    "            X_corpus += X\n",
    "            if training_step:\n",
    "                y_corpus += y\n",
    "        return X_corpus, y_corpus\n",
    "\n",
    "    def fit_transform(self, sentences):\n",
    "        self.fit(sentences)\n",
    "        return self.transform(sentences)\n",
    "\n",
    "    def _transform_sentence(self, sentence, training_step=True):\n",
    "        # We extract y\n",
    "        if training_step:\n",
    "            y = [row[self.output] for row in sentence]\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        # We pad the sentence\n",
    "        sentence = self.start_rows + sentence + self.end_rows\n",
    "\n",
    "        # We extract the features\n",
    "        X = list()\n",
    "        for i in range(len(sentence) - 2 * self.w_size):\n",
    "            # x is a row of X\n",
    "            x = list()\n",
    "            # The words in lower case\n",
    "            for j in range(2 * self.w_size + 1):\n",
    "                if self.tolower:\n",
    "                    x.append(sentence[i + j][self.input].lower())\n",
    "                else:\n",
    "                    x.append(sentence[i + j][self.input])\n",
    "            # We represent the feature vector as a dictionary\n",
    "            X.append(dict(zip(self.feature_names, x)))\n",
    "        return X, y\n",
    "\n",
    "    def print_example(self, sentences, id=1968):\n",
    "        \"\"\"\n",
    "        :param corpus:\n",
    "        :param id:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # We print the features to check they match Table 8.1 in my book (second edition)\n",
    "        # We use the training step extraction with the dynamic features\n",
    "        Xs, ys = self._transform_sentence(sentences[id])\n",
    "        print('X for sentence #', id, Xs)\n",
    "        print('y for sentence #', id, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dictorizer = ContextDictorizer()\n",
    "context_dictorizer.fit(train_dict)\n",
    "X_dict, y_cat = context_dictorizer.transform(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X for sentence # 1968 [{'form_0': '__bos__', 'form_1': '__bos__', 'form_2': 'do', 'form_3': 'we', 'form_4': 'intend'}, {'form_0': '__bos__', 'form_1': 'do', 'form_2': 'we', 'form_3': 'intend', 'form_4': 'to'}, {'form_0': 'do', 'form_1': 'we', 'form_2': 'intend', 'form_3': 'to', 'form_4': 'reference'}, {'form_0': 'we', 'form_1': 'intend', 'form_2': 'to', 'form_3': 'reference', 'form_4': 'a'}, {'form_0': 'intend', 'form_1': 'to', 'form_2': 'reference', 'form_3': 'a', 'form_4': 'particular'}, {'form_0': 'to', 'form_1': 'reference', 'form_2': 'a', 'form_3': 'particular', 'form_4': 'manufacturer'}, {'form_0': 'reference', 'form_1': 'a', 'form_2': 'particular', 'form_3': 'manufacturer', 'form_4': ','}, {'form_0': 'a', 'form_1': 'particular', 'form_2': 'manufacturer', 'form_3': ',', 'form_4': 'or'}, {'form_0': 'particular', 'form_1': 'manufacturer', 'form_2': ',', 'form_3': 'or', 'form_4': 'should'}, {'form_0': 'manufacturer', 'form_1': ',', 'form_2': 'or', 'form_3': 'should', 'form_4': 'this'}, {'form_0': ',', 'form_1': 'or', 'form_2': 'should', 'form_3': 'this', 'form_4': 'be'}, {'form_0': 'or', 'form_1': 'should', 'form_2': 'this', 'form_3': 'be', 'form_4': 'more'}, {'form_0': 'should', 'form_1': 'this', 'form_2': 'be', 'form_3': 'more', 'form_4': 'generic'}, {'form_0': 'this', 'form_1': 'be', 'form_2': 'more', 'form_3': 'generic', 'form_4': '?'}, {'form_0': 'be', 'form_1': 'more', 'form_2': 'generic', 'form_3': '?', 'form_4': '__eos__'}, {'form_0': 'more', 'form_1': 'generic', 'form_2': '?', 'form_3': '__eos__', 'form_4': '__eos__'}]\n",
      "y for sentence # 1968 ['AUX', 'PRON', 'VERB', 'PART', 'VERB', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'CCONJ', 'AUX', 'PRON', 'AUX', 'ADV', 'ADJ', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "context_dictorizer.print_example(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We extract all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique words seen in training corpus: 16655\n"
     ]
    }
   ],
   "source": [
    "corpus_words = []\n",
    "for x in X_dict:\n",
    "    corpus_words.extend(list(x.values()))\n",
    "corpus_words = sorted(list(set(corpus_words)))\n",
    "print('# unique words seen in training corpus:', len(corpus_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We add these words to the vocabulary\n",
    "We add one word to the count for the unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in GloVe: 400000\n",
      "# unique words in the vocabulary: embeddings and corpus: 401594\n"
     ]
    }
   ],
   "source": [
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "vocabulary_words = set(corpus_words + list(embeddings_words))\n",
    "cnt_uniq = len(vocabulary_words) + 1\n",
    "print('# unique words in the vocabulary: embeddings and corpus:', \n",
    "      cnt_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now build an index\n",
    "We keep index 0 for the unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rev_idx = dict(enumerate(vocabulary_words, start=1))\n",
    "word_idx = {v: k for k, v in word_rev_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We replace the words with their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ee1bcac23681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdict_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_idx' is not defined"
     ]
    }
   ],
   "source": [
    "for x_dict in X_dict:\n",
    "    for word in x_dict:\n",
    "        x_dict[word] = word_idx[x_dict[word]]\n",
    "dict_vect = DictVectorizer()\n",
    "X = dict_vect.fit_transform(X_dict)\n",
    "\n",
    "print('X shape', X.shape)\n",
    "print('First line of X:', X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The POS and the number of different POS\n",
    "pos_list = sorted(list(set(y_cat)))\n",
    "NB_CLASSES = len(pos_list) + 1\n",
    "\n",
    "# We build a part-of-speech index. We keep 0 for unknown symbols in the test set\n",
    "pos_rev_idx = dict(enumerate(pos_list, start=1))\n",
    "pos_idx = {v: k for k, v in pos_rev_idx.items()}\n",
    "\n",
    "# We encode y\n",
    "y = [pos_idx[i] for i in y_cat]\n",
    "print(y_cat[:10])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now create the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(vocabulary_words) + 1, \n",
    "                                     EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(cnt_uniq, EMBEDDING_DIM, \n",
    "                                input_length=2 * W_SIZE + 1))\n",
    "if embedding_matrix is not None:\n",
    "    model.layers[0].set_weights([embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NB_CLASSES, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                   optimizer=OPTIMIZER,\n",
    "                   metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dict, y_test_cat = context_dictorizer.transform(test_dict)\n",
    "for x_dict_test in X_test_dict:\n",
    "    for word in x_dict_test:\n",
    "        x_dict_test[word] = word_idx.get(x_dict_test[word], 0)\n",
    "            \n",
    "# We transform the symbols into numbers\n",
    "X_test = dict_vect.transform(X_test_dict)\n",
    "y_test = [pos_idx.get(i, 0) for i in y_test_cat]\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Optimizer', OPTIMIZER, 'Epochs', EPOCHS, 'Batch size', \n",
    "      BATCH_SIZE, 'Mini corpus', MINI_CORPUS)\n",
    "print('Loss:', test_loss)\n",
    "print('Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, dict_vect, model, word_idx, pos_rev_idx):\n",
    "    column_names = ['id', 'form']\n",
    "    sentence = list(enumerate(sentence.lower().split(), start=1))\n",
    "    conll_cols = ''\n",
    "    for tuple in sentence:\n",
    "        conll_cols += str(tuple[0]) + '\\t' + tuple[1] + '\\n'\n",
    "    # print(conll_cols)\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    sent_dict = conll_dict.transform(conll_cols)\n",
    "    # print('Sentence:', sent_dict[0])\n",
    "\n",
    "    context_dictorizer = ContextDictorizer()\n",
    "    context_dictorizer.fit(sent_dict)\n",
    "    X_dict, y = context_dictorizer.transform(sent_dict, \n",
    "                                             training_step=False)\n",
    "    # print('Sentence, padded:', X_dict)\n",
    "    # print('POS, y:', y)\n",
    "\n",
    "    for x_dict in X_dict:\n",
    "        for word in x_dict:\n",
    "            x_dict[word] = word_idx.get(x_dict[word], 1)\n",
    "    X = dict_vect.transform(X_dict)\n",
    "\n",
    "    # print(X)\n",
    "    y_prob = model.predict(X)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    y_pred_cat = [pos_rev_idx[i] for i in y_pred]\n",
    "    return y_pred_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"That round table might collapse .\",\n",
    "                 \"The man can learn well .\",\n",
    "                 \"The man can swim .\",\n",
    "                 \"The man can simwo .\"]\n",
    "for sentence in sentences:\n",
    "    y_test_pred_cat = predict_sentence(sentence.lower(), \n",
    "                                       dict_vect,\n",
    "                                       model, word_idx, \n",
    "                                       pos_rev_idx)\n",
    "    print(sentence)\n",
    "    print(y_test_pred_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

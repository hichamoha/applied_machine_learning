{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging using Feedforward Networks\n",
    "\n",
    "Author: Pierre Nugues\n",
    "A part-of-speech tagger using feed-forward networks and trained on a corpus following the Universal Dependencies format. Here we use the English Web Treebank:\n",
    "https://github.com/UniversalDependencies/UD_English-EWT/tree/master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "from keras import models, layers\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'rmsprop'\n",
    "SCALER = True\n",
    "SIMPLE_MODEL = False\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "MINI_CORPUS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# newdoc id = weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000\\n# sent_id = weblog-jua'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/lab4/'\n",
    "\n",
    "def load_ud_en_ewt():\n",
    "\n",
    "    train_file = BASE_DIR + 'ud_en/en_ewt-ud-train.conllu'\n",
    "    dev_file = BASE_DIR + 'ud_en/en_ewt-ud-dev.conllu'\n",
    "    test_file = BASE_DIR + 'ud_en/en_ewt-ud-test.conllu'\n",
    "    column_names = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', \n",
    "                    'FEATS', 'HEAD', 'DEPREL', 'HEAD', 'DEPS', 'MISC']\n",
    "    column_names = list(map(str.lower, column_names))\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-train-pos.txt'\n",
    "    dev_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-development-pos.txt'\n",
    "    test_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-test-words-pos.txt'\n",
    "    # test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "#train_sentences, dev_sentences, test_sentences, column_names =\\\n",
    "#load_conll2009_pos()\n",
    "train_sentences, dev_sentences, test_sentences, column_names =\\\n",
    "load_ud_en_ewt()\n",
    "train_sentences[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Corpus in a Dictionary\n",
    "We follow the fit-transform pattern of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        rows = [row for row in rows if row[0] != '#']\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, train: [{'id': '1', 'form': 'Al', 'lemma': 'Al', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '0:root', 'deprel': 'root', 'deps': 'SpaceAfter=No'}, {'id': '2', 'form': '-', 'lemma': '-', 'upos': 'PUNCT', 'xpos': 'HYPH', 'feats': '_', 'head': '1:punct', 'deprel': 'punct', 'deps': 'SpaceAfter=No'}, {'id': '3', 'form': 'Zaman', 'lemma': 'Zaman', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '1:flat', 'deprel': 'flat', 'deps': '_'}, {'id': '4', 'form': ':', 'lemma': ':', 'upos': 'PUNCT', 'xpos': ':', 'feats': '_', 'head': '1:punct', 'deprel': 'punct', 'deps': '_'}, {'id': '5', 'form': 'American', 'lemma': 'american', 'upos': 'ADJ', 'xpos': 'JJ', 'feats': 'Degree=Pos', 'head': '6:amod', 'deprel': 'amod', 'deps': '_'}, {'id': '6', 'form': 'forces', 'lemma': 'force', 'upos': 'NOUN', 'xpos': 'NNS', 'feats': 'Number=Plur', 'head': '7:nsubj', 'deprel': 'nsubj', 'deps': '_'}, {'id': '7', 'form': 'killed', 'lemma': 'kill', 'upos': 'VERB', 'xpos': 'VBD', 'feats': 'Mood=Ind|Tense=Past|VerbForm=Fin', 'head': '1:parataxis', 'deprel': 'parataxis', 'deps': '_'}, {'id': '8', 'form': 'Shaikh', 'lemma': 'Shaikh', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '7:obj', 'deprel': 'obj', 'deps': '_'}, {'id': '9', 'form': 'Abdullah', 'lemma': 'Abdullah', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8:flat', 'deprel': 'flat', 'deps': '_'}, {'id': '10', 'form': 'al', 'lemma': 'al', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8:flat', 'deprel': 'flat', 'deps': 'SpaceAfter=No'}, {'id': '11', 'form': '-', 'lemma': '-', 'upos': 'PUNCT', 'xpos': 'HYPH', 'feats': '_', 'head': '8:punct', 'deprel': 'punct', 'deps': 'SpaceAfter=No'}, {'id': '12', 'form': 'Ani', 'lemma': 'Ani', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '8:flat', 'deprel': 'flat', 'deps': 'SpaceAfter=No'}, {'id': '13', 'form': ',', 'lemma': ',', 'upos': 'PUNCT', 'xpos': ',', 'feats': '_', 'head': '8:punct', 'deprel': 'punct', 'deps': '_'}, {'id': '14', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '15:det', 'deprel': 'det', 'deps': '_'}, {'id': '15', 'form': 'preacher', 'lemma': 'preacher', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '8:appos', 'deprel': 'appos', 'deps': '_'}, {'id': '16', 'form': 'at', 'lemma': 'at', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '18:case', 'deprel': 'case', 'deps': '_'}, {'id': '17', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '18:det', 'deprel': 'det', 'deps': '_'}, {'id': '18', 'form': 'mosque', 'lemma': 'mosque', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '7:obl:at', 'deprel': 'obl', 'deps': '_'}, {'id': '19', 'form': 'in', 'lemma': 'in', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '21:case', 'deprel': 'case', 'deps': '_'}, {'id': '20', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '21:det', 'deprel': 'det', 'deps': '_'}, {'id': '21', 'form': 'town', 'lemma': 'town', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '18:nmod:in', 'deprel': 'nmod', 'deps': '_'}, {'id': '22', 'form': 'of', 'lemma': 'of', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '23:case', 'deprel': 'case', 'deps': '_'}, {'id': '23', 'form': 'Qaim', 'lemma': 'Qaim', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Number=Sing', 'head': '21:nmod:of', 'deprel': 'nmod', 'deps': 'SpaceAfter=No'}, {'id': '24', 'form': ',', 'lemma': ',', 'upos': 'PUNCT', 'xpos': ',', 'feats': '_', 'head': '21:punct', 'deprel': 'punct', 'deps': '_'}, {'id': '25', 'form': 'near', 'lemma': 'near', 'upos': 'ADP', 'xpos': 'IN', 'feats': '_', 'head': '28:case', 'deprel': 'case', 'deps': '_'}, {'id': '26', 'form': 'the', 'lemma': 'the', 'upos': 'DET', 'xpos': 'DT', 'feats': 'Definite=Def|PronType=Art', 'head': '28:det', 'deprel': 'det', 'deps': '_'}, {'id': '27', 'form': 'Syrian', 'lemma': 'syrian', 'upos': 'ADJ', 'xpos': 'JJ', 'feats': 'Degree=Pos', 'head': '28:amod', 'deprel': 'amod', 'deps': '_'}, {'id': '28', 'form': 'border', 'lemma': 'border', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'head': '21:nmod:near', 'deprel': 'nmod', 'deps': 'SpaceAfter=No'}, {'id': '29', 'form': '.', 'lemma': '.', 'upos': 'PUNCT', 'xpos': '.', 'feats': '_', 'head': '1:punct', 'deprel': 'punct', 'deps': '_'}]\n"
     ]
    }
   ],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "if MINI_CORPUS:\n",
    "    train_dict = train_dict[:len(train_dict) // 5]\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "print('First sentence, train:', train_dict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Context and Dictorizing it\n",
    "We extract windows of five words surrounding the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDictorizer():\n",
    "    \"\"\"\n",
    "    Extract contexts of words in a sequence\n",
    "    Contexts are of w_size to the left and to the right\n",
    "    Builds an X matrix in the form of a dictionary\n",
    "    and possibly extracts the output, y, if not in the test step\n",
    "    If the test_step is True, returns y = []\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input='form', output='upos', w_size=2, tolower=True):\n",
    "        self.BOS_symbol = '__BOS__'\n",
    "        self.EOS_symbol = '__EOS__'\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.w_size = w_size\n",
    "        self.tolower = tolower\n",
    "        # This was not correct as the names were not sorted\n",
    "        # self.feature_names = [input + '_' + str(i)\n",
    "        #                     for i in range(-w_size, w_size + 1)]\n",
    "        # To be sure the names are ordered\n",
    "        zeros = math.ceil(math.log10(2 * w_size + 1))\n",
    "        self.feature_names = [input + '_' + str(i).zfill(zeros) for \n",
    "                              i in range(2 * w_size + 1)]\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"\n",
    "        Build the padding rows\n",
    "        :param sentences:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.column_names = sentences[0][0].keys()\n",
    "        start = [self.BOS_symbol] * len(self.column_names)\n",
    "        end = [self.EOS_symbol] * len(self.column_names)\n",
    "        start_token = Token(dict(zip(self.column_names, start)))\n",
    "        end_token = Token(dict(zip(self.column_names, end)))\n",
    "        self.start_rows = [start_token] * self.w_size\n",
    "        self.end_rows = [end_token] * self.w_size\n",
    "\n",
    "    def transform(self, sentences, training_step=True):\n",
    "        X_corpus = []\n",
    "        y_corpus = []\n",
    "        for sentence in sentences:\n",
    "            X, y = self._transform_sentence(sentence, training_step)\n",
    "            X_corpus += X\n",
    "            if training_step:\n",
    "                y_corpus += y\n",
    "        return X_corpus, y_corpus\n",
    "\n",
    "    def fit_transform(self, sentences):\n",
    "        self.fit(sentences)\n",
    "        return self.transform(sentences)\n",
    "\n",
    "    def _transform_sentence(self, sentence, training_step=True):\n",
    "        # We extract y\n",
    "        if training_step:\n",
    "            y = [row[self.output] for row in sentence]\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        # We pad the sentence\n",
    "        sentence = self.start_rows + sentence + self.end_rows\n",
    "\n",
    "        # We extract the features\n",
    "        X = list()\n",
    "        for i in range(len(sentence) - 2 * self.w_size):\n",
    "            # x is a row of X\n",
    "            x = list()\n",
    "            # The words in lower case\n",
    "            for j in range(2 * self.w_size + 1):\n",
    "                if self.tolower:\n",
    "                    x.append(sentence[i + j][self.input].lower())\n",
    "                else:\n",
    "                    x.append(sentence[i + j][self.input])\n",
    "            # We represent the feature vector as a dictionary\n",
    "            X.append(dict(zip(self.feature_names, x)))\n",
    "        return X, y\n",
    "\n",
    "    def print_example(self, sentences, id=1968):\n",
    "        \"\"\"\n",
    "        :param corpus:\n",
    "        :param id:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # We print the features to check they match Table 8.1 in my book (second edition)\n",
    "        # We use the training step extraction with the dynamic features\n",
    "        Xs, ys = self._transform_sentence(sentences[id])\n",
    "        print('X for sentence #', id, Xs)\n",
    "        print('y for sentence #', id, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dictorizer = ContextDictorizer()\n",
    "context_dictorizer.fit(train_dict)\n",
    "X_dict, y_cat = context_dictorizer.transform(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X for sentence # 1968 [{'form_0': '__bos__', 'form_1': '__bos__', 'form_2': 'do', 'form_3': 'we', 'form_4': 'intend'}, {'form_0': '__bos__', 'form_1': 'do', 'form_2': 'we', 'form_3': 'intend', 'form_4': 'to'}, {'form_0': 'do', 'form_1': 'we', 'form_2': 'intend', 'form_3': 'to', 'form_4': 'reference'}, {'form_0': 'we', 'form_1': 'intend', 'form_2': 'to', 'form_3': 'reference', 'form_4': 'a'}, {'form_0': 'intend', 'form_1': 'to', 'form_2': 'reference', 'form_3': 'a', 'form_4': 'particular'}, {'form_0': 'to', 'form_1': 'reference', 'form_2': 'a', 'form_3': 'particular', 'form_4': 'manufacturer'}, {'form_0': 'reference', 'form_1': 'a', 'form_2': 'particular', 'form_3': 'manufacturer', 'form_4': ','}, {'form_0': 'a', 'form_1': 'particular', 'form_2': 'manufacturer', 'form_3': ',', 'form_4': 'or'}, {'form_0': 'particular', 'form_1': 'manufacturer', 'form_2': ',', 'form_3': 'or', 'form_4': 'should'}, {'form_0': 'manufacturer', 'form_1': ',', 'form_2': 'or', 'form_3': 'should', 'form_4': 'this'}, {'form_0': ',', 'form_1': 'or', 'form_2': 'should', 'form_3': 'this', 'form_4': 'be'}, {'form_0': 'or', 'form_1': 'should', 'form_2': 'this', 'form_3': 'be', 'form_4': 'more'}, {'form_0': 'should', 'form_1': 'this', 'form_2': 'be', 'form_3': 'more', 'form_4': 'generic'}, {'form_0': 'this', 'form_1': 'be', 'form_2': 'more', 'form_3': 'generic', 'form_4': '?'}, {'form_0': 'be', 'form_1': 'more', 'form_2': 'generic', 'form_3': '?', 'form_4': '__eos__'}, {'form_0': 'more', 'form_1': 'generic', 'form_2': '?', 'form_3': '__eos__', 'form_4': '__eos__'}]\n",
      "y for sentence # 1968 ['AUX', 'PRON', 'VERB', 'PART', 'VERB', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'CCONJ', 'AUX', 'PRON', 'AUX', 'ADV', 'ADJ', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "context_dictorizer.print_example(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the $\\mathbf{X}$ Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204607, 79388)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We transform the X symbols into numbers\n",
    "dict_vectorizer = DictVectorizer()\n",
    "X_num = dict_vectorizer.fit_transform(X_dict)\n",
    "\n",
    "if SCALER:\n",
    "    # We standardize X_num\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    X = scaler.fit_transform(X_num)\n",
    "else:\n",
    "    X = X_num\n",
    "    \n",
    "print(X.shape)\n",
    "X[0, :100].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The POS and the number of different POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PART',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'SYM',\n",
       " 'VERB',\n",
       " 'X']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = sorted(list(set(y_cat)))\n",
    "NB_CLASSES = len(pos_list) + 1\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a part-of-speech index. We keep 0 for unknown symbols in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'PUNCT': 13,\n",
       " 'SCONJ': 14,\n",
       " 'SYM': 15,\n",
       " 'VERB': 16,\n",
       " 'X': 17}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rev_idx = dict(enumerate(pos_list, start=1))\n",
    "pos_idx = {v: k for k, v in pos_rev_idx.items()}\n",
    "pos_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode $\\mathbf{y}$. We use one symbol for unknown parts of speech in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12, 13, 12, 13, 1, 8, 16, 12, 12, 12]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [pos_idx[i] for i in y_cat]\n",
    "print(y_cat[:10])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tagger\n",
    "   With `sparse_categorical_crossentropy` we do not need to vectorize $\\mathbf{y}$ with `to_categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 36)                2858004   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                666       \n",
      "=================================================================\n",
      "Total params: 2,858,670\n",
      "Trainable params: 2,858,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "if SIMPLE_MODEL:\n",
    "    model.add(layers.Dense(NB_CLASSES,\n",
    "                           input_dim=X.shape[1],\n",
    "                           activation='softmax'))\n",
    "else:\n",
    "    model.add(layers.Dense(NB_CLASSES * 2,\n",
    "                           input_dim=X.shape[1],\n",
    "                           activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(NB_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "204607/204607 [==============================] - 134s 655us/step - loss: 0.6892 - acc: 0.8082\n",
      "Epoch 2/2\n",
      "204607/204607 [==============================] - 134s 653us/step - loss: 0.1848 - acc: 0.9494\n",
      "Time: 4.4692875801833\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "# model.save('out.model')\n",
    "print('Time:', (time.perf_counter() - start_time) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'SCONJ', 'PROPN', 'VERB', 'ADP']\n",
      "[11, 14, 12, 16, 2]\n",
      "(25097, 79388)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test_cat = context_dictorizer.transform(test_dict)\n",
    "# We transform the symbols into numbers\n",
    "X_test_num = dict_vectorizer.transform(X_test_dict)\n",
    "if SCALER:\n",
    "    # We standardize X_num\n",
    "    X_test = scaler.transform(X_test_num)\n",
    "else:\n",
    "    X_test = X_test_num\n",
    "print(y_test_cat[:5])\n",
    "y_test = [pos_idx.get(i, 0) for i in y_test_cat]\n",
    "print(y_test[:5])\n",
    "print(X_test.shape)\n",
    "X_test[0, :100].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PRON'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pos_rev_idx[np.argmax(model.predict(X_test[0, :]))])\n",
    "pos_rev_idx[y_test[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25097/25097 [==============================] - 12s 463us/step\n",
      "Optimizer rmsprop Scaler True Epochs 2 Batch size 128 Simple model False Mini corpus False\n",
      "Loss: 0.34958647355961553\n",
      "Accuracy: 0.9041718133665221\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Optimizer', OPTIMIZER, 'Scaler', SCALER, 'Epochs', EPOCHS, 'Batch size', \n",
    "      BATCH_SIZE, 'Simple model', SIMPLE_MODEL, 'Mini corpus', MINI_CORPUS)\n",
    "print('Loss:', test_loss)\n",
    "print('Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, dict_vect, model, pos_rev_idx):\n",
    "    column_names = ['id', 'form']\n",
    "    sentence = list(enumerate(sentence.lower().split(), start=1))\n",
    "    conll_cols = ''\n",
    "    for tuple in sentence:\n",
    "        conll_cols += str(tuple[0]) + '\\t' + tuple[1] + '\\n'\n",
    "    # print(conll_cols)\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    sent_dict = conll_dict.transform(conll_cols)\n",
    "    # print('Sentence:', sent_dict[0])\n",
    "\n",
    "    context_dictorizer = ContextDictorizer()\n",
    "    context_dictorizer.fit(sent_dict)\n",
    "    X_dict, y = context_dictorizer.transform(sent_dict, \n",
    "                                             training_step=False)\n",
    "    # print('Sentence, padded:', X_dict)\n",
    "    # print('POS, y:', y)\n",
    "    \n",
    "    X_num = dict_vect.transform(X_dict)\n",
    "    if SCALER:\n",
    "    # We standardize X_num\n",
    "        X = scaler.transform(X_num)\n",
    "    else:\n",
    "        X = X_num\n",
    "\n",
    "    # print(X)\n",
    "    y_prob = model.predict(X)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    y_pred_cat = [pos_rev_idx[i] for i in y_pred]\n",
    "    return y_pred_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That round table might collapse .\n",
      "['PRON', 'ADV', 'NOUN', 'AUX', 'VERB', 'PUNCT']\n",
      "The man can learn well .\n",
      "['DET', 'NOUN', 'AUX', 'VERB', 'ADV', 'PUNCT']\n",
      "The man can swim .\n",
      "['DET', 'NOUN', 'AUX', 'VERB', 'PUNCT']\n",
      "The man can simwo .\n",
      "['DET', 'NOUN', 'AUX', 'VERB', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"That round table might collapse .\",\n",
    "                 \"The man can learn well .\",\n",
    "                 \"The man can swim .\",\n",
    "                 \"The man can simwo .\"]\n",
    "for sentence in sentences:\n",
    "    y_test_pred_cat = predict_sentence(sentence.lower(), \n",
    "                                       dict_vectorizer,\n",
    "                                       model,\n",
    "                                       pos_rev_idx)\n",
    "    print(sentence)\n",
    "    print(y_test_pred_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging using Recurrent Neural Networks (RNN)\n",
    "Author: Pierre Nugues\n",
    "\n",
    "A part-of-speech tagger using recurrent networks and GloVe embeddings and trained on a corpus following the Universal Dependencies format. Here we use the English Web Treebank:\n",
    "https://github.com/UniversalDependencies/UD_English-EWT/tree/master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "from keras import models, layers\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Bidirectional, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'rmsprop'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "LSTM_UNITS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Embeddings\n",
    "We will use GloVe embeddings and load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    glove = open(file)\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    embeddings_dict = embeddings\n",
    "    embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/lab4/'\n",
    "embedding_file = BASE_DIR + 'glove.6B/glove.6B.100d.txt'\n",
    "embeddings_dict = load(embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61454  ,  0.89693  ,  0.56771  ,  0.39102  , -0.22437  ,\n",
       "        0.49035  ,  0.10868  ,  0.27411  , -0.23833  , -0.52153  ,\n",
       "        0.73551  , -0.32654  ,  0.51304  ,  0.32415  , -0.46709  ,\n",
       "        0.68051  , -0.25497  , -0.040484 , -0.54418  , -1.0548   ,\n",
       "       -0.46692  ,  0.23557  ,  0.31234  , -0.34537  ,  0.14793  ,\n",
       "       -0.53745  , -0.43215  , -0.48724  , -0.51019  , -0.9051   ,\n",
       "       -0.17919  , -0.018376 ,  0.09719  , -0.31623  ,  0.7512   ,\n",
       "        0.92236  , -0.49965  ,  0.14036  , -0.28296  , -0.97443  ,\n",
       "       -0.0094408, -0.62944  ,  0.14711  , -0.94376  ,  0.0075222,\n",
       "        0.18565  , -0.99172  ,  0.072789 , -0.18474  , -0.52901  ,\n",
       "        0.38995  , -0.45677  , -0.21932  ,  1.3723   , -0.29636  ,\n",
       "       -2.2342   , -0.36667  ,  0.04987  ,  0.63421  ,  0.53275  ,\n",
       "       -0.53955  ,  0.31398  , -0.44698  , -0.38389  ,  0.066668 ,\n",
       "       -0.02168  ,  0.20558  ,  0.59456  , -0.24892  , -0.52795  ,\n",
       "       -0.3761   ,  0.077104 ,  0.75222  , -0.2647   , -0.0587   ,\n",
       "        0.67541  , -0.16559  , -0.49278  , -0.26327  , -0.21215  ,\n",
       "        0.24317  ,  0.17006  , -0.2926   , -0.5009   , -0.56638  ,\n",
       "       -0.40377  , -0.48452  , -0.32539  ,  0.75293  ,  0.0049585,\n",
       "       -0.32115  ,  0.28899  , -0.042392 ,  0.63863  , -0.20332  ,\n",
       "       -0.46785  , -0.15661  ,  0.2179   ,  1.4143   ,  0.40034  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_ud_en_ewt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bcdf2572c7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#train_sentences, dev_sentences, test_sentences, column_names = load_conll2009_pos()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mload_ud_en_ewt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_ud_en_ewt' is not defined"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/lab4/'\n",
    "\n",
    "def load_conll2009():\n",
    "    train_file = BASE_DIR + 'NER-data/eng.train'\n",
    "    dev_file = BASE_DIR + 'NER-data/eng.valid'\n",
    "    test_file = BASE_DIR + 'NER-data/eng.test'\n",
    "    # test2_file = 'simple_pos_test.txt'\n",
    "    \n",
    "    #column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    \n",
    "    column_names = list(map(str.lower, column_names))\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-train-pos.txt'\n",
    "    dev_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-development-pos.txt'\n",
    "    test_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-test-words-pos.txt'\n",
    "    # test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "#train_sentences, dev_sentences, test_sentences, column_names = load_conll2009_pos()\n",
    "train_sentences, dev_sentences, test_sentences, column_names =\\\n",
    "load_ud_en_ewt()\n",
    "train_sentences[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Corpus in a Dictionary\n",
    "We follow the fit-transform pattern of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        rows = [row for row in rows if row[0] != '#']\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "print('First sentence, train:', train_dict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to build the two-way sequences\n",
    "Two vectors: $\\mathbf{x}$ and $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='upos', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat, Y_train_cat = build_sequences(train_dict)\n",
    "print('First sentence, words', X_train_cat[0])\n",
    "print('First sentence, POS', Y_train_cat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Unique Words and Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_words = sorted(list(\n",
    "    set([word for sentence \n",
    "         in X_train_cat for word in sentence])))\n",
    "pos = sorted(list(set([pos for sentence \n",
    "                       in Y_train_cat for pos in sentence])))\n",
    "print(pos)\n",
    "NB_CLASSES = len(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create the dictionary\n",
    "We add two words for the padding symbol and unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "vocabulary_words = sorted(list(set(vocabulary_words + \n",
    "                                   list(embeddings_words))))\n",
    "cnt_uniq = len(vocabulary_words) + 2\n",
    "print('# unique words in the vocabulary: embeddings and corpus:', \n",
    "      cnt_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert the words or parts of speech to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(X, idx):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or POS lists) to indexes\n",
    "    :param X: List of word (or POS) lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown words to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, 1), x))\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start at one to make provision for the padding symbol 0 \n",
    "# in RNN and LSTMs and 1 for the unknown words\n",
    "rev_word_idx = dict(enumerate(vocabulary_words, start=2))\n",
    "rev_pos_idx = dict(enumerate(pos, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "pos_idx = {v: k for k, v in rev_pos_idx.items()}\n",
    "print('word index:', list(word_idx.items())[:10])\n",
    "print('POS index:', list(pos_idx.items())[:10])\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_idx = to_index(X_train_cat, word_idx)\n",
    "Y_idx = to_index(Y_train_cat, pos_idx)\n",
    "print('First sentences, word indices', X_idx[:3])\n",
    "print('First sentences, POS indices', Y_idx[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We pad the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X_idx)\n",
    "Y = pad_sequences(Y_idx)\n",
    "\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "# The number of POS classes and 0 (padding symbol)\n",
    "Y_train = to_categorical(Y, num_classes=len(pos) + 2)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create an embedding matrix\n",
    "0 is the padding symbol and index one is a unknown word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdstate = np.random.RandomState(1234567)\n",
    "embedding_matrix = rdstate.uniform(-0.05, 0.05, \n",
    "                                   (len(vocabulary_words) + 2, \n",
    "                                    EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "print('Embedding of table', embedding_matrix[word_idx['table']])\n",
    "print('Embedding of the padding symbol, idx 0, random numbers', \n",
    "      embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(len(vocabulary_words) + 2,\n",
    "                           EMBEDDING_DIM,\n",
    "                           mask_zero=True,\n",
    "                           input_length=None))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "# The default is True\n",
    "model.layers[0].trainable = True\n",
    "# model.add(SimpleRNN(100, return_sequences=True))\n",
    "#model.add(Bidirectional(SimpleRNN(100, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model.add(Dense(NB_CLASSES + 2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "model.fit(X, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In X_dict, we replace the words with their index\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict)\n",
    "# We create the parallel sequences of indexes\n",
    "X_test_idx = to_index(X_test_cat, word_idx)\n",
    "Y_test_idx = to_index(Y_test_cat, pos_idx)\n",
    "print('X[0] test idx', X_test_idx[0])\n",
    "print('Y[0] test idx', Y_test_idx[0])\n",
    "\n",
    "X_test_padded = pad_sequences(X_test_idx)\n",
    "Y_test_padded = pad_sequences(Y_test_idx)\n",
    "print('X[0] test idx passed', X_test_padded[0])\n",
    "print('Y[0] test idx padded', Y_test_padded[0])\n",
    "# One extra symbol for 0 (padding)\n",
    "Y_test_padded_vectorized = to_categorical(Y_test_padded, \n",
    "                                          num_classes=len(pos) + 2)\n",
    "print('Y[0] test idx padded vectorized', Y_test_padded_vectorized[0])\n",
    "print(X_test_padded.shape)\n",
    "print(Y_test_padded_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates with the padding symbol\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, \n",
    "                                     Y_test_padded_vectorized)\n",
    "print('Loss:', test_loss)\n",
    "print('Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We evaluate on all the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_test', X_test_cat[0])\n",
    "print('X_test_padded', X_test_padded[0])\n",
    "corpus_pos_predictions = model.predict(X_test_padded)\n",
    "print('Y_test', Y_test_cat[0])\n",
    "print('Y_test_padded', Y_test_padded[0])\n",
    "print('predictions', corpus_pos_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pred_num = []\n",
    "for sent_nbr, sent_pos_predictions in enumerate(corpus_pos_predictions):\n",
    "    pos_pred_num += [sent_pos_predictions[-len(X_test_cat[sent_nbr]):]]\n",
    "print(pos_pred_num[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert POS indices to symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pred = []\n",
    "for sentence in pos_pred_num:\n",
    "    pos_idx = list(map(np.argmax, sentence))\n",
    "    pos_cat = list(map(rev_pos_idx.get, pos_idx))\n",
    "    pos_pred += [pos_cat]\n",
    "\n",
    "print(pos_pred[:2])\n",
    "print(Y_test_cat[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total, correct, total_ukn, correct_ukn = 0, 0, 0, 0\n",
    "for id_s, sentence in enumerate(X_test_cat):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        total += 1\n",
    "        if pos_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "            correct += 1\n",
    "        # The word is not in the dictionary\n",
    "        if word not in word_idx:\n",
    "            total_ukn += 1\n",
    "            if pos_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "                correct_ukn += 1\n",
    "\n",
    "print('total %d, correct %d, accuracy %f' % \n",
    "      (total, correct, correct / total))\n",
    "if total_ukn != 0:\n",
    "    print('total unknown %d, correct %d, accuracy %f' % \n",
    "          (total_ukn, correct_ukn, correct_ukn / total_ukn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, model, word_idx, \n",
    "                     vocabulary_words, rev_idx_pos, verbose=False):\n",
    "    # Predict one sentence\n",
    "    sentence = sentence.split()\n",
    "    word_idxs = to_index([sentence], word_idx)\n",
    "    word_idx_padded = pad_sequences(word_idxs)\n",
    "\n",
    "    pos_idx_pred = model.predict(word_idx_padded)\n",
    "    # We remove padding\n",
    "    pos_idx_pred = pos_idx_pred[0][-len(sentence):]\n",
    "    pos_idx = list(map(np.argmax, pos_idx_pred))\n",
    "    pos = list(map(rev_idx_pos.get, pos_idx))\n",
    "    if verbose:\n",
    "        print('Sentence', sentence)\n",
    "        print('Sentence word indexes', word_idxs)\n",
    "        print('Padded sentence', word_idx_padded)\n",
    "        print('POS predicted', pos_idx_pred[0])\n",
    "        print('POS shape', pos_idx_pred.shape)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"That round table might collapse .\",\n",
    "                 \"The man can learn well .\",\n",
    "                 \"The man can swim .\",\n",
    "                 \"The man can simwo .\"]\n",
    "for sentence in sentences:\n",
    "    y_test_pred_cat = predict_sentence(sentence.lower(), \n",
    "                                       model, word_idx, \n",
    "                                       vocabulary_words, \n",
    "                                       rev_pos_idx)\n",
    "    print(sentence)\n",
    "    print(y_test_pred_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

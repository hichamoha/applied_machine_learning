{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "## Table of Content:\n",
    "* [0 - Splitting Flowers: Copying images to training, validation, and test directories](#section0)\n",
    "* [1 - Building the Naive Convulation Network](#section1)\n",
    "* [2 - Using Data Augmentation](#section2)\n",
    "* [3 - Using a Pretrained Convolutional Base](#section3)\n",
    "* [4 - Second technique - Feature extraction with data augmentation](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Splitting Flowers: Copying images to training, validation, and test directories <a class=\"anchor\" id=\"section0\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms have hyperparameters, settings that we can use to control the algorithm’s behavior. The values of hyperparameters are not adapted by the learning algorithm itself.\n",
    "\n",
    "Specifically, we split the training data into two disjoint\n",
    "subsets. One of these subsets is used to learn the parameters. The other subset is our validation set, used to estimate the generalization error during or after training, allowing for the hyperparameters to be updated accordingly. \n",
    "\n",
    "The subset of data used to learn the parameters is still typically called the training set, even though this may be confused with the larger pool of data used for the entire training process. The subset of data used to guide the selection of hyperparameters is called the validation set.\n",
    "\n",
    "Since the validation set is used to “train” the hyperparameters, the validation set error will underestimate the generalization error, though typically by a smaller amount than the training error\n",
    "does. After all hyperparameter optimization is complete, the generalization error may be estimated using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1-flowers_create_dataset.py \n",
    "Categorizing the flower dataset\n",
    "Creating the dataset\n",
    "Author: Pierre Nugues\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "vilde = False\n",
    "np.random.seed(1) # XXX this change to seed(1) to avoid double copy \n",
    "random.seed(1)\n",
    "\n",
    "if vilde:\n",
    "    base = '/media/hi8826mo-s/BEEE-DE51/Ultimi/'\n",
    "else:\n",
    "    base = '/media/hi8826mo-s/BEEE-DE51/Ultimi/'\n",
    "    \n",
    "# Path to the directory where the original dataset was uncompressed    \n",
    "original_dataset_dir = os.path.join(base, '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/flowers')\n",
    "\n",
    "# Directory where we'll store our smaller dataset, base_dir\n",
    "dataset = os.path.join(base, '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/flowers_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image types: ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n"
     ]
    }
   ],
   "source": [
    "# Directories for the training, validation and test splits\n",
    "train_dir = os.path.join(dataset, 'train')\n",
    "validation_dir = os.path.join(dataset, 'validation')\n",
    "test_dir = os.path.join(dataset, 'test')\n",
    "\n",
    "# Read all the file names\n",
    "categories = os.listdir(original_dataset_dir)\n",
    "categories = [category for category in categories if not category.startswith('.')]\n",
    "print('Image types:', categories)\n",
    "data_folders = [os.path.join(original_dataset_dir, category) for category in categories]\n",
    "\n",
    "# Create a list of pairs (file_name, category)\n",
    "pairs = []\n",
    "for folder, category in zip(data_folders, categories):\n",
    "    images = os.listdir(folder)\n",
    "    images = [image for image in images if not image.startswith('.')]\n",
    "    pairs.extend([(image, category) for image in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# shuffle() randomizes the items of a list in place\n",
    "random.shuffle(pairs) \n",
    "img_nbr = len(pairs)\n",
    "\n",
    "# creates a new dataset containing 3 subsets 60/20/20 ratio\n",
    "train_images = pairs[0:int(0.6 * img_nbr)]\n",
    "val_images = pairs[int(0.6 * img_nbr):int(0.8 * img_nbr)]\n",
    "test_images = pairs[int(0.8 * img_nbr):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training images: 2595\n",
      "total validation images: 865\n",
      "total test images: 866\n"
     ]
    }
   ],
   "source": [
    "# print(train_images)\n",
    "print('total training images:', len(train_images))\n",
    "print('total validation images:', len(val_images))\n",
    "print('total test images:', len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies the first 60 % flower images to train_dir\n",
    "for image, label in train_images:\n",
    "    src = os.path.join(original_dataset_dir, label, image)\n",
    "    dst = os.path.join(train_dir, label, image)\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copies the next 20 % flower images to validation_dir\n",
    "for image, label in val_images:\n",
    "    src = os.path.join(original_dataset_dir, label, image)\n",
    "    dst = os.path.join(validation_dir, label, image)\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copies the next 20 % flower images to test_dir\n",
    "for image, label in test_images:\n",
    "    src = os.path.join(original_dataset_dir, label, image)\n",
    "    dst = os.path.join(test_dir, label, image)\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    shutil.copyfile(src, dst)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Building the Naive Convulation Network <a class=\"anchor\" id=\"section1\" > </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####      A Convnet Model for Multiple Categories\n",
    "\n",
    "A typical layer of a convolutional network consists of three stages (see figure 9.7 in Goodfellow). In the first stage, the layer performs several convolutions in parallel to produce a set of linear activations. In the second stage, each linear activation is run through\n",
    "a nonlinear activation function, such as the rectified linear activation function.\n",
    "This stage is sometimes called the detector stage. In the third stage, we use a\n",
    "pooling function to modify the output of the layer further.\n",
    "\n",
    "Importantly, a convnet takes as input tensors of shape (image_height, image_width,\n",
    "image_channels) (not including the batch dimension). In this case, we’ll configure\n",
    "the convnet to process inputs of size (128, 128, 3) , which is the format of flowers\n",
    "images. We’ll do this by passing the argument input_shape=(128, 128, 3) to the first\n",
    "layer.\n",
    "\n",
    "The convnet will be a stack of alternated Conv2D (with relu activation) and MaxPooling2D layers.\n",
    "\n",
    "But because we’re dealing with bigger images and a more complex problem, we’ll\n",
    "make the network larger, accordingly: it will have one more Conv2D + MaxPooling2D\n",
    "stage. This serves both to augment the capacity of the network and to further reduce\n",
    "the size of the feature maps so they aren’t overly large when you reach the Flatten\n",
    "layer. Here, because you start from inputs of size 150 × 150 (a somewhat arbitrary\n",
    "choice), you end up with feature maps of size 7 × 7 just before the Flatten layer.\n",
    "\n",
    "NOTE: The depth of the feature maps progressively increases in the network\n",
    "(from 32 to 128), whereas the size of the feature maps decreases (from 148 ×\n",
    "148 to 7 × 7). This is a pattern you’ll see in almost all convnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a small convnet\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                                    input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Dropout(0.5))\n",
    "# Adding a classifier on top of the convnet\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN\tArchitectures\n",
    "Typical\tCNN\tarchitectures stack\ta few convolutional layers (each one generally followed\tby\ta ReLU layer), then\ta pooling layer, then another few convolutional\tlayers (+ReLU), then\tanother\tpooling\tlayer, and\tso\ton.\tThe\timage gets smaller and smaller as it progresses through\tthe\tnetwork, but it\talso typically gets\tdeeper\tand\tdeeper\t(i.e.,\twith\tmore\tfeature\tmaps) thanks to\tthe\tconvolutional\tlayers\t(see Figure\t13-9 ).\tAt\tthe\ttop\tof\tthe\tstack,\ta regular feedforward neural network is\tadded, composed of\ta few fully connected layers (+ReLUs),\tand\tthe\tfinal layer\toutputs\tthe\tprediction\t(e.g., a softmax\tlayer that outputs estimated class probabilities).\n",
    "\n",
    "Let’s look at how the dimensions of the feature maps change with every successive layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_299 (Conv2D)          (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_300 (Conv2D)          (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_301 (Conv2D)          (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_302 (Conv2D)          (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 3,455,173\n",
      "Trainable params: 3,455,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# display the architecture of the convnet\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of\n",
    "shape (height, width, channels) . The width and height dimensions tend to shrink\n",
    "as you go deeper in the network. The number of channels is controlled by the first\n",
    "argument passed to the Conv2D layers (32 or 64).\n",
    "\n",
    "The next step is to feed the last output tensor (of shape (7, 7, 128) ) into a densely\n",
    "connected classifier network: a stack of Dense layers. These classifiers process vectors, which are 1D , whereas the current output is a 3D tensor. First we have to flatten the 3D outputs to 1D , and then add a few Dense layers on top.\n",
    "\n",
    "NOTE 1: Because we’re attacking a multi-classification problem, We use the softmax activation and a categorical entrpopy loss. we’ll end the network with a five units (a Dense layer of size 5) and a softmax activation.\n",
    "\n",
    "NOTE 2: We’ll do 5-way classification, using a final layer with 5 outputs and a softmax activa-\n",
    "tion.\n",
    "\n",
    "NOTE 3: As we can see, the (7, 7, 128) outputs are flattened into vectors of shape (6272)\n",
    "before going through two Dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling\n",
    "\n",
    "For the compilation step, we’ll go with the RMSprop or nadam optimizer. Because we\n",
    "ended the network with a multiclass softmax, we’ll use categorical crossentropy as the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=optimizers.RMSprop(lr=1e-5),  \n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "Data should be formatted into appropriately preprocessed floating-point \n",
    "tensors before being fed into the network. Currently, the data sits on a \n",
    "drive as JPEG files, so the steps for getting it into the network are \n",
    "roughly as follows:\n",
    "\n",
    "1- Read the picture files.\n",
    "\n",
    "2- Decode the JPEG content to RGB grids of pixels.\n",
    "\n",
    "3- Convert these into floating-point tensors.\n",
    "\n",
    "4- Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know,\n",
    "   neural networks prefer to deal with small input values).\n",
    "\n",
    "Keras has a module with image-processing helper tools, located at keras.preprocessing.image. \n",
    "In particular, it contains the class ImageDataGenerator, which lets you quickly set up Python generators that can automatically turn image files on disk into batches of preprocessed tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2593 images belonging to 5 classes.\n",
      "Found 865 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Using ImageDataGenerator to read images from directories\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Rescale all images by 1/255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),       \n",
    "        batch_size=25,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=866,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Data\n",
    "\n",
    "Let’s fit the model to the data using the generator. We do so using the fit_generator\n",
    "method, the equivalent of fit for data generators like this one. It expects as its first\n",
    "argument a Python generator that will yield batches of inputs and targets indefinitely,\n",
    "like this one does. Because the data is being generated endlessly, the Keras model\n",
    "needs to know how many samples to draw from the generator before declaring an\n",
    "epoch over. This is the role of the steps_per_epoch argument: after having drawn\n",
    "steps_per_epoch batches from the generator, that is, after having run for steps_per_epoch gradient descent steps—the fitting process will go to the next epoch.\n",
    "\n",
    "When using fit_generator, you can pass a validation_data argument, much as\n",
    "with the fit method. It’s important to note that this argument is allowed to be a data\n",
    "generator, but it could also be a tuple of Numpy arrays. If you pass a generator as\n",
    "validation_data, then this generator is expected to yield batches of validation data\n",
    "endlessly; thus you should also specify the validation_steps argument, which tells\n",
    "the process how many batches to draw from the validation generator for evaluation.\n",
    "\n",
    "Example: An epoch usually means one iteration over all of the training data. For instance if you have 20,000 images and a batch size of 100 then the epoch should contain 20,000 / 100 = 200 steps. However I usually just set a fixed number of steps like 1000 per epoch even though I have a much larger data set. At the end of the epoch I check the average cost and if it improved I save a checkpoint. There is no difference between steps from one epoch to another. I just treat them as checkpoints.\n",
    "\n",
    "People often shuffle around the data set between epochs. I prefer to use the random.sample function to choose the data to process in my epochs. So say I want to do 1000 steps with a batch size of 32. I will just randomly pick 32,000 sam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 161s 2s/step - loss: 0.6139 - acc: 0.7796 - val_loss: 1.9186 - val_acc: 0.6335\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 161s 2s/step - loss: 0.5644 - acc: 0.7945 - val_loss: 2.1973 - val_acc: 0.6231\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 161s 2s/step - loss: 0.5128 - acc: 0.8163 - val_loss: 2.3153 - val_acc: 0.5965\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 154s 2s/step - loss: 0.4640 - acc: 0.8306 - val_loss: 2.0109 - val_acc: 0.6405\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 156s 2s/step - loss: 0.4541 - acc: 0.8451 - val_loss: 2.2772 - val_acc: 0.6266\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 157s 2s/step - loss: 0.4151 - acc: 0.8512 - val_loss: 1.9335 - val_acc: 0.6509\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 153s 2s/step - loss: 0.3944 - acc: 0.8591 - val_loss: 2.2927 - val_acc: 0.6254\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 155s 2s/step - loss: 0.3695 - acc: 0.8725 - val_loss: 2.3795 - val_acc: 0.6289\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 156s 2s/step - loss: 0.3684 - acc: 0.8669 - val_loss: 2.5015 - val_acc: 0.6116\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 155s 2s/step - loss: 0.3406 - acc: 0.8829 - val_loss: 2.4127 - val_acc: 0.6243\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model using a batch generator\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalutation the Performance\n",
    "The factors determining how well a machine learning algorithm will perform are its ability to\n",
    "\n",
    "1 - Make the training error small.\n",
    "\n",
    "2 - Make the gap between training and test error small.\n",
    "\n",
    "These two factors correspond to the two central challenges in machine learning:\n",
    "underfitting and overfitting. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.\n",
    "\n",
    "We can now finally evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 865 images belonging to 5 classes.\n",
      "test acc: 0.6485549211502075\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(        \n",
    "        test_dir,\n",
    "        shuffle = False,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=866,\n",
    "        class_mode='categorical')\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=1)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[154   4   4   1   3]\n",
      " [ 70  91   5  30   7]\n",
      " [ 18   0  83  17  19]\n",
      " [ 51  12   3  86   6]\n",
      " [ 23   3  16  12 147]]\n"
     ]
    }
   ],
   "source": [
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(test_generator, 1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model\n",
    "It’s good practice to always save your models after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('flowers_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss and accuracy of the model\n",
    "plot the loss and accuracy of the model over the training and validation data\n",
    "during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VNX9//HXh0VZZbcoyKKiInuMQQUVRRFXXFBB9OeOWrGt2lpq/X7hq1XrvlWt1q0LS6lWxQ3rgrsIQQUFRBCoRlADIsgiGPj8/jg3YRKzTMIkM8l9Px+PeWTm3nPvfOYm+dxzzz3njLk7IiISD/XSHYCIiNQcJX0RkRhR0hcRiRElfRGRGFHSFxGJESV9EZEYUdKPITOrb2brzKxTKsumk5ntaWYp739sZkeY2bKE1wvN7OBkylbhvR4ys6urur1IMhqkOwCpmJmtS3jZBNgEbIleX+TuEyqzP3ffAjRLddk4cPe9U7EfM7sAONPdByXs+4JU7FukPEr6tYC7FyXdqCZ5gbu/XFZ5M2vg7gU1EZtIRfT3mFnUvFMHmNkfzOyfZjbJzL4HzjSzA81shpl9Z2YrzOxuM2sYlW9gZm5mXaLX/4jWv2Bm35vZu2bWtbJlo/VHm9mnZrbGzO4xs7fN7Jwy4k4mxovMbLGZrTazuxO2rW9md5jZKjP7DBhazvG5xswml1h2r5ndHj2/wMwWRJ/ns6gWXta+8sxsUPS8iZn9PYptHrBfKe+7JNrvPDM7IVreC/gTcHDUdLYy4diOT9j+4uizrzKzp8xsl2SOTWWOc2E8ZvaymX1rZl+Z2VUJ7/M/0TFZa2a5ZrZraU1pZvZW4e85Op5vRO/zLXCNmXUzs+nRZ1kZHbcWCdt3jj5jfrT+LjNrFMXcPaHcLma2wczalPV5pQLurkctegDLgCNKLPsDsBk4nnAibwzsD/QnXM3tDnwKjInKNwAc6BK9/gewEsgGGgL/BP5RhbI7A98Dw6J1VwA/AueU8VmSifFpoAXQBfi28LMDY4B5QEegDfBG+HMu9X12B9YBTRP2/Q2QHb0+PipjwOHARqB3tO4IYFnCvvKAQdHzW4HXgFZAZ2B+ibKnAbtEv5Mzohh+Fq27AHitRJz/AMZHz4dEMfYFGgH3Aa8mc2wqeZxbAF8DvwR2BHYCcqJ1vwPmAN2iz9AXaA3sWfJYA28V/p6jz1YAXALUJ/w97gUMBnaI/k7eBm5N+DwfR8ezaVR+QLTuQeD6hPe5Engy3f+HtfmR9gD0qOQvrOyk/2oF2/0a+Ff0vLRE/ueEsicAH1eh7HnAmwnrDFhBGUk/yRgPSFj/b+DX0fM3CM1cheuOKZmISux7BnBG9Pxo4NNyyj4LXBo9Ly/pf574uwB+nli2lP1+DBwbPa8o6f8VuCFh3U6E+zgdKzo2lTzOZwG5ZZT7rDDeEsuTSfpLKohhODAren4w8BVQv5RyA4ClgEWvPwROTvX/VZweat6pO75IfGFm+5jZc9Hl+lrgWqBtOdt/lfB8A+XfvC2r7K6JcXj4L80raydJxpjUewH/LSdegInAyOj5GUDRzW8zO87M3ouaN74j1LLLO1aFdikvBjM7x8zmRE0U3wH7JLlfCJ+vaH/uvhZYDXRIKJPU76yC47wbsLiMGHYjJP6qKPn32N7MppjZl1EMj5WIYZmHTgPFuPvbhKuGgWbWE+gEPFfFmAS16dclJbsrPkCoWe7p7jsB/0uoeVenFYSaKABmZhRPUiVtT4wrCMmiUEVdSv8JHGFmHQnNTxOjGBsDjwM3EppeWgL/STKOr8qKwcx2B+4nNHG0ifb7ScJ+K+peupzQZFS4v+aEZqQvk4irpPKO8xfAHmVsV9a69VFMTRKWtS9RpuTnu4nQ66xXFMM5JWLobGb1y4jjb8CZhKuSKe6+qYxykgQl/bqrObAGWB/dCLuoBt7zWSDLzI43swaEduJ21RTjFOBXZtYhuqn32/IKu/vXhCaIR4GF7r4oWrUjoZ05H9hiZscR2p6TjeFqM2tpYRzDmIR1zQiJL59w/ruAUNMv9DXQMfGGagmTgPPNrLeZ7Ug4Kb3p7mVeOZWjvOM8FehkZmPMbAcz28nMcqJ1DwF/MLM9LOhrZq0JJ7uvCB0G6pvZaBJOUOXEsB5YY2a7EZqYCr0LrAJusHBzvLGZDUhY/3dCc9AZhBOAbAcl/brrSuBswo3VBwg13WoVJdbTgdsJ/8R7AB8QanipjvF+4BXgI2AWobZekYmENvqJCTF/B1wOPEm4GTqccPJKxjjCFccy4AUSEpK7zwXuBmZGZfYB3kvY9iVgEfC1mSU20xRuP43QDPNktH0nYFSScZVU5nF29zXAkcAphBvHnwKHRqtvAZ4iHOe1hJuqjaJmuwuBqwk39fcs8dlKMw7IIZx8pgJPJMRQABwHdCfU+j8n/B4K1y8j/J43u/s7lfzsUkLhzRGRlIsu15cDw939zXTHI7WXmf2NcHN4fLpjqe00OEtSysyGEi7XfyB0+Ssg1HZFqiS6PzIM6JXuWOoCNe9Iqg0ElhAu+4cCJ+rGm1SVmd1IGCtwg7t/nu546gI174iIxIhq+iIiMZJxbfpt27b1Ll26pDsMEZFaZfbs2Svdvbwu0kAGJv0uXbqQm5ub7jBERGoVM6toVDqg5h0RkVhR0hcRiRElfRGRGMm4Nv3S/Pjjj+Tl5fHDDz+kOxQpR6NGjejYsSMNG5Y1nYyIpFutSPp5eXk0b96cLl26ECZulEzj7qxatYq8vDy6du1a8QYikha1onnnhx9+oE2bNkr4GczMaNOmja7GRKpgwgTo0gXq1Qs/J0yoaIuqqxU1fUAJvxbQ70ik8iZMgNGjYcOG8Pq//w2vAUZVdV7VctSKmr6ISF31+99vS/iFNmwIy6uDkn4SVq1aRd++fenbty/t27enQ4cORa83b96c1D7OPfdcFi5cWG6Ze++9lwnVeV0nIhnn8zKmkStr+faqNc07lTFhQjhLfv45dOoE11+/fZdJbdq04cMPPwRg/PjxNGvWjF//+tfFyhR96XC90s+jjz76aIXvc+mll1Y9SBGplTp1Ck06pS2vDnWupl/YPvbf/4L7tvax6qhAL168mJ49e3LxxReTlZXFihUrGD16NNnZ2fTo0YNrr722qOzAgQP58MMPKSgooGXLlowdO5Y+ffpw4IEH8s033wBwzTXXcOeddxaVHzt2LDk5Oey999688074wqD169dzyimn0KdPH0aOHEl2dnbRCSnRuHHj2H///YviK5xN9dNPP+Xwww+nT58+ZGVlsWzZMgBuuOEGevXqRZ8+ffh9dV1XishPXH89NGlSfFmTJmF5dahzSb+m28fmz5/P+eefzwcffECHDh344x//SG5uLnPmzOGll15i/vz5P9lmzZo1HHroocyZM4cDDzyQRx55pNR9uzszZ87klltuKTqB3HPPPbRv3545c+YwduxYPvjgg1K3/eUvf8msWbP46KOPWLNmDdOmTQNg5MiRXH755cyZM4d33nmHnXfemWeeeYYXXniBmTNnMmfOHK688soUHR0RqcioUfDgg9C5M5iFnw8+WD03caEOJv2abh/bY4892H///YteT5o0iaysLLKysliwYEGpSb9x48YcffTRAOy3335Fte2STj755J+UeeuttxgxYgQAffr0oUePHqVu+8orr5CTk0OfPn14/fXXmTdvHqtXr2blypUcf/zxQBhM1aRJE15++WXOO+88GjduDEDr1q0rfyBEKqEmuyjWBqNGwbJlsHVr+FldCR/qYJt+TbePNW3atOj5okWLuOuuu5g5cyYtW7bkzDPPLLXf+g477FD0vH79+hQUFJS67x133PEnZZL50psNGzYwZswY3n//fTp06MA111xTFEdp3SrdXd0tpcbUdBdFKa7O1fRrun0s0dq1a2nevDk77bQTK1as4MUXX0z5ewwcOJApU6YA8NFHH5V6JbFx40bq1atH27Zt+f7773niiScAaNWqFW3btuWZZ54BwqC3DRs2MGTIEB5++GE2btwIwLfffpvyuEUK1XQTrBRX55J+TbePJcrKymLfffelZ8+eXHjhhQwYMCDl73HZZZfx5Zdf0rt3b2677TZ69uxJixYtipVp06YNZ599Nj179uSkk06if//+ResmTJjAbbfdRu/evRk4cCD5+fkcd9xxDB06lOzsbPr27csdd9yR8rhFCtV0E6wUl3HfkZudne0lv0RlwYIFdO/ePU0RZZaCggIKCgpo1KgRixYtYsiQISxatIgGDTKjpU6/K6lIly6lN8F27hzas2tSqrt3p5OZzXb37IrKZUamkKStW7eOwYMHU1BQgLvzwAMPZEzCF0nG9dcXb9OHmmuCTRTXewvKFrVMy5YtmT17drrDEKmywoSa7hp2efcWlPRFRFJo1Kj0J9a43luoczdyRUSSUVY37urq3p0plPRFJJbS2b07nZT0RWJGo2GDdHbvTicl/SQMGjToJwOt7rzzTn7+85+Xu12zZs0AWL58OcOHDy9z3yW7qJZ05513siHhjtMxxxzDd999l0zoIsXU5ISEtUFNTn+QKZT0kzBy5EgmT55cbNnkyZMZOXJkUtvvuuuuPP7441V+/5JJ//nnn6dly5ZV3p/El0bDipJ+EoYPH86zzz7Lpk2bAFi2bBnLly9n4MCBRf3ms7Ky6NWrF08//fRPtl+2bBk9e/YEwhQJI0aMoHfv3px++ulFUx8AXHLJJUXTMo8bNw6Au+++m+XLl3PYYYdx2GGHAdClSxdWrlwJwO23307Pnj3p2bNn0bTMy5Yto3v37lx44YX06NGDIUOGFHufQs888wz9+/enX79+HHHEEXz99ddAGAtw7rnn0qtXL3r37l00jcO0adPIysqiT58+DB48OCXHVmpWXHusyDa1rsvmr34FpUwfv1369oUoX5aqTZs25OTkMG3aNIYNG8bkyZM5/fTTMTMaNWrEk08+yU477cTKlSs54IADOOGEE8qcwOz++++nSZMmzJ07l7lz55KVlVW07vrrr6d169Zs2bKFwYMHM3fuXH7xi19w++23M336dNq2bVtsX7Nnz+bRRx/lvffew93p378/hx56KK1atWLRokVMmjSJv/zlL5x22mk88cQTnHnmmcW2HzhwIDNmzMDMeOihh7j55pu57bbbuO6662jRogUfffQRAKtXryY/P58LL7yQN954g65du2p+nlqqpicklMyjmn6SEpt4Ept23J2rr76a3r17c8QRR/Dll18W1ZhL88YbbxQl3969e9O7d++idVOmTCErK4t+/foxb968UidTS/TWW29x0kkn0bRpU5o1a8bJJ5/Mm2++CUDXrl3p27cvUPb0zXl5eRx11FH06tWLW265hXnz5gHw8ssvF/sWr1atWjFjxgwOOeQQunbtCmj65arIhBuoce2xItskVdM3s6HAXUB94CF3/2OJ9Z2AvwItozJj3f15M+sCLAAKvxx2hrtfvD0Bl1cjr04nnngiV1xxBe+//z4bN24sqqFPmDCB/Px8Zs+eTcOGDenSpUup0yknKu0qYOnSpdx6663MmjWLVq1acc4551S4n/LmTSqclhnC1MylNe9cdtllXHHFFZxwwgm89tprjB8/vmi/JWPU9MvbJ1OG/GfKaFhJnwpr+mZWH7gXOBrYFxhpZvuWKHYNMMXd+wEjgPsS1n3m7n2jx3Yl/HRq1qwZgwYN4rzzzit2A3fNmjXsvPPONGzYkOnTp/Pf0q6dExxyyCFFX37+8ccfM3fuXCBMy9y0aVNatGjB119/zQsvvFC0TfPmzfn+++9L3ddTTz3Fhg0bWL9+PU8++SQHH3xw0p9pzZo1dOjQAYC//vWvRcuHDBnCn/70p6LXq1ev5sADD+T1119n6dKlgKZfrqxMuoEaxx4rsk0yzTs5wGJ3X+Lum4HJwLASZRzYKXreAlieuhAzx8iRI5kzZ07RN1cBjBo1itzcXLKzs5kwYQL77LNPufu45JJLWLduHb179+bmm28mJycHCN+C1a9fP3r06MF5551XbFrm0aNHc/TRRxfdyC2UlZXFOeecQ05ODv379+eCCy6gX79+SX+e8ePHc+qpp3LwwQcXu19wzTXXsHr1anr27EmfPn2YPn067dq148EHH+Tkk0+mT58+nH766Um/j+gGqmSOCqdWNrPhwFB3vyB6fRbQ393HJJTZBfgP0ApoChzh7rOj5p15wKfAWuAad3+zlPcYDYwG6NSp034la8uarrf2yMTfVSZMn5tJ0wlL3ZTs1MrJ1PRLa8gteaYYCTzm7h2BY4C/m1k9YAXQKWr2uQKYaGY7ldgWd3/Q3bPdPbtdu3ZJhCSSnEwZjKQbqJIpkkn6ecBuCa878tPmm/OBKQDu/i7QCGjr7pvcfVW0fDbwGbDX9gYtkqxMaUuP65B/yTzJJP1ZQDcz62pmOxBu1E4tUeZzYDCAmXUnJP18M2sX3QjGzHYHugFLqhJopn3Dl/xUJv6OMqktXTdQJRNUmPTdvQAYA7xI6H45xd3nmdm1ZnZCVOxK4EIzmwNMAs7xkAEOAeZGyx8HLnb3Snf7aNSoEatWrcrIpCKBu7Nq1SoaNWqU7lCKiev0uSJlqRXfkfvjjz+Sl5dXYb91Sa9GjRrRsWNHGjZsmO5QipTsHw+hLV1NK1LX1KnvyG3YsGHRSFCRytBgJJHiakXSF9kemfDVfCKZQnPviIjEiJK+iEiMKOlLtcmEWSVFpDi16Uu1yJRZJUWkONX0pVpkykhYESlOSV+qRSaNhBWRbZT0pVpoJKxIZlLSl2qhWSVFMpOSvlQLzSopkpnUe0eqjUbCimQe1fRFRGJESV9EJEaU9OsgjYQVkbKoTb+O0UhYESmPavp1jEbCikh5lPTrGI2EFZHyKOnXMRoJKyLlUdKvYzQSVkTKo6Rfx2gkrIiUR7136iCNhBWRsqimLyISI0r6IiIxoqQvIhIjSSV9MxtqZgvNbLGZjS1lfSczm25mH5jZXDM7JmHd76LtFprZUakMXkREKqfCG7lmVh+4FzgSyANmmdlUd5+fUOwaYIq7329m+wLPA12i5yOAHsCuwMtmtpe7b0n1BxERkYolU9PPARa7+xJ33wxMBoaVKOPATtHzFsDy6PkwYLK7b3L3pcDiaH8iIpIGyST9DsAXCa/zomWJxgNnmlkeoZZ/WSW2xcxGm1mumeXm5+cnGbqIiFRWMknfSlnmJV6PBB5z947AMcDfzaxektvi7g+6e7a7Z7dr1y6JkDKTpjQWkUyXzOCsPGC3hNcd2dZ8U+h8YCiAu79rZo2AtkluWydoSmMRqQ2SqenPArqZWVcz24FwY3ZqiTKfA4MBzKw70AjIj8qNMLMdzawr0A2YmargM4mmNBaR2qDCmr67F5jZGOBFoD7wiLvPM7NrgVx3nwpcCfzFzC4nNN+c4+4OzDOzKcB8oAC4tK723NGUxiJSG1jIzZkjOzvbc3Nz0x1GpXXpEpp0SurcGZYtq+loRCRuzGy2u2dXVE4jclNEUxqLSG2gpJ8imtJYRGoDTa2cQprSWEQynWr6IiIxoqQvIhIjdSbpazSsiEjF6kSbvkbDiogkp07U9DUaVkQkOXUi6Ws0rIhIcupE0u/UqXLLRUTiqk4kfY2GFRFJTp1I+hoNKyKSnDrRewc0GlZEJBl1oqYvIiLJUdIXEYkRJX0RkRhR0hcRiRElfRGRGFHSFxGJESV9EZEYUdIXEYkRJX0RkRhR0hcRiRElfRGRGFHSFxGJESV9EZEYSSrpm9lQM1toZovNbGwp6+8wsw+jx6dm9l3Cui0J66amMngREamcCqdWNrP6wL3AkUAeMMvMprr7/MIy7n55QvnLgH4Ju9jo7n1TF7KIiFRVMjX9HGCxuy9x983AZGBYOeVHApNSEZyIiKRWMkm/A/BFwuu8aNlPmFlnoCvwasLiRmaWa2YzzOzEMrYbHZXJzc/PTzJ0ERGprGSSvpWyzMsoOwJ43N23JCzr5O7ZwBnAnWa2x0925v6gu2e7e3a7du2SCElERKoimaSfB+yW8LojsLyMsiMo0bTj7sujn0uA1yje3i8iIjUomaQ/C+hmZl3NbAdCYv9JLxwz2xtoBbybsKyVme0YPW8LDADml9xWRERqRoW9d9y9wMzGAC8C9YFH3H2emV0L5Lp74QlgJDDZ3RObfroDD5jZVsIJ5o+JvX5ERKRmWfEcnX7Z2dmem5ub7jBERGoVM5sd3T8tl0bkiojEiJK+iEiMKOmLiMSIkr6ISIwo6YuIxIiSvohIjCjpi4jEiJK+iEiMKOmLiMSIkr6ISIwo6YuIxIiSvohIjCjpi4jEiJK+iEiMKOmLiMSIkr6ISIwo6YuIxIiSvohIjCjpi4jEiJK+iEiMKOmLiMSIkr6ISIwo6YuIxIiSvohIjCjpi4jESFJJ38yGmtlCM1tsZmNLWX+HmX0YPT41s+8S1p1tZouix9mpDF5ERCqnQUUFzKw+cC9wJJAHzDKzqe4+v7CMu1+eUP4yoF/0vDUwDsgGHJgdbbs6pZ9CRESSkkxNPwdY7O5L3H0zMBkYVk75kcCk6PlRwEvu/m2U6F8Chm5PwCIiUnXJJP0OwBcJr/OiZT9hZp2BrsCrldnWzEabWa6Z5ebn5ycTt4iIVEEySd9KWeZllB0BPO7uWyqzrbs/6O7Z7p7drl27JEISEZGqSCbp5wG7JbzuCCwvo+wItjXtVHZbERGpZskk/VlANzPramY7EBL71JKFzGxvoBXwbsLiF4EhZtbKzFoBQ6JlIiKSBhX23nH3AjMbQ0jW9YFH3H2emV0L5Lp74QlgJDDZ3T1h22/N7DrCiQPgWnf/NrUfQUREkmUJOTojZGdne25ubrrDEBGpVcxstrtnV1ROI3JFRGJESV9EJEaU9EVEYkRJX0QkRpT0RURiRElfRCRGlPRFRGJESV9EJEaU9EVEYkRJX0QkRpT0RURiRElfRCRGlPRFRGJESV9EJEaU9EVEYkRJX0QkRpT0RURiRElfRCRGlPRFRGJESV9EJEaU9EVEYkRJX0QkRpT0RURiRElfRCRGlPRFRGJESV9EJEaSSvpmNtTMFprZYjMbW0aZ08xsvpnNM7OJCcu3mNmH0WNqqgIXEZHKa1BRATOrD9wLHAnkAbPMbKq7z08o0w34HTDA3Veb2c4Ju9jo7n1THLdIreMOM2ZA167Qvn26o5G4SqamnwMsdvcl7r4ZmAwMK1HmQuBed18N4O7fpDZMkdpr61Z48knIyoKDDoLdd4erroJVq9IdmcRRMkm/A/BFwuu8aFmivYC9zOxtM5thZkMT1jUys9xo+YmlvYGZjY7K5Obn51fqA4hkqq1b4YknoF8/OPlkWLcO/vxnOOUUuPXWUOMfPx7Wrk13pBInySR9K2WZl3jdAOgGDAJGAg+ZWctoXSd3zwbOAO40sz1+sjP3B909292z27Vrl3TwIplo61b417+gTx8YPhx++AH+/ndYsAAuuig8/+gjOPJI+L//C8n/5pthw4Z0Ry5xkEzSzwN2S3jdEVheSpmn3f1Hd18KLCScBHD35dHPJcBrQL/tjFkkI23ZApMnQ69ecNppUFAAEybA/Plw5pnQIOEOWo8e4SogNxdycuC3v4U99oA//Qk2bUrfZ5C6L5mkPwvoZmZdzWwHYARQshfOU8BhAGbWltDcs8TMWpnZjgnLBwDzEalDtmyBiRNDsh85MiybNAk+/hjOOAPq1y972/32gxdegDffhL32gssuCz8feSScNERSrcKk7+4FwBjgRWABMMXd55nZtWZ2QlTsRWCVmc0HpgO/cfdVQHcg18zmRMv/mNjrR6Q2KyiAf/wj1NpHjQrJ/Z//DE03I0aUn+xLGjgQXnsN/vMf+NnP4Pzzw34nTw7NRSKpYu4lm+fTKzs723Nzc9MdhqTI1q2wZg20apXuSFKnoCDU7P/wB1i0CHr3hv/9XzjpJKiXguGO7jB1KvzP/4QTSO/ecN11cPzxYKXdYRMBzGx2dP+0XBqRK9VmxYrQfNG6deiu+Pvfh2aM2tps8eOP8Nhj0L07nH02NG0K//43fPBB6JGTioQPIbEPGwYffhhOLhs3htcHHAAvvxxOCiJVpaQv1WLhwtAnfdGi0Ce9WTO46SY45BBo2zb0ann4Yfjyy3RHWrEffwyx7rMPnHsuNG8OTz0F77+futp9aerVC/cI5s+Hhx6Cr74KPX4OOwzefrt63lPqPiV9Sbn33oMBA2D9+tBOfdNN8MYbYTDS44/DqaeGkakXXAAdO4bmi6uugunTYfPmdEe/zebN8Je/hBurF1wQrlimToXZs0PNu6aaWho0CG38n34K99wDn3wS7gEcc0w48YhUirtn1GO//fZzqb2efda9cWP3PfZwX7So7HJbt7p/9JH7zTe7H364e8OG7uDerJn7sGHu99/vvmxZzcWdaNMm9z//2b1TpxBTTo77c8+FmDPB+vXuN93k3rp1iO+UU9znzUt3VJJuQK4nkWPTnuRLPqqa9LduDYlixgz3H36o0i5kOz38sHv9+u777ef+1VeV23btWvenn3a/+GL3zp3DXya477OP++WXu//nP+4bN1ZL2EV++MH9vvvcd9stvPcBB7i/8ELmJPuSvvvOfdw49+bN3evVcz/rLPfPPkt3VJIusUv6S5duSxQNG7rvv7/7pZe6/+1v7p984r5lS5V2K0nYutX9uuvCsR8yxP3777d/fwsWuN9xR9jfjjuGfTdp4n7sse733OO+eHFqYncPJ5N77nHv0CG8z0EHub/4YuYm+5Ly891/85twhdWggftFF7l/8UW6o5KaFrukv3Vr+EN/4gn3q65yHzQoNBUUnghatnQ/8kj33//eferUytdEpXQFBe6XXBKO8VlnhaaRVFu/PjSvXHaZ+557bvud7rlnWPbcc6FMZW3Y4H7XXe677hr2N3Cg+0sv1Z5kX9Ly5aGi07BhOFFefrn711+nOyqpKbFL+qUpKAjtxg8/7D56tHvfvqH5oTBpdOrkfuqp7rfc4v7GG+7r1qXsrWNhwwb3k04Kx/K3v625ZLloUaiZH3tsqN1CSHJDhoSrgwULyo9lw4ZQrn37sO2vKb4HAAALsUlEQVQhh7i/8krtTfYlLV3qfu65ocmnadNQ0Vm9Ot1RbbN1a2jOW7YsNEfVleOebskm/dgNztqwIfR4mDkz9DKZOROWLQvr6tWDnj2hf/8wH0pOThgVWZmRlXGxejWccELoOnjnnfCLX6Qnjh9+CH3/X3gBpk0Lk5oBdOkCQ4fC0UfD4YeHLqPr18MDD4TJzb7+GgYNgnHjws+6aOHC8Pn++U9o2RJ+85vwe2rWLDX7dw8zhH777bbHqlXFX5e1PHGsRvfuYUTzyJFh2mmpmmQHZ8Uu6Zfmm29C8k88EXz3XVjXtGkYYJR4Ithtt3iPjPzii5BQFy8OM0aedlq6I9pm2TJ48cVwEnjllTCdccOGoQvp/Pnhdz14cBhBe8gh6Y62ZsyZE0b3PvMMtGsHv/sdXHIJNGoU1heOmk4meSeuW706zDtUlmbNQjfXwkebNsVft24dKmH/+lfo0gtw4IFhvqLTToOddy573/JTSvrbwT0ktMITwMyZYdRlYR/y9u23nQD694fs7FCTioOPPw4J//vvwwClww5Ld0Rl27w5XIlMmxbmtGnfHq65JpwA4ui998Lnf/nlkFBbtAgJfPXq8kf57rRT6Qm7vITeqhXssEPysX3+eZikbuJEmDs3XF0PGRJOACeemLqrk0zmHiopzZtXbXsl/RTbtCn8MSaeCBYu3LZ+n322nQgOOgj69q17VwNvvBEGJTVuHBJp797pjkiq4rXX4P77Q2ItrxbeunWozDRsWLPxffxxmJJ64sRwMmjcOPzdjRoFRx1V8/FUp+XL4dVXw+OVV2DPPcPPqlDSrwGrV4f50Aubhd57LzQfQGgnvvlm2H//tIaYMv/+d6h1dekSmk86d053RFLXbd0K77wTTgBTpoQmpTZtwojuUaNC5aq6psCoLt9+G066hUn+k0/C8latwlXz0UeH0d9VoaSfBu6hvfvJJ8MMjCtXhrbJG24IX5BRW913H4wZE5qynn02/OOJ1KTNm0MT3YQJ8PTTYRK6zp3Dzd9Ro0IHjEy0bh289VZI8K++GpqJ3cO9wkMOCZ0MBg8O37K2vScwJf00W7sWbrkFbr89/MFefHG4mVabbk65h5ivvz5M6zt5MjRpku6oJO7WrQv3kyZMgJdeCjeTe/Xa1gOoU6f0xbZpU7jiL0zyM2aEnko77BBuUhcm+f33r9w9j2Qo6WeIFSvC96A+9FBom7zqKrjiinCmz2Q//hhOVI88Ei4377+/+Nf9iWSCb74JTT8TJ8K774ZlBx8cTgDDh1f/VemWLaELeGG7/JtvhquQevVCr7/CJD9gQPVXmJT0M8wnn8DVV4emn/btYfz4MHNiJibS9etDs9Tzz4d+3uPG1b2b0lL3LFkSegBNmBDGazRsGHqajRoVrlRTkXTdQ9ffwjb5114L3V0hjOkZPDgk+kMPrfkefUr6Geqdd0Jt/+23Ye+94cYbQ5e0TEmq+flw3HHhBvV998FFF6U7IpHKcd/2BTSTJoXvbGjWLHz3wahRITFXprK1dOm2JP/qq2FgH4SBZIcfvu3xs59Vz+dJlpJ+BvPo6/DGjg1XAAcdFHr6pLv/+NKloUvcF1+E9vthw9Ibj8j22rIldDWeODF8l8N334X7aqefHk4AOTk/rXB99dW25ppXXw3/FxCu0Aubaw4/PPRkyyRK+rVAQQE8+mhoPlmxIiTZG28Mw9Jr2gcfhC/l2LQpjNxM9wlIJNU2bQpNlhMnhr/xTZtCr7ozzgg3gt98M9Tm588P5Vu2DN0oC2vy3btnzhV5aZT0a5H168P8NTfdFJ6ff35o899115p5/5dfhpNPDn/kL76YnpOOSE1asybcX5swIdTmt24Nbf4HH7ytNt+3b+2ad0tJvxbKzw/9+wt7ylx+eWj/b9Gi+t5z0qTwJd977x1G2XboUH3vJZKJVqyA//4XsrJS342yJiWb9GvZeLa6rV07uOuu0M5/4onbBnXddVe4FE21228Pl7YHHhgubZXwJY522QUOOKB2J/zKUNLPQLvvHtodc3PDJeavfhWaXCZNCpeh22vrVrjyyvAYPjw06cRlwjiRuFPSz2D77RdGHE6bFmY6POOM0NugqhMyQRgdfOaZoZY/ZkzopVM4xa6I1H1JJX0zG2pmC81ssZmNLaPMaWY238zmmdnEhOVnm9mi6HF2qgKPC7PQjfL99+Fvfwvt/kccEQadzJlTuX2tXQvHHhuuGG68Ee6+u3bdqBKR7Vdh0jez+sC9wNHAvsBIM9u3RJluwO+AAe7eA/hVtLw1MA7oD+QA48ysVUo/QUzUqwdnnRWmc7711jCzZ79+8P/+X7gJVZGvvgqjBKdPh8ceC2MEMrn7mYhUj2Rq+jnAYndf4u6bgclAyWE7FwL3uvtqAHePJhjmKOAld/82WvcSMDQ1ocdTo0ahLf6zz8LX302ZAnvtBb/+dZi2tTSffhoGgH36aeiffLaut0RiK5mk3wH4IuF1XrQs0V7AXmb2tpnNMLOhldhWqqBVq9Cvf9Gi0NZ/++3hBvBNN4UJnwrNnBkGWq1bF+YJOfrotIUsIhkgmaRfWiNAyc79DYBuwCBgJPCQmbVMclvMbLSZ5ZpZbn5+fhIhSaHddgujeufMCcl97NhQ83/00TD3/WGHha9fe/vtuvOFLiJSdckk/Txgt4TXHYHlpZR52t1/dPelwELCSSCZbXH3B909292z27VrV5n4JdKrFzz3XGiz32UXOO+8MLPg3nuHSd66dUt3hCKSCZJJ+rOAbmbW1cx2AEYAU0uUeQo4DMDM2hKae5YALwJDzKxVdAN3SLRMqsmgQeFLHKZMCSN6X389TBQlIgKhWaZc7l5gZmMIybo+8Ii7zzOza4Fcd5/KtuQ+H9gC/MbdVwGY2XWEEwfAte5exu1GSRWz8D2ip56a7khEJNNo7h0RkTpAc++IiMhPKOmLiMSIkr6ISIwo6YuIxIiSvohIjCjpi4jEiJK+iEiMZFw/fTPLB5KYLDijtQVWpjuIDKLjUZyOxzY6FsVtz/Ho7O4VzmOTcUm/LjCz3GQGScSFjkdxOh7b6FgUVxPHQ807IiIxoqQvIhIjSvrV48F0B5BhdDyK0/HYRseiuGo/HmrTFxGJEdX0RURiRElfRCRGlPRTyMx2M7PpZrbAzOaZ2S/THVO6mVl9M/vAzJ5NdyzpZmYtzexxM/sk+hs5MN0xpZOZXR79n3xsZpPMrFG6Y6pJZvaImX1jZh8nLGttZi+Z2aLoZ6tUv6+SfmoVAFe6e3fgAOBSM9s3zTGl2y+BBekOIkPcBUxz932APsT4uJhZB+AXQLa79yR8K9+I9EZV4x4DhpZYNhZ4xd27Aa9Er1NKST+F3H2Fu78fPf+e8E/dIb1RpY+ZdQSOBR5KdyzpZmY7AYcADwO4+2Z3/y69UaVdA6CxmTUAmgDL0xxPjXL3N4CSXx87DPhr9PyvwImpfl8l/WpiZl2AfsB76Y0kre4ErgK2pjuQDLA7kA88GjV3PWRmTdMdVLq4+5fArcDnwApgjbv/J71RZYSfufsKCJVIYOdUv4GSfjUws2bAE8Cv3H1tuuNJBzM7DvjG3WenO5YM0QDIAu53937Aeqrh0r22iNqqhwFdgV2BpmZ2Znqjigcl/RQzs4aEhD/B3f+d7njSaABwgpktAyYDh5vZP9IbUlrlAXnuXnjl9zjhJBBXRwBL3T3f3X8E/g0clOaYMsHXZrYLQPTzm1S/gZJ+CpmZEdpsF7j77emOJ53c/Xfu3tHduxBu0L3q7rGtybn7V8AXZrZ3tGgwMD+NIaXb58ABZtYk+r8ZTIxvbCeYCpwdPT8beDrVb9Ag1TuMuQHAWcBHZvZhtOxqd38+jTFJ5rgMmGBmOwBLgHPTHE/auPt7ZvY48D6h19sHxGxKBjObBAwC2ppZHjAO+CMwxczOJ5wYT035+2oaBhGR+FDzjohIjCjpi4jEiJK+iEiMKOmLiMSIkr6ISIwo6YuIxIiSvohIjPx/RhDL8T/HjrQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying curves of loss and accuracy during training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Using Data Augmentation <a class=\"anchor\" id=\"section2\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now going to work with a new one, specific to computer vision and used almost universally when processing images with deep-learning models: data augmentation.\n",
    "\n",
    "Overfitting is caused by having too few samples to learn from, rendering you unable\n",
    "to train a model that can generalize to new data. Given infinite data, your model\n",
    "would be exposed to every possible aspect of the data distribution at hand: you would\n",
    "never overfit. Data augmentation takes the approach of generating more training data\n",
    "from existing training samples, by augmenting the samples via a number of random\n",
    "transformations that yield believable-looking images. The goal is that at training time,\n",
    "your model will never see the exact same picture twice. This helps expose the model\n",
    "to more aspects of the data and generalize better.\n",
    "\n",
    "In Keras, this can be done by configuring a number of random transformations to\n",
    "be performed on the images read by the ImageDataGenerator instance.\n",
    "\n",
    "Note that the validation data and the set data shouldn’t be augmented!\n",
    "\n",
    "Defining a new convnet that includes dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a small convnet\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                                    input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Dropout(0.5))\n",
    "# Adding a classifier on top of the convnet\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),  \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2593 images belonging to 5 classes.\n",
      "Found 865 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 1.5933 - acc: 0.2644 - val_loss: 1.5675 - val_acc: 0.3145\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 90s 896ms/step - loss: 1.5496 - acc: 0.3336 - val_loss: 1.5117 - val_acc: 0.4046\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 1.4974 - acc: 0.3810 - val_loss: 1.4497 - val_acc: 0.4173\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 1.4316 - acc: 0.4012 - val_loss: 1.3892 - val_acc: 0.4173\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 90s 903ms/step - loss: 1.3756 - acc: 0.4288 - val_loss: 1.3387 - val_acc: 0.4416\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 1.3432 - acc: 0.4172 - val_loss: 1.2789 - val_acc: 0.4497\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 93s 931ms/step - loss: 1.2899 - acc: 0.4532 - val_loss: 1.2944 - val_acc: 0.4382\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 1.2726 - acc: 0.4524 - val_loss: 1.2335 - val_acc: 0.4786\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 92s 920ms/step - loss: 1.2205 - acc: 0.4717 - val_loss: 1.2106 - val_acc: 0.4994\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 1.2486 - acc: 0.4554 - val_loss: 1.2341 - val_acc: 0.4786\n"
     ]
    }
   ],
   "source": [
    "# Setting up a data augmentation configuration via ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "            #fill_mode='nearest')\n",
    "\n",
    "# Rescale all images by 1/255\n",
    "#train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150),       \n",
    "        batch_size=25,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=865,\n",
    "        class_mode='categorical')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 865 images belonging to 5 classes.\n",
      "test acc: 0.4728323817253113\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(        \n",
    "        test_dir,\n",
    "        shuffle = False,       # XXXXX \n",
    "        target_size=(150, 150),\n",
    "        batch_size=866,\n",
    "        class_mode='categorical')\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=1)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('flowers_2.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Using a Pretrained Convolutional Base <a class=\"anchor\" id=\"section3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common and highly effective approach to deep learning on small image datasets is\n",
    "to use a pretrained network. A pretrained network is a saved network that was previously\n",
    "trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial feature hierarchy learned by the pre-trained network can effectively act as a generic model of our visual world, and hence its features can prove useful for many different computer vision problems, even though these new problems might involve completely different classes from those of the original task. \n",
    "\n",
    "In our case, we will consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1000 different classes). \n",
    "\n",
    "There are two ways to leverage a pre-trained network: feature extraction and fine-tuning.  Let's start with feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting features\n",
    "Feature extraction consists of using the representations learned by a previous network\n",
    "to extract interesting features from new samples. These features are then run through\n",
    "a new classifier, which is trained from scratch.\n",
    "\n",
    "As you saw previously, convnets used for image classification comprise two parts:\n",
    "they start with a series of pooling and convolution layers, and they end with a densely\n",
    "connected classifier. The first part is called the convolutional base of the model. In the\n",
    "case of convnets, feature extraction consists of taking the convolutional base of previously trained network, running the new data through it, and training a new classifier on top of the output.\n",
    "\n",
    "At this point, there are two ways we could proceed:\n",
    "\n",
    "1 - Running the convolutional base over our dataset, recording its output to a Numpy array on disk, then using this data as input to a standalone densely-connected classifier similar to those you have seen in the first chapters of this book. This solution is very fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. However, for the exact same reason, this technique would not allow us to leverage data augmentation at all.\n",
    "\n",
    "2 - Extending the model we have (conv_base) by adding Dense layers on top, and running the whole thing end-to-end on the input data. This allows us to use data augmentation, because every input image is going through the convolutional base every time it is seen by the model. However, for this same reason, this technique is far more expensive than the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features extraction - First technique\n",
    " Recording the output of conv_base on our data and using these outputs as inputs to a new model.\n",
    " \n",
    "Create the Inception v3\tmodel by calling the inception_v3() function, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Inceptionv3\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "#model = models.Sequential()\n",
    "conv_base = InceptionV3(weights='imagenet', \n",
    "                        include_top=False,\n",
    "                        input_shape=(150,150,3))\n",
    "\n",
    "#model.add(conv_base)\n",
    "#model.add(layers.Flatten())\n",
    "#model.add(layers.Dense(512, activation='relu', input_dim=3,3,2048))\n",
    "#model.add(layers.Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the model looks like now after adding a densely connected classifier on top of the convolutional base build with Inceptionv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 74, 74, 32)   864         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 74, 74, 32)   96          conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 74, 74, 32)   0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 72, 72, 32)   9216        activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 72, 72, 32)   96          conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 72, 72, 32)   0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 72, 72, 64)   18432       activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 72, 72, 64)   192         conv2d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 72, 72, 64)   0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling2D) (None, 35, 35, 64)   0           activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 35, 35, 80)   5120        max_pooling2d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 35, 35, 80)   240         conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 35, 35, 80)   0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 33, 33, 192)  138240      activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 33, 33, 192)  576         conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 33, 33, 192)  0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling2D) (None, 16, 16, 192)  0           activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 16, 16, 64)   192         conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 16, 16, 64)   0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 16, 16, 48)   9216        max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 16, 16, 96)   55296       activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 16, 16, 48)   144         conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 16, 16, 96)   288         conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 16, 16, 48)   0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 16, 16, 96)   0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_37 (AveragePo (None, 16, 16, 192)  0           max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 16, 16, 64)   76800       activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 16, 16, 96)   82944       activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 16, 16, 32)   6144        average_pooling2d_37[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 16, 16, 64)   192         conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 16, 16, 64)   192         conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_387 (BatchN (None, 16, 16, 96)   288         conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 16, 16, 32)   96          conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 16, 16, 64)   0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 16, 16, 64)   0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 16, 16, 96)   0           batch_normalization_387[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 16, 16, 32)   0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_382[0][0]             \n",
      "                                                                 activation_384[0][0]             \n",
      "                                                                 activation_387[0][0]             \n",
      "                                                                 activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 16, 16, 64)   192         conv2d_420[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 16, 16, 64)   0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 16, 16, 96)   55296       activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 16, 16, 48)   144         conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 16, 16, 96)   288         conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 16, 16, 48)   0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 16, 16, 96)   0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_38 (AveragePo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 16, 16, 64)   76800       activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 16, 16, 96)   82944       activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 16, 16, 64)   16384       average_pooling2d_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 16, 16, 64)   192         conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 16, 16, 64)   192         conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 16, 16, 96)   288         conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 16, 16, 64)   192         conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 16, 16, 64)   0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 16, 16, 64)   0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 16, 16, 96)   0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 16, 16, 64)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_389[0][0]             \n",
      "                                                                 activation_391[0][0]             \n",
      "                                                                 activation_394[0][0]             \n",
      "                                                                 activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_427 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 16, 16, 64)   192         conv2d_427[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 16, 16, 64)   0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_428 (Conv2D)             (None, 16, 16, 96)   55296       activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 16, 16, 48)   144         conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 16, 16, 96)   288         conv2d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 16, 16, 48)   0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 16, 16, 96)   0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_39 (AveragePo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 16, 16, 64)   76800       activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_429 (Conv2D)             (None, 16, 16, 96)   82944       activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_430 (Conv2D)             (None, 16, 16, 64)   18432       average_pooling2d_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 16, 16, 64)   192         conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 16, 16, 64)   192         conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 16, 16, 96)   288         conv2d_429[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 16, 16, 64)   192         conv2d_430[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 16, 16, 64)   0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 16, 16, 64)   0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 16, 16, 96)   0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 16, 16, 64)   0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_396[0][0]             \n",
      "                                                                 activation_398[0][0]             \n",
      "                                                                 activation_401[0][0]             \n",
      "                                                                 activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_432 (Conv2D)             (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 16, 16, 64)   192         conv2d_432[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 16, 16, 64)   0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_433 (Conv2D)             (None, 16, 16, 96)   55296       activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 16, 16, 96)   288         conv2d_433[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 16, 16, 96)   0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_431 (Conv2D)             (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_434 (Conv2D)             (None, 7, 7, 96)     82944       activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 7, 7, 384)    1152        conv2d_431[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 7, 7, 96)     288         conv2d_434[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 7, 7, 384)    0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 7, 7, 96)     0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling2D) (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_403[0][0]             \n",
      "                                                                 activation_406[0][0]             \n",
      "                                                                 max_pooling2d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_439 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 7, 7, 128)    384         conv2d_439[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 7, 7, 128)    0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_440 (Conv2D)             (None, 7, 7, 128)    114688      activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 7, 7, 128)    384         conv2d_440[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 7, 7, 128)    0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_436 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_441 (Conv2D)             (None, 7, 7, 128)    114688      activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 7, 7, 128)    384         conv2d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 7, 7, 128)    384         conv2d_441[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 7, 7, 128)    0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 7, 7, 128)    0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_437 (Conv2D)             (None, 7, 7, 128)    114688      activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_442 (Conv2D)             (None, 7, 7, 128)    114688      activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 7, 7, 128)    384         conv2d_437[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 7, 7, 128)    384         conv2d_442[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 7, 7, 128)    0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 7, 7, 128)    0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_40 (AveragePo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_435 (Conv2D)             (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_438 (Conv2D)             (None, 7, 7, 192)    172032      activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_443 (Conv2D)             (None, 7, 7, 192)    172032      activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_444 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 7, 7, 192)    576         conv2d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 7, 7, 192)    576         conv2d_438[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 7, 7, 192)    576         conv2d_443[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_416 (BatchN (None, 7, 7, 192)    576         conv2d_444[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 7, 7, 192)    0           batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 7, 7, 192)    0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 7, 7, 192)    0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 7, 7, 192)    0           batch_normalization_416[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_407[0][0]             \n",
      "                                                                 activation_410[0][0]             \n",
      "                                                                 activation_415[0][0]             \n",
      "                                                                 activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_449 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_421 (BatchN (None, 7, 7, 160)    480         conv2d_449[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 7, 7, 160)    0           batch_normalization_421[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_450 (Conv2D)             (None, 7, 7, 160)    179200      activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_422 (BatchN (None, 7, 7, 160)    480         conv2d_450[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 7, 7, 160)    0           batch_normalization_422[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_446 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_451 (Conv2D)             (None, 7, 7, 160)    179200      activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_418 (BatchN (None, 7, 7, 160)    480         conv2d_446[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_423 (BatchN (None, 7, 7, 160)    480         conv2d_451[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 7, 7, 160)    0           batch_normalization_418[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 7, 7, 160)    0           batch_normalization_423[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_447 (Conv2D)             (None, 7, 7, 160)    179200      activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_452 (Conv2D)             (None, 7, 7, 160)    179200      activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_419 (BatchN (None, 7, 7, 160)    480         conv2d_447[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_424 (BatchN (None, 7, 7, 160)    480         conv2d_452[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 7, 7, 160)    0           batch_normalization_419[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 7, 7, 160)    0           batch_normalization_424[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_41 (AveragePo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_445 (Conv2D)             (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_448 (Conv2D)             (None, 7, 7, 192)    215040      activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_453 (Conv2D)             (None, 7, 7, 192)    215040      activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_454 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_41[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_417 (BatchN (None, 7, 7, 192)    576         conv2d_445[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_420 (BatchN (None, 7, 7, 192)    576         conv2d_448[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_425 (BatchN (None, 7, 7, 192)    576         conv2d_453[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_426 (BatchN (None, 7, 7, 192)    576         conv2d_454[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 7, 7, 192)    0           batch_normalization_417[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 7, 7, 192)    0           batch_normalization_420[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 7, 7, 192)    0           batch_normalization_425[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 7, 7, 192)    0           batch_normalization_426[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_417[0][0]             \n",
      "                                                                 activation_420[0][0]             \n",
      "                                                                 activation_425[0][0]             \n",
      "                                                                 activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_459 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_431 (BatchN (None, 7, 7, 160)    480         conv2d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 7, 7, 160)    0           batch_normalization_431[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_460 (Conv2D)             (None, 7, 7, 160)    179200      activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_432 (BatchN (None, 7, 7, 160)    480         conv2d_460[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 7, 7, 160)    0           batch_normalization_432[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_456 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_461 (Conv2D)             (None, 7, 7, 160)    179200      activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_428 (BatchN (None, 7, 7, 160)    480         conv2d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_433 (BatchN (None, 7, 7, 160)    480         conv2d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 7, 7, 160)    0           batch_normalization_428[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 7, 7, 160)    0           batch_normalization_433[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_457 (Conv2D)             (None, 7, 7, 160)    179200      activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_462 (Conv2D)             (None, 7, 7, 160)    179200      activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_429 (BatchN (None, 7, 7, 160)    480         conv2d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_434 (BatchN (None, 7, 7, 160)    480         conv2d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 7, 7, 160)    0           batch_normalization_429[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 7, 7, 160)    0           batch_normalization_434[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_42 (AveragePo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_455 (Conv2D)             (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_458 (Conv2D)             (None, 7, 7, 192)    215040      activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_463 (Conv2D)             (None, 7, 7, 192)    215040      activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_464 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_42[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_427 (BatchN (None, 7, 7, 192)    576         conv2d_455[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_430 (BatchN (None, 7, 7, 192)    576         conv2d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_435 (BatchN (None, 7, 7, 192)    576         conv2d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_436 (BatchN (None, 7, 7, 192)    576         conv2d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 7, 7, 192)    0           batch_normalization_427[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 7, 7, 192)    0           batch_normalization_430[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 7, 7, 192)    0           batch_normalization_435[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 7, 7, 192)    0           batch_normalization_436[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_427[0][0]             \n",
      "                                                                 activation_430[0][0]             \n",
      "                                                                 activation_435[0][0]             \n",
      "                                                                 activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_469 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_441 (BatchN (None, 7, 7, 192)    576         conv2d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 7, 7, 192)    0           batch_normalization_441[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_470 (Conv2D)             (None, 7, 7, 192)    258048      activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_442 (BatchN (None, 7, 7, 192)    576         conv2d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 7, 7, 192)    0           batch_normalization_442[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_466 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_471 (Conv2D)             (None, 7, 7, 192)    258048      activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_438 (BatchN (None, 7, 7, 192)    576         conv2d_466[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_443 (BatchN (None, 7, 7, 192)    576         conv2d_471[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 7, 7, 192)    0           batch_normalization_438[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 7, 7, 192)    0           batch_normalization_443[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_467 (Conv2D)             (None, 7, 7, 192)    258048      activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_472 (Conv2D)             (None, 7, 7, 192)    258048      activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_439 (BatchN (None, 7, 7, 192)    576         conv2d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_444 (BatchN (None, 7, 7, 192)    576         conv2d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 7, 7, 192)    0           batch_normalization_439[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 7, 7, 192)    0           batch_normalization_444[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_43 (AveragePo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_465 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_468 (Conv2D)             (None, 7, 7, 192)    258048      activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_473 (Conv2D)             (None, 7, 7, 192)    258048      activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_474 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_43[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_437 (BatchN (None, 7, 7, 192)    576         conv2d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_440 (BatchN (None, 7, 7, 192)    576         conv2d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_445 (BatchN (None, 7, 7, 192)    576         conv2d_473[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_446 (BatchN (None, 7, 7, 192)    576         conv2d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 7, 7, 192)    0           batch_normalization_437[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 7, 7, 192)    0           batch_normalization_440[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 7, 7, 192)    0           batch_normalization_445[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 7, 7, 192)    0           batch_normalization_446[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_437[0][0]             \n",
      "                                                                 activation_440[0][0]             \n",
      "                                                                 activation_445[0][0]             \n",
      "                                                                 activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_477 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_449 (BatchN (None, 7, 7, 192)    576         conv2d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 7, 7, 192)    0           batch_normalization_449[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_478 (Conv2D)             (None, 7, 7, 192)    258048      activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_450 (BatchN (None, 7, 7, 192)    576         conv2d_478[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_450 (Activation)     (None, 7, 7, 192)    0           batch_normalization_450[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_475 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_479 (Conv2D)             (None, 7, 7, 192)    258048      activation_450[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_447 (BatchN (None, 7, 7, 192)    576         conv2d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_451 (BatchN (None, 7, 7, 192)    576         conv2d_479[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 7, 7, 192)    0           batch_normalization_447[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_451 (Activation)     (None, 7, 7, 192)    0           batch_normalization_451[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_476 (Conv2D)             (None, 3, 3, 320)    552960      activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_480 (Conv2D)             (None, 3, 3, 192)    331776      activation_451[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_448 (BatchN (None, 3, 3, 320)    960         conv2d_476[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_452 (BatchN (None, 3, 3, 192)    576         conv2d_480[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 3, 3, 320)    0           batch_normalization_448[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_452 (Activation)     (None, 3, 3, 192)    0           batch_normalization_452[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling2D) (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_448[0][0]             \n",
      "                                                                 activation_452[0][0]             \n",
      "                                                                 max_pooling2d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_485 (Conv2D)             (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_457 (BatchN (None, 3, 3, 448)    1344        conv2d_485[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_457 (Activation)     (None, 3, 3, 448)    0           batch_normalization_457[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_482 (Conv2D)             (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_486 (Conv2D)             (None, 3, 3, 384)    1548288     activation_457[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_454 (BatchN (None, 3, 3, 384)    1152        conv2d_482[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_458 (BatchN (None, 3, 3, 384)    1152        conv2d_486[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_454 (Activation)     (None, 3, 3, 384)    0           batch_normalization_454[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_458 (Activation)     (None, 3, 3, 384)    0           batch_normalization_458[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_483 (Conv2D)             (None, 3, 3, 384)    442368      activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_484 (Conv2D)             (None, 3, 3, 384)    442368      activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_487 (Conv2D)             (None, 3, 3, 384)    442368      activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_488 (Conv2D)             (None, 3, 3, 384)    442368      activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_44 (AveragePo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_481 (Conv2D)             (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_455 (BatchN (None, 3, 3, 384)    1152        conv2d_483[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_456 (BatchN (None, 3, 3, 384)    1152        conv2d_484[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_459 (BatchN (None, 3, 3, 384)    1152        conv2d_487[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_460 (BatchN (None, 3, 3, 384)    1152        conv2d_488[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_489 (Conv2D)             (None, 3, 3, 192)    245760      average_pooling2d_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_453 (BatchN (None, 3, 3, 320)    960         conv2d_481[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_455 (Activation)     (None, 3, 3, 384)    0           batch_normalization_455[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_456 (Activation)     (None, 3, 3, 384)    0           batch_normalization_456[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_459 (Activation)     (None, 3, 3, 384)    0           batch_normalization_459[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_460 (Activation)     (None, 3, 3, 384)    0           batch_normalization_460[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_461 (BatchN (None, 3, 3, 192)    576         conv2d_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_453 (Activation)     (None, 3, 3, 320)    0           batch_normalization_453[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_455[0][0]             \n",
      "                                                                 activation_456[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 3, 3, 768)    0           activation_459[0][0]             \n",
      "                                                                 activation_460[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_461 (Activation)     (None, 3, 3, 192)    0           batch_normalization_461[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_453[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 activation_461[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_494 (Conv2D)             (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_466 (BatchN (None, 3, 3, 448)    1344        conv2d_494[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_466 (Activation)     (None, 3, 3, 448)    0           batch_normalization_466[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_491 (Conv2D)             (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_495 (Conv2D)             (None, 3, 3, 384)    1548288     activation_466[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_463 (BatchN (None, 3, 3, 384)    1152        conv2d_491[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_467 (BatchN (None, 3, 3, 384)    1152        conv2d_495[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_463 (Activation)     (None, 3, 3, 384)    0           batch_normalization_463[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_467 (Activation)     (None, 3, 3, 384)    0           batch_normalization_467[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_492 (Conv2D)             (None, 3, 3, 384)    442368      activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_493 (Conv2D)             (None, 3, 3, 384)    442368      activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_496 (Conv2D)             (None, 3, 3, 384)    442368      activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_497 (Conv2D)             (None, 3, 3, 384)    442368      activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_45 (AveragePo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_490 (Conv2D)             (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_464 (BatchN (None, 3, 3, 384)    1152        conv2d_492[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_465 (BatchN (None, 3, 3, 384)    1152        conv2d_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_468 (BatchN (None, 3, 3, 384)    1152        conv2d_496[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_469 (BatchN (None, 3, 3, 384)    1152        conv2d_497[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_498 (Conv2D)             (None, 3, 3, 192)    393216      average_pooling2d_45[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_462 (BatchN (None, 3, 3, 320)    960         conv2d_490[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_464 (Activation)     (None, 3, 3, 384)    0           batch_normalization_464[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_465 (Activation)     (None, 3, 3, 384)    0           batch_normalization_465[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_468 (Activation)     (None, 3, 3, 384)    0           batch_normalization_468[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_469 (Activation)     (None, 3, 3, 384)    0           batch_normalization_469[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_470 (BatchN (None, 3, 3, 192)    576         conv2d_498[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_462 (Activation)     (None, 3, 3, 320)    0           batch_normalization_462[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_464[0][0]             \n",
      "                                                                 activation_465[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3, 3, 768)    0           activation_468[0][0]             \n",
      "                                                                 activation_469[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_470 (Activation)     (None, 3, 3, 192)    0           batch_normalization_470[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_462[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_10[0][0]             \n",
      "                                                                 activation_470[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# display the architecture of the composed model\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature map has shape (3, 3, 2048). That’s the feature on top of which we\n",
    "stick the densely connected classifier. we’ll start by running instances of the previously introduced ImageDataGenerator to extract images as Numpy arrays as well as their labels. we’ll extract features from these images by calling the predict method of the conv_base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2593 images belonging to 5 classes.\n",
      "Found 865 images belonging to 5 classes.\n",
      "Found 865 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = '/media/hi8826mo-s/BEEE-DE51/Ultimi/EDAN95_Applied_Machine_Learning/labs/flowers_split'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 1\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 3, 3, 2048))\n",
    "    #labels = np.zeros(shape=(sample_count))         # binary class\n",
    "    labels = np.zeros(shape=(sample_count, 5))       # XXXXXXXXXX multiclass 5\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "            directory,\n",
    "            target_size=(150, 150),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical')\n",
    "    \n",
    "    # Note that because generators yield data indefinitely in a loop,\n",
    "    # we must break after every image has been seen once.\n",
    "    \n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        #print(i)\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "        \n",
    "    return features, labels\n",
    "    \n",
    "train_features, train_labels = extract_features(train_dir, 2595)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 865)\n",
    "test_features, test_labels = extract_features(test_dir, 866)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The extracted features are currently of shape (samples, 3, 3, 2048) . You’ll feed them\n",
    "to a densely connected classifier, so first you must flatten them to (samples, 3*3*2048 = 18432) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2595, 3 * 3 * 2048))\n",
    "validation_features = np.reshape(validation_features, (865, 3 * 3 * 2048))\n",
    "test_features = np.reshape(test_features, (866, 3 * 3 * 2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can define your densely connected classifier (note the use of drop-\n",
    "out for regularization) and train it on the data and labels that you just recorded.\n",
    "\n",
    "Training is very fast, because you only have to deal with two Dense layers—an epoch\n",
    "takes less than one second even on CPU ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2595 samples, validate on 865 samples\n",
      "Epoch 1/10\n",
      "2595/2595 [==============================] - 23s 9ms/step - loss: 0.9559 - acc: 0.6694 - val_loss: 0.7633 - val_acc: 0.7064\n",
      "Epoch 2/10\n",
      "2595/2595 [==============================] - 14s 6ms/step - loss: 0.4227 - acc: 0.8482 - val_loss: 0.6121 - val_acc: 0.7954\n",
      "Epoch 3/10\n",
      "2595/2595 [==============================] - 14s 5ms/step - loss: 0.2468 - acc: 0.9179 - val_loss: 0.6231 - val_acc: 0.7896\n",
      "Epoch 4/10\n",
      "2595/2595 [==============================] - 14s 5ms/step - loss: 0.1467 - acc: 0.9630 - val_loss: 0.6184 - val_acc: 0.8012\n",
      "Epoch 5/10\n",
      "2595/2595 [==============================] - 14s 5ms/step - loss: 0.0911 - acc: 0.9788 - val_loss: 0.7855 - val_acc: 0.7792\n",
      "Epoch 6/10\n",
      "2595/2595 [==============================] - 14s 5ms/step - loss: 0.0647 - acc: 0.9892 - val_loss: 0.6646 - val_acc: 0.8139\n",
      "Epoch 7/10\n",
      "2595/2595 [==============================] - 14s 5ms/step - loss: 0.0389 - acc: 0.9961 - val_loss: 0.7462 - val_acc: 0.7908\n",
      "Epoch 8/10\n",
      "2595/2595 [==============================] - 14s 5ms/step - loss: 0.0263 - acc: 0.9961 - val_loss: 0.7103 - val_acc: 0.8185\n",
      "Epoch 9/10\n",
      "2595/2595 [==============================] - 14s 6ms/step - loss: 0.0201 - acc: 0.9973 - val_loss: 0.7517 - val_acc: 0.8104\n",
      "Epoch 10/10\n",
      "2595/2595 [==============================] - 14s 6ms/step - loss: 0.0152 - acc: 0.9973 - val_loss: 0.8039 - val_acc: 0.8092\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_dim=3 * 3 * 2048))\n",
    "#model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                  epochs=10,\n",
    "                  batch_size=20,\n",
    "                  validation_data=(validation_features, validation_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866/866 [==============================] - 0s 510us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.8198614318706697\n"
     ]
    }
   ],
   "source": [
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Second technique - Feature extraction with data augmentation <a class=\"anchor\" id=\"section4\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's review the second technique for doing feature extraction, which is much slower and more expensive, but which allows us to leverage data augmentation during training: extending the conv_base model and running it end-to-end on the inputs. \n",
    "\n",
    "Note that this technique is in fact so expensive that you should only attempt it if you have access to a GPU: it is absolutely intractable on CPU. If you cannot run your code on GPU, then the previous technique is the way to go.\n",
    "\n",
    "Because models behave just like layers, you can add a model (like our conv_base) to a Sequential model just like you would add a layer. So you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu', input_dim=3*3*2048))\n",
    "model.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 3, 3, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 512)               9437696   \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 31,243,045\n",
      "Trainable params: 31,208,613\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the convolutional base of InceptionV3 has 21,802,784 parameters, which is\n",
    "very large. The classifier you’re adding on top has 9,437,696 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Freezing the convulational base\n",
    "Before we compile and train the model, it’s very important to freeze the convolutional base. Freezing a layer or set of layers means preventing their weights from being updated during training. If you don’t do this, then the representations that were previously learned by the convolutional base will be modified during training. Because the Dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.\n",
    "\n",
    "In Keras, you freeze a network by setting its trainable attribute to False :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weights before freezing the conv base: 192\n"
     ]
    }
   ],
   "source": [
    "print('This is the number of trainable weights '\n",
    "'before freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weights after freezing the conv base: 4\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = False\n",
    "print('This is the number of trainable weights '\n",
    "'after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this setup, only the weights from the two Dense layers that we added will be\n",
    "trained. That’s a total of four weight tensors: two per layer (the main weight matrix\n",
    "and the bias vector). Note that in order for these changes to take effect, we must first\n",
    "compile the model. If we ever modify weight trainability after compilation, we\n",
    "should then recompile the model, or these changes will be ignored.\n",
    "\n",
    "Now we can start training your model, with the same data-augmentation configuration that we used in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2593 images belonging to 5 classes.\n",
      "Found 865 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 180s 2s/step - loss: 0.7253 - acc: 0.7383 - val_loss: 2.9460 - val_acc: 0.5399\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 158s 2s/step - loss: 0.7229 - acc: 0.7368 - val_loss: 2.2359 - val_acc: 0.6023\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 156s 2s/step - loss: 0.7223 - acc: 0.7352 - val_loss: 2.4288 - val_acc: 0.5827\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 158s 2s/step - loss: 0.6555 - acc: 0.7547 - val_loss: 2.9317 - val_acc: 0.5503\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 161s 2s/step - loss: 0.7093 - acc: 0.7506 - val_loss: 1.9994 - val_acc: 0.6208\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 156s 2s/step - loss: 0.6646 - acc: 0.7618 - val_loss: 3.0776 - val_acc: 0.5514\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 158s 2s/step - loss: 0.6743 - acc: 0.7507 - val_loss: 1.8646 - val_acc: 0.6254\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 160s 2s/step - loss: 0.6269 - acc: 0.7724 - val_loss: 2.3576 - val_acc: 0.6069\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 156s 2s/step - loss: 0.6561 - acc: 0.7510 - val_loss: 2.5818 - val_acc: 0.5942\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 167s 2s/step - loss: 0.6229 - acc: 0.7670 - val_loss: 1.8631 - val_acc: 0.6520\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "# Setting up a data augmentation configuration via ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')\n",
    "\n",
    "# Rescale all images by 1/255\n",
    "#train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150),       \n",
    "        batch_size=25,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=865,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now finally evaluate this model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 865 images belonging to 5 classes.\n",
      "test acc: 0.6693641543388367\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=866,\n",
    "        class_mode='categorical')\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=1)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('flowers_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our results again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4VGX2wPHvoRlQmoBlQYouFkRAiFjA3tBVUbCA6Ir8gHWl2F0QFESxoAK6sgq6WFHWitjXwooNlrAgTSlCQKqAgCAIJDm/P84EJiFlkkzmTjmf55knM3fu3HvmJjn3vW+7oqo455xLDRWCDsA551zseNJ3zrkU4knfOedSiCd955xLIZ70nXMuhXjSd865FOJJPwWJSEUR2SYiDaO5bpBE5I8iEvX+xyJyjohkhr1eKCKnRrJuKfb1rIjcVdrPOxeJSkEH4IonItvCXlYDdgLZodd/UdUJJdmeqmYDB0R73VSgqkdFYzsi0hO4RlXPCNt2z2hs27mieNJPAKq6J+mGSpI9VfXTwtYXkUqqmhWL2Jwrjv89xhev3kkCInK/iPxLRF4Vka3ANSJysohME5HNIrJGRJ4Qkcqh9SuJiIpI49Drl0PvfygiW0XkWxFpUtJ1Q+9fICKLRGSLiPxdRL4Wke6FxB1JjH8RkSUisklEngj7bEURGSUiG0XkR6BDEcdnsIhMzLdsjIiMDD3vKSLfh77Pj6FSeGHbWikiZ4SeVxORl0KxzQfaFLDfpaHtzheRS0LLjwOeBE4NVZ1tCDu2Q8M+f0Pou28UkUkicmgkx6Ykxzk3HhH5VER+EZG1InJn2H7uDh2TX0UkQ0T+UFBVmoh8lft7Dh3PqaH9/AIMFpGmIjIl9F02hI5bzbDPNwp9x/Wh9x8XkbRQzMeErXeoiGwXkTqFfV9XDFX1RwI9gEzgnHzL7gd2ARdjJ/KqwAnAidjV3OHAIqBvaP1KgAKNQ69fBjYA6UBl4F/Ay6VY9yBgK9Ax9N6twG6geyHfJZIY3wFqAo2BX3K/O9AXmA80AOoAU+3PucD9HA5sA/YP2/bPQHro9cWhdQQ4C9gBtAi9dw6QGbatlcAZoeePAv8BagONgAX51r0SODT0O7k6FMPBofd6Av/JF+fLwNDQ8/NCMbYC0oB/AJ9HcmxKeJxrAuuAm4D9gBpA29B7A4HvgKah79AKOBD4Y/5jDXyV+3sOfbcs4K9ARezv8UjgbKBK6O/ka+DRsO8zL3Q89w+t3y703jhgeNh+bgPeDvr/MJEfgQfgjxL+wgpP+p8X87nbgddDzwtK5E+HrXsJMK8U6/YAvgx7T4A1FJL0I4zxpLD33wJuDz2filVz5b53Yf5ElG/b04CrQ88vABYVse57QJ/Q86KS/orw3wVwY/i6BWx3HvCn0PPikv4LwANh79XA2nEaFHdsSnicrwUyClnvx9x48y2PJOkvLSaGy4EZoeenAmuBigWs1w5YBkjo9WygU7T/r1Lp4dU7yeOn8BcicrSIvB+6XP8VGAbULeLza8Oeb6foxtvC1v1DeBxq/6UrC9tIhDFGtC9geRHxArwCdA09vxrY0/gtIheJyPRQ9cZmrJRd1LHKdWhRMYhIdxH5LlRFsRk4OsLtgn2/PdtT1V+BTUD9sHUi+p0Vc5wPA5YUEsNhWOIvjfx/j4eIyGsisioUw/P5YshU6zSQh6p+jV01tBeR5kBD4P1SxuTwOv1kkr+74lisZPlHVa0B3IOVvMvTGqwkCoCICHmTVH5liXENlixyFdel9F/AOSLSAKt+eiUUY1XgDeBBrOqlFvDvCONYW1gMInI48BRWxVEntN0fwrZbXPfS1ViVUe72qmPVSKsiiCu/oo7zT8ARhXyusPd+C8VULWzZIfnWyf/9HsZ6nR0XiqF7vhgaiUjFQuJ4EbgGuyp5TVV3FrKei4An/eRVHdgC/BZqCPtLDPb5HtBaRC4WkUpYPXG9corxNeBmEakfatT7W1Erq+o6rAriOWChqi4OvbUfVs+8HsgWkYuwuudIY7hLRGqJjWPoG/beAVjiW4+d/3piJf1c64AG4Q2q+bwK/J+ItBCR/bCT0peqWuiVUxGKOs6TgYYi0ldEqohIDRFpG3rvWeB+ETlCTCsRORA72a3FOgxUFJHehJ2giojhN2CLiByGVTHl+hbYCDwg1jheVUTahb3/ElYddDV2AnBl4Ek/ed0GXIc1rI7FSrrlKpRYrwJGYv/ERwCzsBJetGN8CvgMmAvMwErrxXkFq6N/JSzmzcAtwNtYY+jl2MkrEkOwK45M4EPCEpKqzgGeAP4bWudoYHrYZz8BFgPrRCS8mib38x9h1TBvhz7fEOgWYVz5FXqcVXULcC7QGWs4XgScHnr7EWASdpx/xRpV00LVdr2Au7BG/T/m+24FGQK0xU4+k4E3w2LIAi4CjsFK/Suw30Pu+5nY73mXqn5Twu/u8sltHHEu6kKX66uBy1X1y6DjcYlLRF7EGoeHBh1LovPBWS6qRKQDdrn+O9blLwsr7TpXKqH2kY7AcUHHkgy8esdFW3tgKXbZ3wG41BveXGmJyIPYWIEHVHVF0PEkA6/ecc65FOIlfeecSyFxV6dft25dbdy4cdBhOOdcQpk5c+YGVS2qizQQh0m/cePGZGRkBB2Gc84lFBEpblQ64NU7zjmXUjzpO+dcCvGk75xzKcSTvnPOpRBP+s45l0I86TvnYm7CBGjcGCpUsJ8TJhT3CRctcddl0zmX3CZMgN69Yft2e718ub0G6FbaeURdxLyk75yLqUGD9ib8XNu323JX/jzpO+diakUh06YVttxFlyd951xMNSzkxpaFLXfR5UnfORdTw4dDtWp5l1WrZstTVSwbtj3pO+diqls3GDcOGjUCEfs5blzqNuLmNmwvXw6qexu2yyvxx918+unp6eoTrjnnUkXjxpbo82vUCDIzI9+OiMxU1fTi1vOSvnPOBSjWDdue9J1zLkCxbtj2pO9cjPgoVFeQWDdse9J3LgZi3VjnEkesG7a9Ide5GIhWY51zhfGGXOfiiI9CdfHCk75zMeCjUF288KTvyo03XO7lo1BdvPCk78qFN1zm5aNQXbzwhlxXLrzh0rnY8oZcFyhvuHQuPkWU9EWkg4gsFJElIjKggPdHicjs0GORiGwOey877L3J0Qzexa94arj0toW8/HjslZLHQlWLfAAVgR+Bw4EqwHdAsyLW7weMD3u9rbh9hD/atGmjLvG9/LJqtWqqVqNvj2rVbHkqxhEv/HjslWzHAsjQCHJssXX6InIyMFRVzw+9Hhg6WTxYyPrfAENU9ZPQ622qekCkJyGv008eEybYLfBWrLAS/vDhsW+49LaFvPx47JVsxyLSOv1Ikv7lQAdV7Rl6fS1woqr2LWDdRsA0oIGqZoeWZQGzgSzgIVWdVMDnegO9ARo2bNhmeUG/CedKoUIFK8PlJwI5ObGPJ2h+PPZKtmMRzYZcKWBZYWeKLsAbuQk/pGEokKuB0SJyxD4bUx2nqumqml6vXr0IQnIuMvHUthAP/HjslarHIpKkvxI4LOx1A2B1Iet2AV4NX6Cqq0M/lwL/AY4vcZTOlZIPisrLj8deqXosIkn6M4CmItJERKpgiX2fXjgichRQG/g2bFltEdkv9Lwu0A5YEI3AnYuED4rKy4/HXql6LCIanCUiFwKjsZ4841V1uIgMw1qLJ4fWGQqkqeqAsM+dAowFcrATzGhV/WdR+/KGXOecK7moNeTGmid955wrOR+R65xzbh9Jk/RTcmSdc86VUKWgA4iG3Bkdt2+317kzOkLyN8o451xJJEVJf9CgvQk/1/bttjyW/GrDORfvkiLpx8OMjvE0f7yffJxzhUmKpB8PI+vi6WojXk4+zrn4kxRJPx5G1sXD1QbEz8nHORefkiLpx8PIuni42oD4Ofk45+JTUiR9sASfmWmz42Vmxr7XTjxcbUD8nHycc/EpaZJ+0OLhagPi5+TjnItPSdFPP1506xb8uIDc/Qd98xLnXHzypJ+E4uHk45yLT16945xzKcSTvnPOpRBP+s45l0I86TvnXArxpO+ccynEk75zzqUQT/rOOZdCPOk751wK8aTvnHMpxJO+c86lEE/6zjmXQjzpO+dcCvGk75xzKcSTvnPOpRBP+s45l0I86TvnXArxpO+ccynEk75zzqUQT/rOOZdCPOk751wK8aTvnHMpxJO+c86lkIiSvoh0EJGFIrJERAYU8P4oEZkdeiwSkc1h710nIotDj+uiGbxzzrmSqVTcCiJSERgDnAusBGaIyGRVXZC7jqreErZ+P+D40PMDgSFAOqDAzNBnN0X1WzjnnItIJCX9tsASVV2qqruAiUDHItbvCrwaen4+8Imq/hJK9J8AHcoSsHPOudKLJOnXB34Ke70ytGwfItIIaAJ8XpLPikhvEckQkYz169dHErdzzrlSiCTpSwHLtJB1uwBvqGp2ST6rquNUNV1V0+vVqxdBSM4550ojkqS/Ejgs7HUDYHUh63Zhb9VOST/rnHOunEWS9GcATUWkiYhUwRL75PwrichRQG3g27DFHwPniUhtEakNnBda5pxzLgDF9t5R1SwR6Ysl64rAeFWdLyLDgAxVzT0BdAUmqqqGffYXEbkPO3EADFPVX6L7FZxzzkVKwnJ0XEhPT9eMjIygw3DOuYQiIjNVNb249XxErnPOpRBP+s45l0I86TvnXArxpO+ccynEk75zzqUQT/rOOZdCPOk751wK8aTvnHMpxJO+c86lEE/6zjmXQjzpO+dcCvGk75xzKcSTvnPOpRBP+s45l0I86TvnXArxpO+ccynEk75zzqUQT/rOOZdCPOk751wK8aTvnHMpxJO+c86lEE/6zjmXQjzpO+dcCvGk75xzKcSTvnPOpRBP+s45l0I86TvnXArxpO+ccynEk75zzqUQT/rOOZdCPOk751wK8aTvnHMpxJO+c86lkIiSvoh0EJGFIrJERAYUss6VIrJAROaLyCthy7NFZHboMTlagTvnXDJZsgTWrSv//VQqbgURqQiMAc4FVgIzRGSyqi4IW6cpMBBop6qbROSgsE3sUNVWUY7bOeeSxq5dcOWVsHMnzJ0LFcqxDqbYpA+0BZao6lIAEZkIdAQWhK3TCxijqpsAVPXnaAfqnHPJ6r77YNYsmDSpfBM+RFa9Ux/4Kez1ytCycEcCR4rI1yIyTUQ6hL2XJiIZoeWXFrQDEekdWidj/fr1JfoCzjmXyKZNgwcegO7doWPH8t9fJCV9KWCZFrCdpsAZQAPgSxFprqqbgYaqulpEDgc+F5G5qvpjno2pjgPGAaSnp+fftnPOJaXt2+HPf4YGDWD06NjsM5KS/krgsLDXDYDVBazzjqruVtVlwELsJICqrg79XAr8Bzi+jDE751xSuPNOWLwYnn8eataMzT4jSfozgKYi0kREqgBdgPy9cCYBZwKISF2sumepiNQWkf3Clrcjb1uAc86lpH//G8aMgVtugTPPjN1+i63eUdUsEekLfAxUBMar6nwRGQZkqOrk0HvnicgCIBu4Q1U3isgpwFgRycFOMA+F9/pxzrlUtGkT9OgBxxwDw4fHdt+iGl9V6Onp6ZqRkRF0GM45V26uuQb+9S9rxG3TJjrbFJGZqppe3Ho+Itc552Lo9ddhwgS4++7oJfyS8KTvnHMxsmYN3HADnHACDBwYTAye9J1zLgZUoWdP66b54otQuXIwcUTST98551wZPfssfPABPPEEHH10cHF4Sd8558rZ0qXWNfPss6FPn2Bj8aTvnHPlKDsbrrsOKlWC554r/7l1iuPVO845V45GjoSvvrJ6/MMOK3798uYlfeecKydz5sDgwdCpk/XNjwee9J1zrhzs3AnXXgu1a8PTT4MUNHVlALx6xznnysG991pJf/JkqFcv6Gj28pK+c85F2TffwMMPw//9H1x8cdDR5OVJ3znnomjbNpsjv2FDa8SNN16945xzUXTHHdYvf8oUqFEj6Gj25SV955yLkg8/tEbbW2+F008POpqCedJ3LsXs3AkjRsD06UFHklx++cXq8I89Fu6/P+hoCudJ37kUM2YM/O1vcNJJdsemDz+0ycBc2fTpA+vXw0svQVpa0NEUzpO+cylkyxa7U9PZZ1sj45IlcOGF0KqVzfGelRV0hIlp4kR7DB0Kx8f5XcA96TuXQkaMsGqIRx6xCcB+/NFuyp2VZSNG//hHePJJm/7XRWb1arjxRjjxRLuCinee9F1KmDgRPvkk6CiCtXo1jBoFV1+9tzRapYpNBjZ3LrzzDtSvD/36QaNGMGwYbNwYbMzxTtXudfv77za3TqUE6A/pSd8lvR9+sFLs5ZfDzz8HHU1whg2zEv199+37XoUKcMkl8PXX8OWXVt8/ZIj1Nb/5ZlixIvbxJoKxY+Hjj+3K6cgjg44mMp70XdK74w6oVs2qLO65J+hogrFwod3E44Yb4PDDi163fXt4910r/V9+uTX8HnGEDTiaNy828SaCJUvgttvg3HOteidReNJ3Se3TT+G99+wm1H36wDPP2HwoqWbwYKha1X5GqnlzeOEFq/fv2xfeeguOOw4uusiuBlK5x092tp0Eq1SB8ePjZzK1SHjSd0krO9sGyTRpAv37W3VFrVpWXZFKCWv6dHjjDbj9djjooJJ/vmFDawtYscKqiKZPh9NOg3btrB0gJyf6Mce7Rx6Bb7+1q6AGDYKOpmQ86bukNX68VVGMGAH77WdT3A4bZsPjJ00KOrrYUIUBAyzZ33pr2bZ14IF2xbR8ufXwWbMGLr3Urgieew527YpOzPFu9myrJrziCujaNehoSk40zoo86enpmpGREXQYLsFt3WrdD488EqZO3Xv5nZVlfdJ37IAFC+xkkMw++gguuMCSdLTvzZqVBa+/brNJfved9fy55Rbo3RuqV4/uvuLFzp2Qng4bNliBom7doCPaS0Rmqmp6cet5Sd8lpYcesp46I0fmrW+tVMmqKpYuhdGjg4svFnJyrN/44YdDr17R336lSlbSnTXLRvUeeaRVITVsCIMGwbp10d9n0O65xxqzn302vhJ+iahqXD3atGmjzpVFZqbqfvupXnNN4etcfLFq9eqqa9bELq5Ye+klVVB99dXY7XP6dNVOnVRF7Hdwww2qS5bEbv/l6csv7Xv17h10JAUDMjSCHOvVOy7pXH211dkvXFj4jagXL7aJsf78Zyu1JZudO+Hoo60efsYM64cfSwsXwqOP2oClrCzr+vm3v0Hr1rGNI1q2boWWLe2q8bvv4IADgo5oX16941LStGnw6qtWzVBYwgdo2tR69Iwfb9UTyebppyEz06q5Yp3wAY46yrrHLltmv4uPPoI2baxP+6efJl7vqdtus+P5wgvxmfBLwkv6LmmowimnWO+SRYuK/+fcvNmS/zHHwBdfJFZf66L8+qsNpmrVKn6mntiyxU5Eo0fD2rV2ArjzTujcGSpWDDq6or3/vo1NuPNOa7SOV17SdynntdespD98eGSlsVq1bN7zL7+0fuzJ4tFHrXfJQw8FHcleNWta9c6yZTBunJ2YrrrKTrp33QUZGfFZ+t+wwebIb97cuvsmAy/pu6Tw++9Wh127tiWQSEuP2dlWz7xli83RE8/zoEdi7Vor5V98sU0yF6+ys63d5emnbdxEdrZVx112GXTqZFNBBH0FoApXXmkD0GbMsDr9eOYlfZdSRo+2ap2RI0uWLCpWzPvZRDdsmA2Siuc7N4Ed986drfrp559teufjj7ergDPOgEMPtW6mH35ojdJBePVVuwIcNiz+E35JeEnfJbx166ya4KyzSj/StlMn+Pe/rS3gD3+IbnyxsngxNGtmg6PGjAk6mtLZts0afd96y+ZM2rrVBnpddJH9jjp0iE1D6sqVNs9Qs2Y2uC/oq45IRLWkLyIdRGShiCwRkQGFrHOliCwQkfki8krY8utEZHHocV3kXyExzZoFLVrAvfdaQ6Erf/fcYyNsR4wo/TYeeQR277b65UQ1eLCNML777qAjKb0DDrDuna+8YrcefP99q2L55BOb9qBePZv64cUX7WYw5SEnx+bI37XLeuskQsIvkeI68gMVgR+Bw4EqwHdAs3zrNAVmAbVDrw8K/TwQWBr6WTv0vHZR+0vkwVm7dqm2aqVataoNiqlRQ/Xuu1U3bgw6suQ1Z45qhQqqN99c9m3deaf93v7737JvK9ZmzLDY77kn6EjKx+7dqv/5j2r//qoNGth3rVRJ9dxzVf/xD9XVq6O3ryeftO0/9VT0thkLRDg4K5KkfzLwcdjrgcDAfOuMAHoW8NmuwNiw12OBrkXtL5GT/sMP2xF9803VWbNUO3e219Wrq951l+qGDUFHmFxyclTPOUe1du3onFi3bFE9+GDVU06xbSeKnBzVs85SrVvXvkOyy8mxE/OAAapHHmn/YyL2e3v0UdUffyz9thcutELb+ecn1t+AanST/uXAs2GvrwWezLfOpFDi/xqYBnQILb8dGBy23t3A7QXsozeQAWQ0bNgwJgco2pYsUU1LU7300rzL58xRvfJK+6M84ADVv/1N9eefg4kx2bz/vv0Fjx4dvW0++2zspy4oq48/tpgffzzoSGIvJ0d1/nzV++5TPf54Ow5gV9z33qs6d27kyXv3btUTT7RCxKpV5Rt3eYhm0r+igKT/93zrvAe8DVQGmgArgVrAHQUk/duK2l8ilvRzclTPPtuqc1auLHidefNUu3a15F+tmurtt6uuXRvbOJPJrl2qRx9tJb1du6K33awsSx6HHab622/R2255yc62eJs0Uf3996CjCd7SpaqPPabarp39r4Fq06ZW2Jo+vegTwP332/oTJ8Yu3miKdfXO00D3sNefASekSvXO889rxHWA339vE4FVqGCXkbfcktyTfpWX3HrXd96J/ranTrVt33tv9Lcdba+8YrG+/HLQkcSfNWtUn35a9bzzrP4frD2gXz/Vzz+3kn2umTNtnS5dgou3rKKZ9CuFGmCbhDXkHptvnQ7AC6HndYGfgDqhBtxloUbc2qHnBxa1v0RL+uvWqR54oGr79lbqitTCharXXadasaJVC/Xvn5iXlEH45RfVOnWsHru86l2vuMKuyH76qXy2Hw07d6oefrhqy5Yl+9tLRb/8ovrii1b9mpZmma9OHdUePVQnT1Zt1kz10EMTu9NF1JK+bYsLgUWhXjyDQsuGAZeEngswElgAzAW6hH22B7Ak9Li+uH0lWtK/+mrVKlVUFywo3ecXL7Y/vIoVbSraPn1UV6yIbozJ5rbb7NJ91qzy28eyZfb76Nat/PZRVn//u/0Hf/RR0JEklm3brLNFt25WJZvbDvDhh0FHVjZRTfqxfCRS0v/gAzuCQ4eWfVtLl6r26mWXmFWq2Dzky5eXfbvJZskS1cqV7URZ3u66y36/335b/vsqqV9/Va1XT/XMMxOvl0k82bnT/o/feivoSMou0qTvI3JLads2m499//1tQFa0bru3fLlNlPXPf9rr7t1twFDjxtHZfqLr3Bk+/thGnx56aPnua9s2uxtUw4bwzTfBTFFcmKFDbQDgf/8LJ5wQdDQuHvjcO+Xs7rthxQqbMzya91lt1Aieegp+/NGG07/wgk0x8H//Z8tS2dSpNjx/wIDyT/hgo0MffBCmT7cRovFi3Tp47DEboeoJ35WUl/RLYcYMOOkk+Mtf4B//KN99rVpl0wuMG2fTBFxzjd1/tGnT8t1vvMnJgbZtbXKuhQuhatXY7ffEE2HNGtvv/vvHZr9F6dfPCgYLFtiViHPgJf1ys3s39OwJhxxipcDyVr8+PP643ci7f3+bM/7oo+Haa20q4FTx8sswc6Yd81glfLAqndGj7eQbDzfQ+PFHm464Vy9P+K50POmX0GOPwZw5NothzZqx2++hh9rUv8uWwa23WjVHs2Z2P9gFC2IXRxB++83aNU44Abp2jf3+27WDLl1sUrbly2O//3B33w1Vqtgkc86Vhif9Eli82BrPOne2mf6CcPDBlnyWLbPbt02ebHf1ueoqmDs3mJjK26OPWkl71KjgGlMffthup/i3vwWzf4D//c/meL/llti0abjk5Ek/QqpWh7/ffvD3vwcdDRx0kPXyycyEgQPtZhMtWti0tN99F3R00ZPbpnHFFVbiDkrDhnDHHfCvf8HXXwcTw8CBUKeOxeFcaXnSj9Bzz9lt3UaMiK9SVt26dk/YzEy79P/kE7sh9mWXWVfSRDdoEGRlxUd9+p13WhvLTTdZA28sffqp3eRl0KDYViu6JBRJZ/5YPuJxcNbatTbz3qmnxv9w919+scFitWrZ/D4jRybu4J2MDBscdeedQUey18svW0zPPRe7fWZnq7Zpo9qokU+q5gpHhIOzvKQfgZtussbEcePia4BOQWrXhiFDrM7/0kut0bd7d7txeCJRhdtuszslxdPdrLp2tS6cAwfarfxi4Y03rOfSffdFd0yIS01xnsKC9/77Vo87eLB1lUwUtWrB66/bTZ1ffBFOP93qxxPFpEnwxRcWfzxVZ1SoYF1o166NTZfd3Fs4Hnec9dRyrqx8cFYRtm61qRZq1LCeE1WqBB1R6UyaZP36q1e3rp4nnRR0REXbtcu6o6alwezZUKlS0BHt69pr7aT6/ffQpEn57ecf/4A+fazwceGF5bcfl/h8cFYUDB4MK1faVAuJmvDBqnm+/dYGNZ1+Ojz/fNARFe3JJ20Q0mOPxWfCB+s5VbGiNe6Wl23b7Ern9NPhggvKbz8utXjSL8S0adY188Yb4eSTg46m7Jo3t8m5Tj0Vrr8ebr7ZesXEmw0bLNF16ADnnx90NIWrX9/mAHrjDauGKg+jRtk8Ow89ZGMEnIsGr94pwK5d0KYNbN4M8+db9U6yyMqyft6jR8PZZ1t7RZ06QUe1V+68Mt99Z1Vr8WzHDjjqKDt+GRlW8o+W9evh8MPhvPPgzTejt12XvLx6pwweeQTmzbP61GRK+GDVJaNG2biDL7+0SczmzQs6KvPDD5bwe/eO/4QPVl02YoS1Ozz3XHS3PXy4nVQeeCC623XOS/r5LFpkI1svucQmN0tm06bZIK5t2+Cll4KbWiLXxRfb9MlLllhXzUSgalVmixfbIxqFhGXL7Aqie3frJuz3JXjGAAATC0lEQVRcJCIt6cdpM1kwcnKslFm1KjzxRNDRlL+TTrJqiU6dLPnfe681XgcxFuHTT+G996zknCgJH6yuffRomwxu+PDojBy+5x67IhsypOzbKqndu3ezcuVKfk+0gR0pJC0tjQYNGlC5cuXSbSCSEVyxfAQ5IveZZ2y05TPPBBZCIHbsUL32WvvunTqpbt0a2/1nZaked5xqkyYWSyLq3t1uc7lkSdm2M2uW3f93wIDoxFVSS5cu1fXr12tOog7jTnI5OTm6fv16Xbp06T7v4SNyS2btWmvgPP10u0tVKklLszt0jRxpffpPOcWqGGJl/HibIfThhy2WRPTAA1C5Mtx+e9m2M3CgDawLajbP33//nTp16iDeXSguiQh16tQp05WYJ/2Q/v2t4WzcuNTsHidiU/Z+9JGNTTjhBPj88/Lf79atVqXUrp3NEJqoDj3URs5OmlT64zZlih3/QYMs8QfFE358K+vvx5M+Nif966/bLJWpfjeic8+1/vwHHWTdBZ980hory8tDD9ktEEeNSvyT7a232g3sSzMGQtVK94cdZiNwnSsvKZ/0f/3VBmAdd5zPU57rj3+0nj1/+pP1m+/VC3bujP5+li+3UbfXXJMcN/hOS7PuvnPnwrPPluyzb75p914eNiyxqrgmTLATXYUK9nPChLJtb+PGjbRq1YpWrVpxyCGHUL9+/T2vd+3aFdE2rr/+ehYuXFjkOmPGjGFCWYNNVJFU/MfyEeuG3D59rOFs2rSY7jYhZGerDh5sDbynnKK6Zk10t9+1q2pamuqKFdHdbpByclRPO021bl3VTZsi+8yuXapHHql67LHWqB2kBQsWRLzuyy+rVqtmfx+5j2rVbHk0DBkyRB955JF9lufk5Gh2vM9xXs4K+j3hDbnF+/ZbG4DVr59Nl+vyqlDBpvN97TUbgJSebl08o2HaNLv13+23W5VGssjtwrlxox27SIwfb+NDHnwwuqN6y9ugQbB9e95l27fb8mhbsmQJzZs354YbbqB169asWbOG3r17k56ezrHHHsuwYcP2rNu+fXtmz55NVlYWtWrVYsCAAbRs2ZKTTz6Zn3/+GYDBgwczevToPesPGDCAtm3bctRRR/HNN98A8Ntvv9G5c2datmxJ165dSU9PZ/bs2fvENmTIEE444YQ98WmoPnTRokWcddZZtGzZktatW5OZmQnAAw88wHHHHUfLli0ZVB4HqziRnBli+YhVSX/nTitZHXaY6q+/xmSXCW3WLLuJR1qa6ksvlW1bOTmqJ52kesghse8eGis9e6pWqqS6cGHR623bZsehffv4uNlNSUr6InlL+bkPkejEEl7SX7x4sYqI/ve//93z/saNG1VVdffu3dq+fXudP3++qqq2a9dOZ82apbt371ZAP/jgA1VVveWWW/TBBx9UVdVBgwbpqFGj9qx/Z+hOPe+8846ef/75qqr64IMP6o033qiqqrNnz9YKFSrorFmz9okzN46cnBzt0qXLnv21bt1aJ0+erKqqO3bs0N9++00nT56s7du31+3bt+f5bEl5Sb8UHn7Y5tV56imbctgVrVUrq3M+8USbVviOOyA7u3Tbeu01K+kPHw4HHBDdOOPF/ffbIL/bbit6vdy5+XNvvJ5IGjYs2fKyOuKIIzghrPHn1VdfpXXr1rRu3Zrvv/+eBQsW7POZqlWrckFoitI2bdrsKW3n16lTp33W+eqrr+jSpQsALVu25NhC5gb57LPPaNu2LS1btuSLL75g/vz5bNq0iQ0bNnDxxRcDNqCqWrVqfPrpp/To0YOqVasCcOCBB5b8QJRRSib9H36wf8qrrrLGSheZevXsHrx9+sCjj8JFF8GmTSXbxu+/Wy+VVq3guuvKJ854cPDB1hX1vffs3rYF2bDBkn3HjjY2ItEMHw7VquVdVq2aLS8P+++//57nixcv5vHHH+fzzz9nzpw5dOjQocC+61XC5kSvWLEiWYV0q9ovdEuy8HU0gm5r27dvp2/fvrz99tvMmTOHHj167ImjoK6Vqhp4l9iUS/o5OdYbZf/9rZTlSqZyZevGOW4cfPaZlfx/+CHyz48ebb12Ro5MrPrr0rjpJjjiCBv/UFCueeABm/coUSdV69bN/g4aNbKrlEaN7HW3buW/719//ZXq1atTo0YN1qxZw8cffxz1fbRv357XQhNwzZ07t8AriR07dlChQgXq1q3L1q1beTM0JWrt2rWpW7cu7777LmCD3rZv3855553HP//5T3bs2AHAL7/8EvW4i5NySf+ZZ+Crr6ykevDBQUeTuHr1skFIW7ZY4n/vveI/s26dJbiOHeHMM8s/xqDtt5/9nS1YAGPH5n1v+XIYM8bubdCsWTDxRUO3bpCZaYWpzMzYJHyA1q1b06xZM5o3b06vXr1o165d1PfRr18/Vq1aRYsWLXjsscdo3rw5NfPdu7NOnTpcd911NG/enMsuu4wTw3qETJgwgccee4wWLVrQvn171q9fz0UXXUSHDh1IT0+nVatWjBo1KupxFyuSiv9YPsqzIXfVKtUaNVTPPDM+Gs2SwfLlqq1bW+PdAw8UfVx7946scTOZ5OSonnWW6oEHqoa32f35z9Yo/tNPwcVWkJI05Ca73bt3647QZFCLFi3Sxo0b6+7duwOOynhDboT69bMbpIwdm3iNZvGqYUObl79LF5uGoGvXfbvxwd4BS336pNaoZxEbbbx5s81iCnYsXnrJpv5o0CDY+Fzhtm3bRrt27WjZsiWdO3dm7NixVIrX+3eWQOJ/gwhNmmQ3BX/wQWjaNOhokku1ajYSs1Uru4XgokV2vHN7cahaL5aaNW3a4FTTooVVh40ZAzfcYJOq1axpx8rFr1q1ajFz5sygw4i6iEr6ItJBRBaKyBIR2edPVUS6i8h6EZkdevQMey87bPnkaAYfqS1brITZsmXxXehc6YjYTcLfe89uap6eblcAAB9+aL1+hgyBAHqoxYX77rPuqZ06wfvvW+KvXTvoqFwqKjbpi0hFYAxwAdAM6CoiBTU9/UtVW4Ue4TOP7Ahbfkl0wi6ZgQOtL/Qzz1jvE1d+LrzQJmyrXRvOOstGPN92m1Xp3Hhj0NEFp149u8r54Qe7qXq/fkFH5FJVJNU7bYElqroUQEQmAh2BffsvxaGvvrIBWDffnByTeiWCo46C6dPh6qv3zhj5zjt+wu3b16ax6NbNBm45F4RIkn594Kew1yuBgmaq6SwipwGLgFtUNfczaSKSAWQBD6nqpPwfFJHeQG+AhlEczrdzp9WlNmoU+TwoLjpq1YJ337XjvmGD3f821VWpAq+8EnQULtVFUqdfUD+X/EPV3gUaq2oL4FPghbD3GqrdrPdqYLSIHLHPxlTHqWq6qqbXi+INUh980C6nn3oqeYf7x7OKFWHoUBvM5b2lXCTOOOOMfQZajR49mhuLqRs8IPQPvnr1ai4v5G48Z5xxBhnFzBg4evRotod1P7vwwgvZvHlzJKEnjEiS/kogfB7EBsDq8BVUdaOq5s64/gzQJuy91aGfS4H/AMeXId6ILVhgA4G6doXQ1BvOuTjXtWtXJk6cmGfZxIkT6dq1a0Sf/8Mf/sAbb7xR6v3nT/offPABtYK8jVk5iKR6ZwbQVESaAKuALlipfQ8ROVRV14ReXgJ8H1peG9iuqjtFpC7QDhgRreALkzvVQvXqNuzfOVdyN99sU2pHU6tWRf9PXn755QwePJidO3ey3377kZmZyerVq2nfvj3btm2jY8eObNq0id27d3P//ffTsWPHPJ/PzMzkoosuYt68eezYsYPrr7+eBQsWcMwxx+yZ+gDgr3/9KzNmzGDHjh1cfvnl3HvvvTzxxBOsXr2aM888k7p16zJlyhQaN25MRkYGdevWZeTIkYwfPx6Anj17cvPNN5OZmckFF1xA+/bt+eabb6hfvz7vvPPOngnVcr377rvcf//97Nq1izp16jBhwgQOPvhgtm3bRr9+/cjIyEBEGDJkCJ07d+ajjz7irrvuIjs7m7p16/LZZ59F7XdQbNJX1SwR6Qt8DFQExqvqfBEZho0Amwz0F5FLsHr7X4DuoY8fA4wVkRzsquIhVS33BuCxY+Gbb+D55+22f865xFCnTh3atm3LRx99RMeOHZk4cSJXXXUVIkJaWhpvv/02NWrUYMOGDZx00klccsklhU5g9tRTT1GtWjXmzJnDnDlzaN269Z73hg8fzoEHHkh2djZnn302c+bMoX///owcOZIpU6ZQt27dPNuaOXMmzz33HNOnT0dVOfHEEzn99NOpXbs2ixcv5tVXX+WZZ57hyiuv5M033+Saa67J8/n27dszbdo0RIRnn32WESNG8Nhjj3HfffdRs2ZN5s6dC8CmTZtYv349vXr1YurUqTRp0iTq8/NENDhLVT8APsi37J6w5wOBgQV87hvguDLGWCKrVtksjuecA3/+cyz37FxyCeoqObeKJzfp55auVZW77rqLqVOnUqFCBVatWsW6des45JBDCtzO1KlT6d+/PwAtWrSgRYsWe9577bXXGDduHFlZWaxZs4YFCxbkeT+/r776issuu2zPTJ+dOnXiyy+/5JJLLqFJkya0atUKKHz65pUrV3LVVVexZs0adu3aRZMmTQD49NNP81Rn1a5dm3fffZfTTjttzzrRnn45qaZhULUugllZ8PTT3njoXCK69NJL+eyzz/jf//7Hjh079pTQJ0yYwPr165k5cyazZ8/m4IMPLnA65XAFXQUsW7aMRx99lM8++4w5c+bwpz/9qdjtaBHTLOdOywyFT9/cr18/+vbty9y5cxk7duye/WkBUy0XtCyakirpv/WW9QcfOtSmtHXOJZ4DDjiAM844gx49euRpwN2yZQsHHXQQlStXZsqUKSxfvrzI7Zx22ml7bn4+b9485syZA9i0zPvvvz81a9Zk3bp1fPjhh3s+U716dbZu3VrgtiZNmsT27dv57bffePvttzn11FMj/k5btmyhfv36ALzwwt7Ojeeddx5PPvnkntebNm3i5JNP5osvvmDZsmVA9KdfTpqkv3mzDX5p1QpuvTXoaJxzZdG1a1e+++67PXeuAujWrRsZGRmkp6czYcIEjj766CK38de//pVt27bRokULRowYQdu2bQG7C9bxxx/PscceS48ePfJMy9y7d28uuOACzsw393fr1q3p3r07bdu25cQTT6Rnz54cf3zkHRGHDh3KFVdcwamnnpqnvWDw4MFs2rSJ5s2b07JlS6ZMmUK9evUYN24cnTp1omXLllx11VUR7ycSUtRlSxDS09O1uL60BVm3Dv7yF7tbUXp6OQTmXAr4/vvvOeaYY4IOwxWjoN+TiMwMjYkqUtLMsnnwwTazo3POucIlTfWOc8654nnSd87lEW9Vvi6vsv5+POk75/ZIS0tj48aNnvjjlKqyceNG0tLSSr2NpKnTd86VXYMGDVi5ciXr168POhRXiLS0NBqU4T6bnvSdc3tUrlx5z0hQl5y8esc551KIJ33nnEshnvSdcy6FxN2IXBFZDxQ9qUbR6gIbohROovNjkZcfj7z8eOyVDMeikaoWe+vBuEv6ZSUiGZEMRU4Ffizy8uORlx+PvVLpWHj1jnPOpRBP+s45l0KSMemPCzqAOOLHIi8/Hnn58dgrZY5F0tXpO+ecK1wylvSdc84VwpO+c86lkKRJ+iLSQUQWisgSERkQdDxBEpHDRGSKiHwvIvNF5KagYwqaiFQUkVki8l7QsQRNRGqJyBsi8kPob+TkoGMKkojcEvo/mScir4pI6aewTABJkfRFpCIwBrgAaAZ0FZFmwUYVqCzgNlU9BjgJ6JPixwPgJuD7oIOIE48DH6nq0UBLUvi4iEh9oD+QrqrNgYpAl6I/ldiSIukDbYElqrpUVXcBE4GOAccUGFVdo6r/Cz3fiv1T1w82quCISAPgT8CzQccSNBGpAZwG/BNAVXep6uZgowpcJaCqiFQCqgGrA46nXCVL0q8P/BT2eiUpnOTCiUhj4HhgerCRBGo0cCeQE3QgceBwYD3wXKi661kR2T/ooIKiqquAR4EVwBpgi6r+O9ioyleyJH0pYFnK90UVkQOAN4GbVfXXoOMJgohcBPysqjODjiVOVAJaA0+p6vHAb0DKtoGJSG2sVqAJ8AdgfxG5JtioyleyJP2VwGFhrxuQ5JdoxRGRyljCn6CqbwUdT4DaAZeISCZW7XeWiLwcbEiBWgmsVNXcK783sJNAqjoHWKaq61V1N/AWcErAMZWrZEn6M4CmItJERKpgDTGTA44pMCIiWJ3t96o6Muh4gqSqA1W1gao2xv4uPlfVpC7JFUVV1wI/ichRoUVnAwsCDCloK4CTRKRa6P/mbJK8YTspbpeoqlki0hf4GGt9H6+q8wMOK0jtgGuBuSIyO7TsLlX9IMCYXPzoB0wIFZCWAtcHHE9gVHW6iLwB/A/r9TaLJJ+SwadhcM65FJIs1TvOOeci4EnfOedSiCd955xLIZ70nXMuhXjSd865FOJJ3znnUognfeecSyH/D9lK0foZJN8eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion\tMatrix\n",
    "A much better way to evaluate the performance of a classifier is to look at the\tconfusion\tmatrix.\tThe general\tidea is\tto count the number\tof times instances of class\tA are classified as\tclass B. For example, to know the number of times the classifier confused images of\t5s\twith 3s, you would look\tin the 5th row and 3rd column of the confusion matrix.\n",
    "\n",
    "To compute the confusion matrix, you first need\tto have\ta set of predictions, so they can be compared to the actual\ttargets. You could make\tpredictions\ton the test\tset, but let’s keep it untouched for now (remember that\tyou\twant to\tuse\tthe\ttest set only at the very end of your project, once\tyou\thave a classifier that you are ready to\tlaunch). Instead, you can\tuse\tthe\tcross_val_predict() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[57 16 24 28 32]\n",
      " [49 27 34 45 59]\n",
      " [49 15 14 29 35]\n",
      " [39 15 24 28 43]\n",
      " [58 22 26 36 61]]\n"
     ]
    }
   ],
   "source": [
    "#Confution Matrix \n",
    "Y_pred = model.predict_generator(validation_generator, 1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fine-tuning\n",
    "Another widely used technique for model reuse, complementary to feature extraction, is fine-tuning (see figure 5.19 in Chollet). Fine-tuning consists of unfreezing a few of\n",
    "the top layers of a frozen model base used for feature extraction, and jointly training\n",
    "both the newly added part of the model (in this case, the fully connected classifier)\n",
    "and these top layers. This is called fine-tuning because it slightly adjusts the more\n",
    "abstract representations of the model being reused, in order to make them more rele-\n",
    "vant for the problem at hand.\n",
    "\n",
    "As mentioned before, it’s necessary to freeze the convolution base of InceptionV3 in order to be able to train a randomly initialized classifier on top. For the same reason, it’s only\n",
    "possible to fine-tune the top layers of the convolutional base once the classifier on top\n",
    "has already been trained. If the classifier isn’t already trained, then the error signal\n",
    "propagating through the network during training will be too large, and the represen-\n",
    "tations previously learned by the layers being fine-tuned will be destroyed. Thus the\n",
    "steps for fine-tuning a network are as follow:\n",
    "\n",
    "1 - Add your custom network on top of an already-trained base network.\n",
    "\n",
    "2 - Freeze the base network.\n",
    "\n",
    "3 - Train the part you added.\n",
    "\n",
    "4 - Unfreeze some layers in the base network.\n",
    "\n",
    "5 - Jointly train both these layers and the part you added.\n",
    "\n",
    "We already completed the first three steps when doing feature extraction. Let’s pro-\n",
    "ceed with step 4: we’ll unfreeze our conv_base and then freeze individual layers\n",
    "inside it.\n",
    "\n",
    "As a reminder, this is what your convolutional base looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 74, 74, 32)   864         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 74, 74, 32)   96          conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 74, 74, 32)   0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 72, 72, 32)   9216        activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 72, 72, 32)   96          conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 72, 72, 32)   0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 72, 72, 64)   18432       activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 72, 72, 64)   192         conv2d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 72, 72, 64)   0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling2D) (None, 35, 35, 64)   0           activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 35, 35, 80)   5120        max_pooling2d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 35, 35, 80)   240         conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 35, 35, 80)   0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 33, 33, 192)  138240      activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 33, 33, 192)  576         conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 33, 33, 192)  0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling2D) (None, 16, 16, 192)  0           activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 16, 16, 64)   192         conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 16, 16, 64)   0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 16, 16, 48)   9216        max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 16, 16, 96)   55296       activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 16, 16, 48)   144         conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 16, 16, 96)   288         conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 16, 16, 48)   0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 16, 16, 96)   0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_37 (AveragePo (None, 16, 16, 192)  0           max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 16, 16, 64)   76800       activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 16, 16, 96)   82944       activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 16, 16, 32)   6144        average_pooling2d_37[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 16, 16, 64)   192         conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 16, 16, 64)   192         conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_387 (BatchN (None, 16, 16, 96)   288         conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 16, 16, 32)   96          conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 16, 16, 64)   0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 16, 16, 64)   0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 16, 16, 96)   0           batch_normalization_387[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 16, 16, 32)   0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_382[0][0]             \n",
      "                                                                 activation_384[0][0]             \n",
      "                                                                 activation_387[0][0]             \n",
      "                                                                 activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 16, 16, 64)   192         conv2d_420[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 16, 16, 64)   0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 16, 16, 96)   55296       activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 16, 16, 48)   144         conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 16, 16, 96)   288         conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 16, 16, 48)   0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 16, 16, 96)   0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_38 (AveragePo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 16, 16, 64)   76800       activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 16, 16, 96)   82944       activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 16, 16, 64)   16384       average_pooling2d_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 16, 16, 64)   192         conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 16, 16, 64)   192         conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 16, 16, 96)   288         conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 16, 16, 64)   192         conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 16, 16, 64)   0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 16, 16, 64)   0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 16, 16, 96)   0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 16, 16, 64)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_389[0][0]             \n",
      "                                                                 activation_391[0][0]             \n",
      "                                                                 activation_394[0][0]             \n",
      "                                                                 activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_427 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 16, 16, 64)   192         conv2d_427[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 16, 16, 64)   0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_428 (Conv2D)             (None, 16, 16, 96)   55296       activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 16, 16, 48)   144         conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 16, 16, 96)   288         conv2d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 16, 16, 48)   0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 16, 16, 96)   0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_39 (AveragePo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 16, 16, 64)   76800       activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_429 (Conv2D)             (None, 16, 16, 96)   82944       activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_430 (Conv2D)             (None, 16, 16, 64)   18432       average_pooling2d_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 16, 16, 64)   192         conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 16, 16, 64)   192         conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 16, 16, 96)   288         conv2d_429[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 16, 16, 64)   192         conv2d_430[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 16, 16, 64)   0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 16, 16, 64)   0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 16, 16, 96)   0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 16, 16, 64)   0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_396[0][0]             \n",
      "                                                                 activation_398[0][0]             \n",
      "                                                                 activation_401[0][0]             \n",
      "                                                                 activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_432 (Conv2D)             (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 16, 16, 64)   192         conv2d_432[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 16, 16, 64)   0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_433 (Conv2D)             (None, 16, 16, 96)   55296       activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 16, 16, 96)   288         conv2d_433[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 16, 16, 96)   0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_431 (Conv2D)             (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_434 (Conv2D)             (None, 7, 7, 96)     82944       activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 7, 7, 384)    1152        conv2d_431[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 7, 7, 96)     288         conv2d_434[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 7, 7, 384)    0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 7, 7, 96)     0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling2D) (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_403[0][0]             \n",
      "                                                                 activation_406[0][0]             \n",
      "                                                                 max_pooling2d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_439 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 7, 7, 128)    384         conv2d_439[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 7, 7, 128)    0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_440 (Conv2D)             (None, 7, 7, 128)    114688      activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 7, 7, 128)    384         conv2d_440[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 7, 7, 128)    0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_436 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_441 (Conv2D)             (None, 7, 7, 128)    114688      activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 7, 7, 128)    384         conv2d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 7, 7, 128)    384         conv2d_441[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 7, 7, 128)    0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 7, 7, 128)    0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_437 (Conv2D)             (None, 7, 7, 128)    114688      activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_442 (Conv2D)             (None, 7, 7, 128)    114688      activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 7, 7, 128)    384         conv2d_437[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 7, 7, 128)    384         conv2d_442[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 7, 7, 128)    0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 7, 7, 128)    0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_40 (AveragePo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_435 (Conv2D)             (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_438 (Conv2D)             (None, 7, 7, 192)    172032      activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_443 (Conv2D)             (None, 7, 7, 192)    172032      activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_444 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 7, 7, 192)    576         conv2d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 7, 7, 192)    576         conv2d_438[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 7, 7, 192)    576         conv2d_443[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_416 (BatchN (None, 7, 7, 192)    576         conv2d_444[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 7, 7, 192)    0           batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 7, 7, 192)    0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 7, 7, 192)    0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 7, 7, 192)    0           batch_normalization_416[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_407[0][0]             \n",
      "                                                                 activation_410[0][0]             \n",
      "                                                                 activation_415[0][0]             \n",
      "                                                                 activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_449 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_421 (BatchN (None, 7, 7, 160)    480         conv2d_449[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 7, 7, 160)    0           batch_normalization_421[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_450 (Conv2D)             (None, 7, 7, 160)    179200      activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_422 (BatchN (None, 7, 7, 160)    480         conv2d_450[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 7, 7, 160)    0           batch_normalization_422[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_446 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_451 (Conv2D)             (None, 7, 7, 160)    179200      activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_418 (BatchN (None, 7, 7, 160)    480         conv2d_446[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_423 (BatchN (None, 7, 7, 160)    480         conv2d_451[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 7, 7, 160)    0           batch_normalization_418[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 7, 7, 160)    0           batch_normalization_423[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_447 (Conv2D)             (None, 7, 7, 160)    179200      activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_452 (Conv2D)             (None, 7, 7, 160)    179200      activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_419 (BatchN (None, 7, 7, 160)    480         conv2d_447[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_424 (BatchN (None, 7, 7, 160)    480         conv2d_452[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 7, 7, 160)    0           batch_normalization_419[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 7, 7, 160)    0           batch_normalization_424[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_41 (AveragePo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_445 (Conv2D)             (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_448 (Conv2D)             (None, 7, 7, 192)    215040      activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_453 (Conv2D)             (None, 7, 7, 192)    215040      activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_454 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_41[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_417 (BatchN (None, 7, 7, 192)    576         conv2d_445[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_420 (BatchN (None, 7, 7, 192)    576         conv2d_448[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_425 (BatchN (None, 7, 7, 192)    576         conv2d_453[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_426 (BatchN (None, 7, 7, 192)    576         conv2d_454[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 7, 7, 192)    0           batch_normalization_417[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 7, 7, 192)    0           batch_normalization_420[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 7, 7, 192)    0           batch_normalization_425[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 7, 7, 192)    0           batch_normalization_426[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_417[0][0]             \n",
      "                                                                 activation_420[0][0]             \n",
      "                                                                 activation_425[0][0]             \n",
      "                                                                 activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_459 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_431 (BatchN (None, 7, 7, 160)    480         conv2d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 7, 7, 160)    0           batch_normalization_431[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_460 (Conv2D)             (None, 7, 7, 160)    179200      activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_432 (BatchN (None, 7, 7, 160)    480         conv2d_460[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 7, 7, 160)    0           batch_normalization_432[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_456 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_461 (Conv2D)             (None, 7, 7, 160)    179200      activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_428 (BatchN (None, 7, 7, 160)    480         conv2d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_433 (BatchN (None, 7, 7, 160)    480         conv2d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 7, 7, 160)    0           batch_normalization_428[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 7, 7, 160)    0           batch_normalization_433[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_457 (Conv2D)             (None, 7, 7, 160)    179200      activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_462 (Conv2D)             (None, 7, 7, 160)    179200      activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_429 (BatchN (None, 7, 7, 160)    480         conv2d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_434 (BatchN (None, 7, 7, 160)    480         conv2d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 7, 7, 160)    0           batch_normalization_429[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 7, 7, 160)    0           batch_normalization_434[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_42 (AveragePo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_455 (Conv2D)             (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_458 (Conv2D)             (None, 7, 7, 192)    215040      activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_463 (Conv2D)             (None, 7, 7, 192)    215040      activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_464 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_42[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_427 (BatchN (None, 7, 7, 192)    576         conv2d_455[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_430 (BatchN (None, 7, 7, 192)    576         conv2d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_435 (BatchN (None, 7, 7, 192)    576         conv2d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_436 (BatchN (None, 7, 7, 192)    576         conv2d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 7, 7, 192)    0           batch_normalization_427[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 7, 7, 192)    0           batch_normalization_430[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 7, 7, 192)    0           batch_normalization_435[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 7, 7, 192)    0           batch_normalization_436[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_427[0][0]             \n",
      "                                                                 activation_430[0][0]             \n",
      "                                                                 activation_435[0][0]             \n",
      "                                                                 activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_469 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_441 (BatchN (None, 7, 7, 192)    576         conv2d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 7, 7, 192)    0           batch_normalization_441[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_470 (Conv2D)             (None, 7, 7, 192)    258048      activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_442 (BatchN (None, 7, 7, 192)    576         conv2d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 7, 7, 192)    0           batch_normalization_442[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_466 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_471 (Conv2D)             (None, 7, 7, 192)    258048      activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_438 (BatchN (None, 7, 7, 192)    576         conv2d_466[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_443 (BatchN (None, 7, 7, 192)    576         conv2d_471[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 7, 7, 192)    0           batch_normalization_438[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 7, 7, 192)    0           batch_normalization_443[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_467 (Conv2D)             (None, 7, 7, 192)    258048      activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_472 (Conv2D)             (None, 7, 7, 192)    258048      activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_439 (BatchN (None, 7, 7, 192)    576         conv2d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_444 (BatchN (None, 7, 7, 192)    576         conv2d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 7, 7, 192)    0           batch_normalization_439[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 7, 7, 192)    0           batch_normalization_444[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_43 (AveragePo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_465 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_468 (Conv2D)             (None, 7, 7, 192)    258048      activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_473 (Conv2D)             (None, 7, 7, 192)    258048      activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_474 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_43[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_437 (BatchN (None, 7, 7, 192)    576         conv2d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_440 (BatchN (None, 7, 7, 192)    576         conv2d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_445 (BatchN (None, 7, 7, 192)    576         conv2d_473[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_446 (BatchN (None, 7, 7, 192)    576         conv2d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 7, 7, 192)    0           batch_normalization_437[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 7, 7, 192)    0           batch_normalization_440[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 7, 7, 192)    0           batch_normalization_445[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 7, 7, 192)    0           batch_normalization_446[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_437[0][0]             \n",
      "                                                                 activation_440[0][0]             \n",
      "                                                                 activation_445[0][0]             \n",
      "                                                                 activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_477 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_449 (BatchN (None, 7, 7, 192)    576         conv2d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 7, 7, 192)    0           batch_normalization_449[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_478 (Conv2D)             (None, 7, 7, 192)    258048      activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_450 (BatchN (None, 7, 7, 192)    576         conv2d_478[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_450 (Activation)     (None, 7, 7, 192)    0           batch_normalization_450[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_475 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_479 (Conv2D)             (None, 7, 7, 192)    258048      activation_450[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_447 (BatchN (None, 7, 7, 192)    576         conv2d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_451 (BatchN (None, 7, 7, 192)    576         conv2d_479[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 7, 7, 192)    0           batch_normalization_447[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_451 (Activation)     (None, 7, 7, 192)    0           batch_normalization_451[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_476 (Conv2D)             (None, 3, 3, 320)    552960      activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_480 (Conv2D)             (None, 3, 3, 192)    331776      activation_451[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_448 (BatchN (None, 3, 3, 320)    960         conv2d_476[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_452 (BatchN (None, 3, 3, 192)    576         conv2d_480[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 3, 3, 320)    0           batch_normalization_448[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_452 (Activation)     (None, 3, 3, 192)    0           batch_normalization_452[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling2D) (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_448[0][0]             \n",
      "                                                                 activation_452[0][0]             \n",
      "                                                                 max_pooling2d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_485 (Conv2D)             (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_457 (BatchN (None, 3, 3, 448)    1344        conv2d_485[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_457 (Activation)     (None, 3, 3, 448)    0           batch_normalization_457[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_482 (Conv2D)             (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_486 (Conv2D)             (None, 3, 3, 384)    1548288     activation_457[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_454 (BatchN (None, 3, 3, 384)    1152        conv2d_482[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_458 (BatchN (None, 3, 3, 384)    1152        conv2d_486[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_454 (Activation)     (None, 3, 3, 384)    0           batch_normalization_454[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_458 (Activation)     (None, 3, 3, 384)    0           batch_normalization_458[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_483 (Conv2D)             (None, 3, 3, 384)    442368      activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_484 (Conv2D)             (None, 3, 3, 384)    442368      activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_487 (Conv2D)             (None, 3, 3, 384)    442368      activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_488 (Conv2D)             (None, 3, 3, 384)    442368      activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_44 (AveragePo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_481 (Conv2D)             (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_455 (BatchN (None, 3, 3, 384)    1152        conv2d_483[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_456 (BatchN (None, 3, 3, 384)    1152        conv2d_484[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_459 (BatchN (None, 3, 3, 384)    1152        conv2d_487[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_460 (BatchN (None, 3, 3, 384)    1152        conv2d_488[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_489 (Conv2D)             (None, 3, 3, 192)    245760      average_pooling2d_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_453 (BatchN (None, 3, 3, 320)    960         conv2d_481[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_455 (Activation)     (None, 3, 3, 384)    0           batch_normalization_455[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_456 (Activation)     (None, 3, 3, 384)    0           batch_normalization_456[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_459 (Activation)     (None, 3, 3, 384)    0           batch_normalization_459[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_460 (Activation)     (None, 3, 3, 384)    0           batch_normalization_460[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_461 (BatchN (None, 3, 3, 192)    576         conv2d_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_453 (Activation)     (None, 3, 3, 320)    0           batch_normalization_453[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_455[0][0]             \n",
      "                                                                 activation_456[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 3, 3, 768)    0           activation_459[0][0]             \n",
      "                                                                 activation_460[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_461 (Activation)     (None, 3, 3, 192)    0           batch_normalization_461[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_453[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 activation_461[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_494 (Conv2D)             (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_466 (BatchN (None, 3, 3, 448)    1344        conv2d_494[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_466 (Activation)     (None, 3, 3, 448)    0           batch_normalization_466[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_491 (Conv2D)             (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_495 (Conv2D)             (None, 3, 3, 384)    1548288     activation_466[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_463 (BatchN (None, 3, 3, 384)    1152        conv2d_491[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_467 (BatchN (None, 3, 3, 384)    1152        conv2d_495[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_463 (Activation)     (None, 3, 3, 384)    0           batch_normalization_463[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_467 (Activation)     (None, 3, 3, 384)    0           batch_normalization_467[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_492 (Conv2D)             (None, 3, 3, 384)    442368      activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_493 (Conv2D)             (None, 3, 3, 384)    442368      activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_496 (Conv2D)             (None, 3, 3, 384)    442368      activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_497 (Conv2D)             (None, 3, 3, 384)    442368      activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_45 (AveragePo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_490 (Conv2D)             (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_464 (BatchN (None, 3, 3, 384)    1152        conv2d_492[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_465 (BatchN (None, 3, 3, 384)    1152        conv2d_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_468 (BatchN (None, 3, 3, 384)    1152        conv2d_496[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_469 (BatchN (None, 3, 3, 384)    1152        conv2d_497[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_498 (Conv2D)             (None, 3, 3, 192)    393216      average_pooling2d_45[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_462 (BatchN (None, 3, 3, 320)    960         conv2d_490[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_464 (Activation)     (None, 3, 3, 384)    0           batch_normalization_464[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_465 (Activation)     (None, 3, 3, 384)    0           batch_normalization_465[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_468 (Activation)     (None, 3, 3, 384)    0           batch_normalization_468[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_469 (Activation)     (None, 3, 3, 384)    0           batch_normalization_469[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_470 (BatchN (None, 3, 3, 192)    576         conv2d_498[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_462 (Activation)     (None, 3, 3, 320)    0           batch_normalization_462[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_464[0][0]             \n",
      "                                                                 activation_465[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3, 3, 768)    0           activation_468[0][0]             \n",
      "                                                                 activation_469[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_470 (Activation)     (None, 3, 3, 192)    0           batch_normalization_470[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_462[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_10[0][0]             \n",
      "                                                                 activation_470[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 0\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
